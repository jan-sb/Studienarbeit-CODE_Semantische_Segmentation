{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Zelle 1\n",
    "# =============\n",
    "# Hier laden wir die wichtigsten Bibliotheken und stellen sicher,\n",
    "# dass wir im richtigen Environment sind.\n",
    "\n",
    "# Wichtigste Imports\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# Für ONNX-Export:\n",
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "\n",
    "import ray.cloudpickle as pickle\n",
    "import torch\n",
    "\n",
    "\n",
    "from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType, CalibrationMethod\n",
    "import onnxruntime.quantization.calibrate as calibrate\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "# (Optional) Für FP16-Konvertierung oder INT8-Quantisierung:\n",
    "# Hier kann man z.B. onnxruntime-extensions / PyTorch FX Graph Mode etc. nutzen.\n",
    "# Zunächst installieren wir ggf. onnxruntime-tools, falls wir quantisieren wollen.\n",
    "# Das kann man in der requirements.txt so angeben:\n",
    "# onnxruntime\n",
    "# onnx\n",
    "# torch\n",
    "# numpy\n",
    "# psutil   # falls wir später GPU/CPU usage tracken\n",
    "# onnxruntime-tools  # optional für quantization utilities\n",
    "\n",
    "# Kontrolle über das Device:\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_onnx(\n",
    "    torch_model: torch.nn.Module,\n",
    "    output_filename: str,\n",
    "    input_shape=(1, 3, 520, 520),\n",
    "    dynamic_batch: bool=True,\n",
    "    opset_version: int=13\n",
    ") -> None:\n",
    "    # Bestimme das Device aus dem ersten Parameter des Modells\n",
    "    # (funktioniert nur, wenn das Modell mindestens einen Parameter hat!)\n",
    "    model_device = next(torch_model.parameters()).device\n",
    "\n",
    "    # 1) Lege einen Dummy-Input an (float32)\n",
    "    dummy_input = torch.randn(*input_shape, dtype=torch.float32, device=model_device)\n",
    "\n",
    "    if dynamic_batch:\n",
    "        dynamic_axes = {\n",
    "            'input': {0: 'batch_size'},\n",
    "            'output': {0: 'batch_size'}\n",
    "        }\n",
    "    else:\n",
    "        dynamic_axes = {}\n",
    "\n",
    "    # 2) Export nach ONNX\n",
    "    torch.onnx.export(\n",
    "        model=torch_model,\n",
    "        args=dummy_input,\n",
    "        f=output_filename,\n",
    "        export_params=True,\n",
    "        opset_version=opset_version,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes=dynamic_axes,\n",
    "    )\n",
    "\n",
    "    print(f\"ONNX-Export abgeschlossen. Gespeichert unter: {output_filename}\")\n",
    "    if dynamic_batch:\n",
    "        print(\" --> Batch-Dimension ist dynamisch.\")\n",
    "    else:\n",
    "        print(\" --> Feste Batch-Dimension im ONNX-Graph.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-29 13:33:36.188810: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-29 13:33:36.730041: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefundene Modelle in JSON: ['lraspp_mobilenet_v3_large', 'fcn_resnet101', 'deeplabv3_resnet50', 'fcn_resnet50', 'deeplabv3_mobilenet_v3_large', 'deeplabv3_resnet101']\n",
      "\n",
      "--- Start Export für lraspp_mobilenet_v3_large ---\n",
      "Using CUDA GPU\n",
      "Model loaded: lraspp_mobilenet_v3_large | Device: cuda \n",
      "Skipping local .pth load logic (likely using external Ray checkpoint).\n",
      "ONNX-Export abgeschlossen. Gespeichert unter: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/ONNX/lraspp_mobilenet_v3_large.onnx\n",
      " --> Batch-Dimension ist dynamisch.\n",
      "Export und Entladen von lraspp_mobilenet_v3_large abgeschlossen.\n",
      "\n",
      "\n",
      "--- Start Export für fcn_resnet101 ---\n",
      "Using CUDA GPU\n",
      "Model loaded: fcn_resnet101 | Device: cuda \n",
      "Skipping local .pth load logic (likely using external Ray checkpoint).\n",
      "ONNX-Export abgeschlossen. Gespeichert unter: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/ONNX/fcn_resnet101.onnx\n",
      " --> Batch-Dimension ist dynamisch.\n",
      "Export und Entladen von fcn_resnet101 abgeschlossen.\n",
      "\n",
      "\n",
      "--- Start Export für deeplabv3_resnet50 ---\n",
      "Using CUDA GPU\n",
      "Model loaded: deeplabv3_resnet50 | Device: cuda \n",
      "Skipping local .pth load logic (likely using external Ray checkpoint).\n",
      "ONNX-Export abgeschlossen. Gespeichert unter: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/ONNX/deeplabv3_resnet50.onnx\n",
      " --> Batch-Dimension ist dynamisch.\n",
      "Export und Entladen von deeplabv3_resnet50 abgeschlossen.\n",
      "\n",
      "\n",
      "--- Start Export für fcn_resnet50 ---\n",
      "Using CUDA GPU\n",
      "Model loaded: fcn_resnet50 | Device: cuda \n",
      "Skipping local .pth load logic (likely using external Ray checkpoint).\n",
      "ONNX-Export abgeschlossen. Gespeichert unter: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/ONNX/fcn_resnet50.onnx\n",
      " --> Batch-Dimension ist dynamisch.\n",
      "Export und Entladen von fcn_resnet50 abgeschlossen.\n",
      "\n",
      "\n",
      "--- Start Export für deeplabv3_mobilenet_v3_large ---\n",
      "Using CUDA GPU\n",
      "Model loaded: deeplabv3_mobilenet_v3_large | Device: cuda \n",
      "Skipping local .pth load logic (likely using external Ray checkpoint).\n",
      "ONNX-Export abgeschlossen. Gespeichert unter: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/ONNX/deeplabv3_mobilenet_v3_large.onnx\n",
      " --> Batch-Dimension ist dynamisch.\n",
      "Export und Entladen von deeplabv3_mobilenet_v3_large abgeschlossen.\n",
      "\n",
      "\n",
      "--- Start Export für deeplabv3_resnet101 ---\n",
      "Using CUDA GPU\n",
      "Model loaded: deeplabv3_resnet101 | Device: cuda \n",
      "Skipping local .pth load logic (likely using external Ray checkpoint).\n",
      "ONNX-Export abgeschlossen. Gespeichert unter: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/ONNX/deeplabv3_resnet101.onnx\n",
      " --> Batch-Dimension ist dynamisch.\n",
      "Export und Entladen von deeplabv3_resnet101 abgeschlossen.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Beispiel: Falls deine TrainedModel-Klasse in Helper/ml_models.py liegt:\n",
    "from Helper.ml_models import TrainedModel\n",
    "\n",
    "# Pfad zur JSON mit den Checkpoints\n",
    "best_checkpoints_json = \"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/best_checkpoints.json\"\n",
    "\n",
    "# 1) JSON laden\n",
    "with open(best_checkpoints_json, \"r\") as f:\n",
    "    best_checkpoints = json.load(f)\n",
    "\n",
    "print(\"Gefundene Modelle in JSON:\", list(best_checkpoints.keys()))\n",
    "\n",
    "\n",
    "# 2) Hilfsfunktion zum Laden eines PyTorch-Modells aus checkpoint.pkl\n",
    "def load_pytorch_model(model_name: str, checkpoint_path: str, \n",
    "                       width=2048, height=1024, \n",
    "                       skip_local_load=True) -> TrainedModel:\n",
    "    \"\"\"\n",
    "    Erstellt ein TrainedModel-Objekt und lädt die Ray-Tune Checkpointdaten.\n",
    "    \"\"\"\n",
    "    # Wir legen irgendein Dummy-Folder und Dummy-Gewichtsname an, \n",
    "    # damit die Klasse nicht meckert.\n",
    "    dummy_folder_path = \"/tmp/onnx_export_temp\"\n",
    "    dummy_weights_name = \"temp_weights\"\n",
    "\n",
    "    # 1) Erzeuge das TrainedModel (skip_local_load=True -> kein .pth-Laden)\n",
    "    model_obj = TrainedModel(\n",
    "        model_name=model_name,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        weights_name=dummy_weights_name,\n",
    "        folder_path=dummy_folder_path,\n",
    "        start_epoch=\"latest\",\n",
    "        skip_local_load=skip_local_load\n",
    "    )\n",
    "    \n",
    "    # 2) Ray-Checkpoint laden (per pickle)\n",
    "    if not os.path.isfile(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found at: {checkpoint_path}\")\n",
    "    with open(checkpoint_path, \"rb\") as fp:\n",
    "        checkpoint_data = pickle.load(fp)\n",
    "    \n",
    "    # 3) State_dicts wiederherstellen\n",
    "    model_obj.model.load_state_dict(checkpoint_data[\"model_state\"])\n",
    "    if \"optimizer_state\" in checkpoint_data:\n",
    "        model_obj.optimizer.load_state_dict(checkpoint_data[\"optimizer_state\"])\n",
    "    \n",
    "    model_obj.model.eval()\n",
    "    model_obj.model.to(device)  # auf CPU oder GPU\n",
    "    return model_obj\n",
    "\n",
    "\n",
    "for arch_name, ckp_path in best_checkpoints.items():\n",
    "    print(f\"\\n--- Start Export für {arch_name} ---\")\n",
    "    # 1) Laden\n",
    "    model_trained = load_pytorch_model(arch_name, ckp_path)\n",
    "    \n",
    "    # 2) Export nach ONNX\n",
    "    onnx_filename = f\"{arch_name}.onnx\"\n",
    "    export_to_onnx(\n",
    "        torch_model=model_trained.model, \n",
    "        output_filename=f'/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/ONNX/{onnx_filename}',\n",
    "        input_shape=(1, 3, 520, 520),  # oder (1,3,H,W)\n",
    "        dynamic_batch=True,           # True oder False\n",
    "        opset_version=13\n",
    "    )\n",
    "    \n",
    "    # 3) Modell entladen, GPU-Cache leeren\n",
    "    del model_trained\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Export und Entladen von {arch_name} abgeschlossen.\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Für FP16-Konvertierung\n",
    "# (Installiere vorher: pip install onnxruntime-extensions onnxruntime-tools)\n",
    "try:\n",
    "    from onnxconverter_common import float16\n",
    "    HAS_FLOAT16_CONVERTER = True\n",
    "except ImportError:\n",
    "    HAS_FLOAT16_CONVERTER = False\n",
    "    print(\"[WARN] float16_converter nicht verfügbar. FP16-Konvertierung wird ggf. übersprungen.\")\n",
    "\n",
    "# Für INT8-Dynamic Quantization\n",
    "try:\n",
    "    from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "    HAS_QUANT_DYNAMIC = True\n",
    "except ImportError:\n",
    "    HAS_QUANT_DYNAMIC = False\n",
    "    print(\"[WARN] quantize_dynamic nicht verfügbar. Dynamische INT8-Quantisierung wird ggf. übersprungen.\")\n",
    "\n",
    "\n",
    "def convert_fp16_onnx(fp32_onnx_path: str, fp16_onnx_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Konvertiert ein ONNX-Modell von FP32 nach FP16 mit onnxconverter-common.float16.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(fp32_onnx_path):\n",
    "        print(f\"[ERROR] Datei existiert nicht: {fp32_onnx_path}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        print(f\"Starte FP16-Konvertierung von: {fp32_onnx_path}\")\n",
    "\n",
    "        # Lade das ONNX-Modell\n",
    "        model = onnx.load(fp32_onnx_path)\n",
    "\n",
    "        # Wandle auf FP16 um\n",
    "        model_fp16 = float16.convert_float_to_float16(model)\n",
    "\n",
    "        # Speichere das FP16-Modell\n",
    "        onnx.save(model_fp16, fp16_onnx_path)\n",
    "        print(f\"✅ FP16-Modell gespeichert unter: {fp16_onnx_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] FP16-Konvertierung fehlgeschlagen: {e}\")\n",
    "\n",
    "\n",
    "def quantize_int8_dynamic(fp32_onnx_path: str, int8_onnx_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Dynamische INT8-Quantisierung mit onnxruntime.quantization.quantize_dynamic.\n",
    "    Achtung: Eher einfache, heuristische Quantisierung ohne Kalibrierung.\n",
    "    \"\"\"\n",
    "    if not HAS_QUANT_DYNAMIC:\n",
    "        print(\"[ERROR] quantize_dynamic nicht verfügbar.\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.isfile(fp32_onnx_path):\n",
    "        print(f\"[ERROR] Datei existiert nicht: {fp32_onnx_path}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        print(f\"Starte dynamische INT8-Quantisierung: {fp32_onnx_path}\")\n",
    "        # Liste der Ops, die quantisiert werden sollen\n",
    "        # Typischerweise: ['Conv', 'MatMul', 'Gemm'] \n",
    "        # (je nach Modell ggf. mehr)\n",
    "        op_types_to_quantize = ['Conv', 'MatMul']\n",
    "\n",
    "        quantize_dynamic(\n",
    "            model_input=fp32_onnx_path,\n",
    "            model_output=int8_onnx_path,\n",
    "            op_types_to_quantize=op_types_to_quantize,\n",
    "            weight_type=QuantType.QUInt8  # oder QuantType.QInt8\n",
    "        )\n",
    "        print(f\"INT8-Modell gespeichert unter: {int8_onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Dynamische INT8-Quantisierung schlug fehl: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefundene Kalibrierbilder: 3475\n",
      "🚀 Starte INT8-Kalibrierung für deeplabv3_resnet50.onnx...\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "Unable to open proto file: deeplabv3_resnet50.onnx. Please check if it is a valid proto. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ INT8-Modell gespeichert unter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mint8_onnx_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Beispielaufruf:\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m \u001b[43mquantize_int8_calibrated\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp32_onnx_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeeplabv3_resnet50.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mint8_onnx_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeeplabv3_resnet50_int8.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcalibration_data_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/CityscapesDaten/images/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_tensor_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Falls unbekannt, mit onnx.load() prüfen\u001b[39;49;00m\n\u001b[1;32m     83\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 69\u001b[0m, in \u001b[0;36mquantize_int8_calibrated\u001b[0;34m(fp32_onnx_path, int8_onnx_path, calibration_data_path, input_tensor_name)\u001b[0m\n\u001b[1;32m     61\u001b[0m data_reader \u001b[38;5;241m=\u001b[39m CalibrationDataLoader(\n\u001b[1;32m     62\u001b[0m     calibration_data_path\u001b[38;5;241m=\u001b[39mcalibration_data_path,\n\u001b[1;32m     63\u001b[0m     input_tensor_name\u001b[38;5;241m=\u001b[39minput_tensor_name,\n\u001b[1;32m     64\u001b[0m     target_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m520\u001b[39m, \u001b[38;5;241m520\u001b[39m)  \u001b[38;5;66;03m# Modell-Inputgröße setzen\u001b[39;00m\n\u001b[1;32m     65\u001b[0m )\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🚀 Starte INT8-Kalibrierung für \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfp32_onnx_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[43mquantize_static\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp32_onnx_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mint8_onnx_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcalibration_data_reader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_reader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ INT8-Modell gespeichert unter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mint8_onnx_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/studi/lib/python3.9/site-packages/onnxruntime/quantization/quantize.py:460\u001b[0m, in \u001b[0;36mquantize_static\u001b[0;34m(model_input, model_output, calibration_data_reader, quant_format, op_types_to_quantize, per_channel, reduce_range, activation_type, weight_type, nodes_to_quantize, nodes_to_exclude, use_external_data_format, calibrate_method, extra_options)\u001b[0m\n\u001b[1;32m    454\u001b[0m     qdq_ops \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(QDQRegistry\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    455\u001b[0m     op_types_to_quantize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(q_linear_ops \u001b[38;5;241m+\u001b[39m qdq_ops))\n\u001b[1;32m    457\u001b[0m model \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    458\u001b[0m     save_and_reload_model_with_shape_infer(model_input)\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_input, onnx\u001b[38;5;241m.\u001b[39mModelProto)\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mload_model_with_shape_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m )\n\u001b[1;32m    463\u001b[0m pre_processed: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m model_has_pre_process_metadata(model)\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pre_processed:\n",
      "File \u001b[0;32m~/anaconda3/envs/studi/lib/python3.9/site-packages/onnxruntime/quantization/quant_utils.py:822\u001b[0m, in \u001b[0;36mload_model_with_shape_infer\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model_with_shape_infer\u001b[39m(model_path: Path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModelProto:\n\u001b[1;32m    821\u001b[0m     inferred_model_path \u001b[38;5;241m=\u001b[39m generate_identified_filename(model_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-inferred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 822\u001b[0m     \u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape_inference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_shapes_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minferred_model_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    823\u001b[0m     model \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mload(inferred_model_path\u001b[38;5;241m.\u001b[39mas_posix())\n\u001b[1;32m    824\u001b[0m     add_infer_metadata(model)\n",
      "File \u001b[0;32m~/anaconda3/envs/studi/lib/python3.9/site-packages/onnx/shape_inference.py:96\u001b[0m, in \u001b[0;36minfer_shapes_path\u001b[0;34m(model_path, output_path, check_type, strict_mode, data_prop)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_path \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     95\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m model_path\n\u001b[0;32m---> 96\u001b[0m \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_shapes_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_prop\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: Unable to open proto file: deeplabv3_resnet50.onnx. Please check if it is a valid proto. "
     ]
    }
   ],
   "source": [
    "class CalibrationDataLoader(CalibrationDataReader):\n",
    "    \"\"\"\n",
    "    Custom DataLoader für ONNX INT8-Kalibrierung.\n",
    "    Lädt Bilder aus einem Ordner, skaliert sie auf das Modellinputformat und gibt sie als Batch zurück.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, calibration_data_path: str, input_tensor_name: str, target_shape=(1, 3, 520, 520)):\n",
    "        \"\"\"\n",
    "        :param calibration_data_path: Pfad zum Ordner mit den Kalibrierungsbildern\n",
    "        :param input_tensor_name: Name des Eingabetensors für ONNX\n",
    "        :param target_shape: Zielgröße der Bilder (Batch, Channels, Height, Width)\n",
    "        \"\"\"\n",
    "        self.image_paths = [os.path.join(calibration_data_path, f) \n",
    "                            for f in os.listdir(calibration_data_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.input_tensor_name = input_tensor_name\n",
    "        self.target_shape = target_shape\n",
    "        self.index = 0\n",
    "        print(f\"Gefundene Kalibrierbilder: {len(self.image_paths)}\")\n",
    "        \n",
    "        if len(self.image_paths) == 0:\n",
    "            raise ValueError(\"[ERROR] Keine Bilder für die Kalibrierung gefunden!\")\n",
    "\n",
    "    def get_next(self):\n",
    "        \"\"\"\n",
    "        Lädt das nächste Batch an Bildern, skaliert sie und gibt sie zurück.\n",
    "        \"\"\"\n",
    "        if self.index >= len(self.image_paths):\n",
    "            return None  # Ende der Daten\n",
    "\n",
    "        batch_images = []\n",
    "        for _ in range(self.target_shape[0]):  # Batch-Größe\n",
    "            if self.index >= len(self.image_paths):\n",
    "                break\n",
    "            \n",
    "            img_path = self.image_paths[self.index]\n",
    "            image = cv2.imread(img_path)  # Lade Bild mit OpenCV\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Konvertiere zu RGB\n",
    "            image = cv2.resize(image, (self.target_shape[3], self.target_shape[2]))  # Resize auf (Width, Height)\n",
    "            image = image.astype(np.float32) / 255.0  # Normalisieren auf [0,1]\n",
    "            image = np.transpose(image, (2, 0, 1))  # Channel-First Format (C, H, W)\n",
    "            batch_images.append(image)\n",
    "            self.index += 1\n",
    "\n",
    "        batch_images = np.array(batch_images).astype(np.float32)  # In NumPy Array umwandeln\n",
    "        batch_images = np.expand_dims(batch_images, axis=0)  # Batch-Dimension hinzufügen\n",
    "        return {self.input_tensor_name: batch_images}\n",
    "\n",
    "    def rewind(self):\n",
    "        \"\"\"\n",
    "        Setzt den Index zurück, um die Daten erneut zu laden.\n",
    "        \"\"\"\n",
    "        self.index = 0\n",
    "\n",
    "\n",
    "\n",
    "def quantize_int8_calibrated(fp32_onnx_path: str, int8_onnx_path: str, calibration_data_path: str, input_tensor_name: str):\n",
    "    \"\"\"\n",
    "    Führt eine INT8-Quantisierung mit Kalibrierung durch.\n",
    "    \"\"\"\n",
    "    # Lade Kalibrierungs-Dataloader\n",
    "    data_reader = CalibrationDataLoader(\n",
    "        calibration_data_path=calibration_data_path,\n",
    "        input_tensor_name=input_tensor_name,\n",
    "        target_shape=(1, 3, 520, 520)  # Modell-Inputgröße setzen\n",
    "    )\n",
    "\n",
    "    print(f\"🚀 Starte INT8-Kalibrierung für {fp32_onnx_path}...\")\n",
    "    \n",
    "    quantize_static(\n",
    "        model_input=fp32_onnx_path,\n",
    "        model_output=int8_onnx_path,\n",
    "        calibration_data_reader=data_reader,\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ INT8-Modell gespeichert unter: {int8_onnx_path}\")\n",
    "\n",
    "# Beispielaufruf:\n",
    "quantize_int8_calibrated(\n",
    "    fp32_onnx_path=\"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/ONNX/deeplabv3_resnet50.onnx\",\n",
    "    int8_onnx_path=\"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/ONNX/deeplabv3_resnet50_int8.onnx\",\n",
    "    calibration_data_path=\"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/CityscapesDaten/images/\",\n",
    "    input_tensor_name=\"input\"  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
