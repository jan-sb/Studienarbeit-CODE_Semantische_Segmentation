{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import, division\n",
    "from collections import namedtuple\n",
    "from cityscapesscripts.helpers.labels import *\n",
    "import Helper.Helper_functions as hf\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "from PIL import Image\n",
    "from Helper.ml_models import *\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of cityscapes labels:\n",
      "\n",
      "                     name |  id | trainId |       category | categoryId | hasInstances |        color |   ignoreInEval\n",
      "    --------------------------------------------------------------------------------------------------\n",
      "                 unlabeled |   0 |     255 |           void |          0 |            0 |    (0, 0, 0) | True\n",
      "               ego vehicle |   1 |     255 |           void |          0 |            0 |    (0, 0, 0) | True\n",
      "      rectification border |   2 |     255 |           void |          0 |            0 |    (0, 0, 0) | True\n",
      "                out of roi |   3 |     255 |           void |          0 |            0 |    (0, 0, 0) | True\n",
      "                    static |   4 |     255 |           void |          0 |            0 |    (0, 0, 0) | True\n",
      "                   dynamic |   5 |     255 |           void |          0 |            0 | (111, 74, 0) | True\n",
      "                    ground |   6 |     255 |           void |          0 |            0 |  (81, 0, 81) | True\n",
      "                      road |   7 |       0 |           flat |          1 |            0 | (128, 64, 128) | False\n",
      "                  sidewalk |   8 |       1 |           flat |          1 |            0 | (244, 35, 232) | False\n",
      "                   parking |   9 |     255 |           flat |          1 |            0 | (250, 170, 160) | True\n",
      "                rail track |  10 |     255 |           flat |          1 |            0 | (230, 150, 140) | True\n",
      "                  building |  11 |       2 |   construction |          2 |            0 | (70, 70, 70) | False\n",
      "                      wall |  12 |       3 |   construction |          2 |            0 | (102, 102, 156) | False\n",
      "                     fence |  13 |       4 |   construction |          2 |            0 | (190, 153, 153) | False\n",
      "                guard rail |  14 |     255 |   construction |          2 |            0 | (180, 165, 180) | True\n",
      "                    bridge |  15 |     255 |   construction |          2 |            0 | (150, 100, 100) | True\n",
      "                    tunnel |  16 |     255 |   construction |          2 |            0 | (150, 120, 90) | True\n",
      "                      pole |  17 |       5 |         object |          3 |            0 | (153, 153, 153) | False\n",
      "                 polegroup |  18 |     255 |         object |          3 |            0 | (153, 153, 153) | True\n",
      "             traffic light |  19 |       6 |         object |          3 |            0 | (250, 170, 30) | False\n",
      "              traffic sign |  20 |       7 |         object |          3 |            0 | (220, 220, 0) | False\n",
      "                vegetation |  21 |       8 |         nature |          4 |            0 | (107, 142, 35) | False\n",
      "                   terrain |  22 |       9 |         nature |          4 |            0 | (152, 251, 152) | False\n",
      "                       sky |  23 |      10 |            sky |          5 |            0 | (70, 130, 180) | False\n",
      "                    person |  24 |      11 |          human |          6 |            1 | (220, 20, 60) | False\n",
      "                     rider |  25 |      12 |          human |          6 |            1 |  (255, 0, 0) | False\n",
      "                       car |  26 |      13 |        vehicle |          7 |            1 |  (0, 0, 142) | False\n",
      "                     truck |  27 |      14 |        vehicle |          7 |            1 |   (0, 0, 70) | False\n",
      "                       bus |  28 |      15 |        vehicle |          7 |            1 | (0, 60, 100) | False\n",
      "                   caravan |  29 |     255 |        vehicle |          7 |            1 |   (0, 0, 90) | True\n",
      "                   trailer |  30 |     255 |        vehicle |          7 |            1 |  (0, 0, 110) | True\n",
      "                     train |  31 |      16 |        vehicle |          7 |            1 | (0, 80, 100) | False\n",
      "                motorcycle |  32 |      17 |        vehicle |          7 |            1 |  (0, 0, 230) | False\n",
      "                   bicycle |  33 |      18 |        vehicle |          7 |            1 | (119, 11, 32) | False\n",
      "             license plate |  -1 |      -1 |        vehicle |          7 |            0 |  (0, 0, 142) | True\n",
      "  \n",
      " 35\n"
     ]
    }
   ],
   "source": [
    "print(\"List of cityscapes labels:\")\n",
    "print(\"\")\n",
    "print(\"    {:>21} | {:>3} | {:>7} | {:>14} | {:>10} | {:>12} | {:>12} | {:>14}\".format('name', 'id', 'trainId', 'category',\n",
    "                                                                              'categoryId', 'hasInstances',\n",
    "                                                                              'color', 'ignoreInEval'))\n",
    "print(\"    \" + ('-' * 98))\n",
    "counter = 0\n",
    "for label in labels:\n",
    "    #if label.ignoreInEval ==0:\n",
    "    counter +=1\n",
    "    print(\"     {:>21} | {:>3} | {:>7} | {:>14} | {:>10} | {:>12} | {:>12} | {}\".format(label.name, label.id, label.trainId,\n",
    "                                                                                  label.category, label.categoryId,\n",
    "                                                                                  label.hasInstances,\n",
    "                                                                                  str(label.color),  # Convert the tuple to a string\n",
    "                                                                                  label.ignoreInEval,\n",
    "                                                                                  ))\n",
    "print(\" \", '\\n', counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionary with the ID and Values for remapping the annotation images\n",
    "\n",
    "New dictionary gets compared to the cityscapes mapping and differences are printed. Values of 255 got mapped to value 19, so only IDs which initially had the value 155 and got changed to 19 should be printed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: 0, id_to_trainId: 255, id_to_trainId_updated: 19\n",
      "Key: 1, id_to_trainId: 255, id_to_trainId_updated: 19\n",
      "Key: 2, id_to_trainId: 255, id_to_trainId_updated: 19\n",
      "Key: 3, id_to_trainId: 255, id_to_trainId_updated: 19\n",
      "Key: 4, id_to_trainId: 255, id_to_trainId_updated: 19\n",
      "Key: 5, id_to_trainId: 255, id_to_trainId_updated: 19\n",
      "Key: 6, id_to_trainId: 255, id_to_trainId_updated: 19\n",
      "Key: 9, id_to_trainId: 255, id_to_trainId_updated: 19\n",
      "Key: 10, id_to_trainId: 255, id_to_trainId_updated: 19\n",
      "Key: 14, id_to_trainId: 255, id_to_trainId_updated: 19\n",
      "Key: 15, id_to_trainId: 255, id_to_trainId_updated: 19\n",
      "Key: 16, id_to_trainId: 255, id_to_trainId_updated: 19\n",
      "Key: 18, id_to_trainId: 255, id_to_trainId_updated: 19\n",
      "Key: 29, id_to_trainId: 255, id_to_trainId_updated: 19\n",
      "Key: 30, id_to_trainId: 255, id_to_trainId_updated: 19\n"
     ]
    }
   ],
   "source": [
    "id_to_trainId = {label.id: label.trainId for label in labels if label.trainId >= 0}\n",
    "id_to_trainId_updated = {k: 19 if v == 255 else v for k, v in id_to_trainId.items()}\n",
    "\n",
    "# Iterate over the keys in id_to_trainId_updated\n",
    "for key in id_to_trainId_updated:\n",
    "    # Check if the value in id_to_trainId_updated matches the value in id_to_trainId\n",
    "    if id_to_trainId_updated[key] != id_to_trainId.get(key, None):\n",
    "        # If the values do not match, print out the key and the corresponding values\n",
    "        print(f'Key: {key}, id_to_trainId: {id_to_trainId.get(key, None)}, id_to_trainId_updated: {id_to_trainId_updated[key]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling of the Annotations\n",
    "\n",
    "The annotations get labeled according to the above table, using trainID for the resulting value in the annotation. Annotations with value 255 get labeled to channel 19. All labels got preprocessed by the cityscapes function and sorted intp the CityscapesDaten/semantic_default folder. Here the annotations are still labeled by the id range. Annotations get opened from this folder, processed by a script in this jupyter notebook and saved the CityscapesDaten/semantic folder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ground_truth_V2(in_path, out_path, batch_size=6):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Create a mapping from label ID to trainId\n",
    "    id_to_trainId = {label.id: label.trainId for label in labels if label.trainId >= 0}\n",
    "    id_to_trainId = {k: 19 if v == 255 else v for k, v in id_to_trainId.items()}\n",
    "    id_to_trainId = torch.tensor([id_to_trainId.get(i, -1) for i in range(256)], dtype=torch.long, device=device)\n",
    "\n",
    "    # Get the list of image files\n",
    "    image_files = os.listdir(in_path)\n",
    "\n",
    "    # Process the images in batches\n",
    "    for i in tqdm(range(0, len(image_files), batch_size), desc=\"Processing images\"):\n",
    "        batch_files = image_files[i:i+batch_size]\n",
    "        batch_images = []\n",
    "        for image_file in batch_files:\n",
    "            # Load the annotation image\n",
    "            annotation = Image.open(os.path.join(in_path, image_file))\n",
    "            # Convert the annotation image to a PyTorch tensor and move it to the device\n",
    "            annotation = torch.from_numpy(np.array(annotation)).to(device)\n",
    "            batch_images.append(annotation)\n",
    "\n",
    "        # Stack images into a batch\n",
    "        batch_images = torch.stack(batch_images)\n",
    "\n",
    "        # Convert the label IDs to trainId\n",
    "        batch_images = id_to_trainId[batch_images.long()]\n",
    "\n",
    "        # Save the images\n",
    "        for j, image in enumerate(batch_images):\n",
    "            # Convert the tensor back to a PIL image\n",
    "            image = transforms.ToPILImage()(image.cpu().byte())\n",
    "            # Save the annotation image with the original name\n",
    "            image.save(os.path.join(out_path, batch_files[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_input = 'CityscapesDaten/semantic_default'\n",
    "annotation_output = 'CityscapesDaten/semantic'\n",
    "\n",
    "os.makedirs(annotation_output, exist_ok=True)\n",
    "\n",
    "run = False\n",
    "while run: \n",
    "    create_ground_truth_V2(annotation_input, annotation_output)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Dataloader to load an image and annotation from the folder\n",
    "\n",
    "The shape of the Image should be [3, 520, 520] and the shape of the annotation should be [20, 520, 520]. The unique annotation value range should be\n",
    "[  0   1   2   5   6   7   8   9  10  11  13  15  18 19] and the Dataloader annotation Value range shoudl be [0. 1.]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 520, 520]), Annotation shape: torch.Size([20, 520, 520])\n",
      "Dataloader annotation Value range: [0. 1.]\n",
      "All unique annotation values: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
     ]
    }
   ],
   "source": [
    "annotation_dir = 'CityscapesDaten/semantic'\n",
    "dataset = CustomDataSet(image_dir='CityscapesDaten/images',\n",
    "                        annotation_dir='CityscapesDaten/semantic')\n",
    "image, annotation = dataset.__getitem__(0)\n",
    "\n",
    "print(f'Image shape: {image.shape}, Annotation shape: {annotation.shape}')\n",
    "print(f'Dataloader annotation Value range: {np.unique(annotation)}')\n",
    "\n",
    "annotation_files = os.listdir(annotation_dir)[:100]  # Get the first 100 files\n",
    "\n",
    "all_values = []\n",
    "\n",
    "for annotation_file in annotation_files:\n",
    "    # Load the annotation image\n",
    "    annotation = Image.open(os.path.join(annotation_dir, annotation_file))\n",
    "    # Convert the annotation image to a numpy array\n",
    "    annotation = np.array(annotation)\n",
    "    # Append the unique values in this image to the list of all values\n",
    "    all_values.extend(np.unique(annotation))\n",
    "\n",
    "# Get unique values from all_values\n",
    "unique_values = np.unique(all_values)\n",
    "\n",
    "# Print all unique values\n",
    "print(f'All unique annotation values: {unique_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
