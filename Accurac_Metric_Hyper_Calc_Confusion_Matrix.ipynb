{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ray.cloudpickle as pickle\n",
    "from Helper.ml_models import *\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_PATH: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation\n",
      "CONF_MATRIX_DIR: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/confusion_matrices_hyper_small\n",
      "\n",
      "Dataset-Pfade:\n",
      "  Train Images      : /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Mapillary_Vistas/training/images\n",
      "  Train Annotations : /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Mapillary_Vistas/training/v2.0/labels_small\n",
      "  Validation Images : /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Mapillary_Vistas/validation/images\n",
      "  Validation Annotations: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Mapillary_Vistas/validation/v2.0/labels_small\n",
      "\n",
      "Hyperparameter Folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small\n",
      "NUM_CLASSES_ORIGINAL: 124\n",
      "run_all = True\n"
     ]
    }
   ],
   "source": [
    "# Basisverzeichnis des Projekts\n",
    "BASE_PATH = \"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation\"\n",
    "\n",
    "# Verzeichnis zur Speicherung der Confusion Matrices\n",
    "CONF_MATRIX_DIR = os.path.join(BASE_PATH, \"FINAL_DATEN/confusion_matrices_hyper_small\")\n",
    "os.makedirs(CONF_MATRIX_DIR, exist_ok=True)\n",
    "\n",
    "# Dataset-Pfade (Mapillary)\n",
    "TRAIN_IMAGES_DIR = os.path.join(BASE_PATH, \"Mapillary_Vistas/training/images\")\n",
    "TRAIN_ANNOTATIONS_DIR = os.path.join(BASE_PATH, \"Mapillary_Vistas/training/v2.0/labels_small\")\n",
    "VAL_IMAGES_DIR = os.path.join(BASE_PATH, \"Mapillary_Vistas/validation/images\")\n",
    "VAL_ANNOTATIONS_DIR = os.path.join(BASE_PATH, \"Mapillary_Vistas/validation/v2.0/labels_small\")\n",
    "\n",
    "# Evaluationseinstellungen\n",
    "# Hier wird die originale Anzahl der Klassen definiert, die das Modell ausgibt.\n",
    "# Für gerelabelte Datensätze kann dieser Wert variieren.\n",
    "NUM_CLASSES_ORIGINAL = 124\n",
    "\n",
    "# Pfad zum Ordner, in dem die Hyperparameter-Runs (inkl. Checkpoints) gespeichert sind\n",
    "HYPER_FOLDER = os.path.join(BASE_PATH, \"HyperparameterLOG_small\")\n",
    "HYPER_FOLDER = os.path.abspath(HYPER_FOLDER)\n",
    "\n",
    "# Steuerelement: Sollen alle Modelle automatisch evaluiert werden?\n",
    "run_all = True\n",
    "\n",
    "# Ausgabe der gesetzten Parameter zur Kontrolle:\n",
    "print(\"BASE_PATH:\", BASE_PATH)\n",
    "print(\"CONF_MATRIX_DIR:\", CONF_MATRIX_DIR)\n",
    "print(\"\\nDataset-Pfade:\")\n",
    "print(\"  Train Images      :\", TRAIN_IMAGES_DIR)\n",
    "print(\"  Train Annotations :\", TRAIN_ANNOTATIONS_DIR)\n",
    "print(\"  Validation Images :\", VAL_IMAGES_DIR)\n",
    "print(\"  Validation Annotations:\", VAL_ANNOTATIONS_DIR)\n",
    "print(\"\\nHyperparameter Folder:\", HYPER_FOLDER)\n",
    "print(\"NUM_CLASSES_ORIGINAL:\", NUM_CLASSES_ORIGINAL)\n",
    "print(\"run_all =\", run_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelle mit extrahierten besten Checkpoints:\n",
      "  fcn_resnet101: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/fcn_resnet101/train_hyper_7adb91eb_1_auto_cast=True,batch_size=6,learning_rate=0.0001,max_epochs=100,weight_decay=0_2025-03-11_12-03-54/checkpoint_000099/checkpoint.pkl\n",
      "  deeplabv3_resnet50: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/deeplabv3_resnet50/train_hyper_a8ae06ed_1_auto_cast=True,batch_size=6,learning_rate=0.0001,max_epochs=100,weight_decay=0_2025-03-07_20-02-56/checkpoint_000099/checkpoint.pkl\n",
      "  fcn_resnet50: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/fcn_resnet50/train_hyper_bc32f73a_1_auto_cast=True,batch_size=6,learning_rate=0.0001,max_epochs=100,weight_decay=0_2025-03-10_11-25-21/checkpoint_000099/checkpoint.pkl\n",
      "  deeplabv3_resnet101: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/deeplabv3_resnet101/train_hyper_ae167f80_1_auto_cast=True,batch_size=6,learning_rate=0.0001,max_epochs=100,weight_decay=0_2025-03-09_00-01-13/checkpoint_000099/checkpoint.pkl\n"
     ]
    }
   ],
   "source": [
    "def load_hyperparameter_runs_as_dict(base_folder: str):\n",
    "    \"\"\"\n",
    "    Lädt alle Hyperparameter-Runs aus der Ordnerstruktur als Dictionary.\n",
    "    \"\"\"\n",
    "    runs_data = {}\n",
    "    if not os.path.isdir(base_folder):\n",
    "        print(f\"ERROR: Base folder does not exist: {base_folder}\")\n",
    "        return runs_data\n",
    "\n",
    "    for model_folder in os.listdir(base_folder):\n",
    "        model_path = os.path.join(base_folder, model_folder)\n",
    "        if not os.path.isdir(model_path):\n",
    "            continue\n",
    "\n",
    "        runs_data[model_folder] = {}\n",
    "\n",
    "        for train_folder in os.listdir(model_path):\n",
    "            train_folder_path = os.path.join(model_path, train_folder)\n",
    "            # Nur Trainingsordner berücksichtigen (müssen mit \"train_hyper_\" beginnen)\n",
    "            if not train_folder.startswith(\"train_hyper_\") or not os.path.isdir(train_folder_path):\n",
    "                continue\n",
    "\n",
    "            # Benötigte Dateien: params.json, progress.csv, result.json\n",
    "            params_file = os.path.join(train_folder_path, \"params.json\")\n",
    "            progress_file = os.path.join(train_folder_path, \"progress.csv\")\n",
    "            result_file = os.path.join(train_folder_path, \"result.json\")\n",
    "\n",
    "            if not (os.path.isfile(params_file) and os.path.isfile(progress_file) and os.path.isfile(result_file)):\n",
    "                continue\n",
    "\n",
    "            with open(params_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                params_dict = json.load(f)\n",
    "\n",
    "            progress_df = pd.read_csv(progress_file)\n",
    "            progress_records = progress_df.to_dict(orient=\"records\")\n",
    "\n",
    "            result_records = []\n",
    "            with open(result_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        result_records.append(json.loads(line))\n",
    "\n",
    "            run_dict = {\n",
    "                \"id\": train_folder,\n",
    "                **params_dict,\n",
    "                \"result\": result_records,\n",
    "                \"progress\": progress_records,\n",
    "            }\n",
    "            runs_data[model_folder][train_folder] = run_dict\n",
    "\n",
    "    return runs_data\n",
    "\n",
    "# Lade die Hyperparameter-Daten\n",
    "hyperparameter_data = load_hyperparameter_runs_as_dict(HYPER_FOLDER)\n",
    "\n",
    "# Sortiere die Runs pro Modell und extrahiere den Checkpoint des besten Runs\n",
    "sorted_hyperparameter_data = {}\n",
    "for model_name, runs_dict in hyperparameter_data.items():\n",
    "    runs_list = []\n",
    "    \n",
    "    for run_name, run_data in runs_dict.items():\n",
    "        # Bestimme die höchste Validierungsgenauigkeit aus der \"progress\"-Liste (falls vorhanden)\n",
    "        if \"progress\" in run_data and run_data[\"progress\"]:\n",
    "            best_val_acc = max((r.get(\"val_acc\", float(\"-inf\")) for r in run_data[\"progress\"]), default=float(\"-inf\"))\n",
    "        else:\n",
    "            best_val_acc = float(\"-inf\")\n",
    "        \n",
    "        # Absoluter Pfad zum Run-Ordner\n",
    "        run_folder_path = os.path.join(HYPER_FOLDER, model_name, run_name)\n",
    "        run_folder_path = os.path.abspath(run_folder_path)\n",
    "        \n",
    "        # Suche nach Checkpoint-Verzeichnissen in diesem Run-Ordner\n",
    "        if os.path.isdir(run_folder_path):\n",
    "            checkpoint_dirs = [\n",
    "                d for d in os.listdir(run_folder_path)\n",
    "                if d.startswith(\"checkpoint_\") and os.path.isdir(os.path.join(run_folder_path, d))\n",
    "            ]\n",
    "        else:\n",
    "            checkpoint_dirs = []\n",
    "        \n",
    "        # Wähle den letzten Checkpoint numerisch sortiert aus\n",
    "        if checkpoint_dirs:\n",
    "            checkpoint_dirs.sort(key=lambda x: int(x.split(\"_\")[1]))\n",
    "            last_checkpoint_dir = checkpoint_dirs[-1]\n",
    "            last_checkpoint = os.path.join(run_folder_path, last_checkpoint_dir, \"checkpoint.pkl\")\n",
    "            last_checkpoint = os.path.abspath(last_checkpoint)\n",
    "        else:\n",
    "            last_checkpoint = None\n",
    "        \n",
    "        runs_list.append((run_name, run_data, best_val_acc, last_checkpoint))\n",
    "    \n",
    "    # Sortiere Runs absteigend nach der Validierungsgenauigkeit\n",
    "    sorted_runs = sorted(runs_list, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Erstelle ein neues Dictionary mit fortlaufenden Schlüsseln (\"0\", \"1\", …)\n",
    "    sorted_hyperparameter_data[model_name] = {\n",
    "        str(i): {\n",
    "            \"max_validation_accuracy\": best_val_acc,\n",
    "            \"path\": last_checkpoint\n",
    "        }\n",
    "        for i, (run_name, run_data, best_val_acc, last_checkpoint) in enumerate(sorted_runs)\n",
    "    }\n",
    "\n",
    "# Debug: Ausgabe einiger Informationen\n",
    "print(\"Modelle mit extrahierten besten Checkpoints:\")\n",
    "for model_name, runs in sorted_hyperparameter_data.items():\n",
    "    best_checkpoint = runs.get(\"0\", {}).get(\"path\", \"Kein Checkpoint gefunden\")\n",
    "    print(f\"  {model_name}: {best_checkpoint}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpointed_model_ray(model_name, checkpoint_path, num_classes=None):\n",
    "    \"\"\"\n",
    "    Lädt ein Modell aus einem Checkpoint, wobei das Modell über MapillaryTrainedModel instanziiert wird.\n",
    "    Der Parameter 'skip_local_load' sorgt dafür, dass keine lokalen Gewichte geladen werden.\n",
    "    \"\"\"\n",
    "    # Instanziiere das Modell; passe 'width' und 'height' bei Bedarf an.\n",
    "    loaded_model = MapillaryTrainedModel(\n",
    "        model_name=model_name,\n",
    "        width=520,\n",
    "        height=520,\n",
    "        weights_name='',\n",
    "        skip_local_load=True  # WICHTIG!\n",
    "    )\n",
    "    # Lade den Checkpoint mit ray.cloudpickle (alias pickle)\n",
    "    with open(checkpoint_path, \"rb\") as fp:\n",
    "        checkpoint_data = pickle.load(fp)\n",
    "    # Lade die Modellgewichte\n",
    "    loaded_model.model.load_state_dict(checkpoint_data[\"model_state\"], strict=True)\n",
    "    # Falls vorhanden, lade auch den Optimizer-Zustand\n",
    "    if \"optimizer_state\" in checkpoint_data:\n",
    "        loaded_model.optimizer.load_state_dict(checkpoint_data[\"optimizer_state\"])\n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset loaded with 2000 samples.\n",
      "\n",
      "Evaluating model: fcn_resnet101\n",
      "Using checkpoint: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/fcn_resnet101/train_hyper_7adb91eb_1_auto_cast=True,batch_size=6,learning_rate=0.0001,max_epochs=100,weight_decay=0_2025-03-11_12-03-54/checkpoint_000099/checkpoint.pkl\n",
      "Using CUDA GPU\n",
      "Model loaded: fcn_resnet101 | Device: cuda \n",
      "Error loading Model with Epoch latest: Error(s) in loading state_dict for FCN:\n",
      "\tsize mismatch for classifier.4.weight: copying a param with shape torch.Size([20, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([124, 512, 1, 1]).\n",
      "\tsize mismatch for classifier.4.bias: copying a param with shape torch.Size([20]) from checkpoint, the shape in current model is torch.Size([124]).\n",
      "Skipping local .pth load due to error above.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Dataset: 100%|██████████| 2000/2000 [03:29<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix saved to: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/confusion_matrices_hyper_small/fcn_resnet101_confusion_matrix.pt\n",
      "\n",
      "Evaluating model: deeplabv3_resnet50\n",
      "Using checkpoint: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/deeplabv3_resnet50/train_hyper_a8ae06ed_1_auto_cast=True,batch_size=6,learning_rate=0.0001,max_epochs=100,weight_decay=0_2025-03-07_20-02-56/checkpoint_000099/checkpoint.pkl\n",
      "Using CUDA GPU\n",
      "Model loaded: deeplabv3_resnet50 | Device: cuda \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Dataset: 100%|██████████| 2000/2000 [02:53<00:00, 11.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix saved to: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/confusion_matrices_hyper_small/deeplabv3_resnet50_confusion_matrix.pt\n",
      "\n",
      "Evaluating model: fcn_resnet50\n",
      "Using checkpoint: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/fcn_resnet50/train_hyper_bc32f73a_1_auto_cast=True,batch_size=6,learning_rate=0.0001,max_epochs=100,weight_decay=0_2025-03-10_11-25-21/checkpoint_000099/checkpoint.pkl\n",
      "Using CUDA GPU\n",
      "Model loaded: fcn_resnet50 | Device: cuda \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Dataset: 100%|██████████| 2000/2000 [02:43<00:00, 12.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix saved to: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/confusion_matrices_hyper_small/fcn_resnet50_confusion_matrix.pt\n",
      "\n",
      "Evaluating model: deeplabv3_resnet101\n",
      "Using checkpoint: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/deeplabv3_resnet101/train_hyper_ae167f80_1_auto_cast=True,batch_size=6,learning_rate=0.0001,max_epochs=100,weight_decay=0_2025-03-09_00-01-13/checkpoint_000099/checkpoint.pkl\n",
      "Using CUDA GPU\n",
      "Model loaded: deeplabv3_resnet101 | Device: cuda \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Dataset: 100%|██████████| 2000/2000 [03:15<00:00, 10.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix saved to: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/confusion_matrices_hyper_small/deeplabv3_resnet101_confusion_matrix.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Laden des Datensatzes (hier verwenden wir den Validierungsdatensatz als Testset)\n",
    "mapillary_loader = MapillaryDataLoader(\n",
    "    train_images_dir=TRAIN_IMAGES_DIR,\n",
    "    train_annotations_dir=TRAIN_ANNOTATIONS_DIR,\n",
    "    val_images_dir=VAL_IMAGES_DIR,\n",
    "    val_annotations_dir=VAL_ANNOTATIONS_DIR\n",
    ")\n",
    "test_dataset = mapillary_loader.test_dataset\n",
    "print(f\"Test dataset loaded with {len(test_dataset)} samples.\")\n",
    "\n",
    "# Funktion zur Berechnung der Confusion Matrix für ein einzelnes Bild\n",
    "def compute_confusion_matrix(predicted, ground_truth, num_classes):\n",
    "    mask = (ground_truth >= 0) & (ground_truth < num_classes)\n",
    "    label = num_classes * ground_truth[mask] + predicted[mask]\n",
    "    count = torch.bincount(label, minlength=num_classes**2)\n",
    "    confusion_matrix = count.reshape(num_classes, num_classes)\n",
    "    return confusion_matrix\n",
    "\n",
    "# Evaluierungsfunktion, die über den gesamten Datensatz läuft, die Confusion Matrix aufsummiert und speichert\n",
    "def evaluate_and_store_confusion_matrix(model, dataset, num_classes, save_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.model.to(device)\n",
    "    confusion_matrix_total = torch.zeros((num_classes, num_classes), dtype=torch.int64).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(dataset)), desc=\"Evaluating Dataset\"):\n",
    "            image, annotation = dataset[i]\n",
    "            image = image.to(device)\n",
    "            annotation = annotation.to(device)\n",
    "            output = model.inference(image)\n",
    "            predicted = output.argmax(1).squeeze(0)\n",
    "            conf_mat = compute_confusion_matrix(predicted.cpu(), annotation.cpu(), num_classes)\n",
    "            confusion_matrix_total += conf_mat.to(device)\n",
    "    \n",
    "    torch.save(confusion_matrix_total, save_path)\n",
    "    print(f\"Confusion Matrix saved to: {save_path}\")\n",
    "\n",
    "# Iteriere über alle Modelle aus sorted_hyperparameter_data und berechne deren Confusion Matrix\n",
    "for model_name, runs_dict in sorted_hyperparameter_data.items():\n",
    "    best_run_info = runs_dict.get(\"0\", {})\n",
    "    checkpoint_path = best_run_info.get(\"path\", None)\n",
    "    \n",
    "    if not checkpoint_path or not os.path.isfile(checkpoint_path):\n",
    "        print(f\"[WARNING] No valid checkpoint for {model_name}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    print(f\"Using checkpoint: {checkpoint_path}\")\n",
    "    \n",
    "    try:\n",
    "        model_loaded = load_checkpointed_model_ray(model_name, checkpoint_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Could not load model {model_name}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Definiere den Speicherpfad für die Confusion Matrix\n",
    "    conf_matrix_save_path = os.path.join(CONF_MATRIX_DIR, f\"{model_name}_confusion_matrix.pt\")\n",
    "    evaluate_and_store_confusion_matrix(model_loaded, test_dataset, NUM_CLASSES_ORIGINAL, conf_matrix_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing all saved Confusion Matrix files in: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/confusion_matrices_hyper_small\n",
      "Model: fcn_resnet101, Confusion Matrix shape: torch.Size([124, 124])\n",
      "Model: deeplabv3_resnet101, Confusion Matrix shape: torch.Size([124, 124])\n",
      "Model: deeplabv3_resnet50, Confusion Matrix shape: torch.Size([124, 124])\n",
      "Model: fcn_resnet50, Confusion Matrix shape: torch.Size([124, 124])\n",
      "Summary saved to: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/confusion_matrices_hyper_small/confusion_matrices_summary.json\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 5: Summary of Saved Confusion Matrices\n",
    "import glob\n",
    "\n",
    "print(\"Listing all saved Confusion Matrix files in:\", CONF_MATRIX_DIR)\n",
    "conf_files = glob.glob(os.path.join(CONF_MATRIX_DIR, \"*_confusion_matrix.pt\"))\n",
    "summary = {}\n",
    "for file in conf_files:\n",
    "    cm = torch.load(file)\n",
    "    shape = cm.shape\n",
    "    model_name = os.path.basename(file).split(\"_confusion_matrix.pt\")[0]\n",
    "    summary[model_name] = {\"path\": file, \"shape\": shape}\n",
    "    print(f\"Model: {model_name}, Confusion Matrix shape: {shape}\")\n",
    "\n",
    "# Optionally, save summary to JSON file for reference\n",
    "summary_save_path = os.path.join(CONF_MATRIX_DIR, \"confusion_matrices_summary.json\")\n",
    "with open(summary_save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    # Konvertiere die Shape in eine Liste, damit es JSON-kompatibel ist\n",
    "    json.dump({k: {\"path\": v[\"path\"], \"shape\": list(v[\"shape\"])} for k, v in summary.items()}, f, indent=4)\n",
    "\n",
    "print(f\"Summary saved to: {summary_save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
