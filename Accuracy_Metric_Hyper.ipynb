{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subfolder name: lraspp_mobilenet_v3_large\n",
      "Subfolder name: fcn_resnet101\n",
      "Subfolder name: deeplabv3_resnet50\n",
      "Subfolder name: fcn_resnet50\n",
      "Subfolder name: deeplabv3_mobilenet_v3_large\n",
      "Subfolder name: deeplabv3_resnet101\n"
     ]
    }
   ],
   "source": [
    "main_folder_hyper = \"HyperparameterLOG\"\n",
    "\n",
    "for folder_name in os.listdir(main_folder_hyper):\n",
    "    folder_path = os.path.join(main_folder_hyper, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f\"Subfolder name: {folder_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping folder 'train_hyper_659805fc_2_auto_cast=True,batch_size=8,learning_rate=0.0040,max_epochs=100,weight_decay=0.0076_2025-01-21_10-27-28' due to missing files.\n",
      "Skipping folder 'train_hyper_457ebcd4_2_auto_cast=True,batch_size=8,learning_rate=0.0040,max_epochs=100,weight_decay=0.0076_2025-01-16_10-24-54' due to missing files.\n"
     ]
    }
   ],
   "source": [
    "def load_hyperparameter_runs_as_dict(base_folder: str):\n",
    "    \"\"\"\n",
    "    Lädt die Hyperparameter-Runs und organisiert sie nach Modellen.\n",
    "    \"\"\"\n",
    "    runs_data = {}\n",
    "\n",
    "    # Durchlaufe alle Modellordner\n",
    "    for model_folder in os.listdir(base_folder):\n",
    "        model_path = os.path.join(base_folder, model_folder, \"Hyperparameter_Tuning_Deeplabv3\")\n",
    "        \n",
    "        # Überspringe, wenn der Pfad ungültig ist\n",
    "        if not os.path.isdir(model_path):\n",
    "            print(f\"Skipping invalid model path: {model_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Initialisiere die Modellgruppe\n",
    "        if model_folder not in runs_data:\n",
    "            runs_data[model_folder] = {}\n",
    "\n",
    "        # Durchlaufe alle 'train_hyper_*'-Ordner innerhalb des Modells\n",
    "        for train_folder in os.listdir(model_path):\n",
    "            train_folder_path = os.path.join(model_path, train_folder)\n",
    "            \n",
    "            # Prüfe, ob es sich um einen 'train_hyper_*'-Ordner handelt\n",
    "            if not (os.path.isdir(train_folder_path) and train_folder.startswith(\"train_hyper_\")):\n",
    "                continue\n",
    "            \n",
    "            # Pfade zu den benötigten Dateien\n",
    "            params_file = os.path.join(train_folder_path, \"params.json\")\n",
    "            progress_file = os.path.join(train_folder_path, \"progress.csv\")\n",
    "            result_file = os.path.join(train_folder_path, \"result.json\")\n",
    "            \n",
    "            # Debugging: Drucke die überprüften Pfade\n",
    "            #print(f\"Checking files in: {train_folder_path}\")\n",
    "            #print(f\"  params.json: {'Exists' if os.path.isfile(params_file) else 'Missing'}\")\n",
    "            #print(f\"  progress.csv: {'Exists' if os.path.isfile(progress_file) else 'Missing'}\")\n",
    "            #print(f\"  result.json: {'Exists' if os.path.isfile(result_file) else 'Missing'}\")\n",
    "            \n",
    "            # Überprüfe, ob alle Dateien vorhanden sind\n",
    "            if not (\n",
    "                os.path.isfile(params_file)\n",
    "                and os.path.isfile(progress_file)\n",
    "                and os.path.isfile(result_file)\n",
    "            ):\n",
    "                print(f\"Skipping folder '{train_folder}' due to missing files.\")\n",
    "                continue\n",
    "            \n",
    "            # Lade params.json\n",
    "            with open(params_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                params_dict = json.load(f)\n",
    "            \n",
    "            # Lade progress.csv\n",
    "            progress_df = pd.read_csv(progress_file)\n",
    "            progress_records = progress_df.to_dict(orient=\"records\")\n",
    "            \n",
    "            # Lade result.json\n",
    "            result_records = []\n",
    "            with open(result_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        result_records.append(json.loads(line))\n",
    "            \n",
    "            # Erstelle das Dictionary für diesen Lauf\n",
    "            run_dict = {\n",
    "                \"id\": train_folder,  # Der Name des 'train_hyper_*'-Ordners\n",
    "                **params_dict,  # Füge alle Parameter aus params.json hinzu\n",
    "                \"result\": result_records,  # Ergebnisse aus result.json\n",
    "                \"progress\": progress_records,  # Fortschritte aus progress.csv\n",
    "            }\n",
    "            \n",
    "            # Speichere die Daten unter dem Modellnamen und train_hyper_*\n",
    "            runs_data[model_folder][train_folder] = run_dict\n",
    "\n",
    "    return runs_data\n",
    "\n",
    "\n",
    "# Nutzung\n",
    "main_folder_hyper = \"HyperparameterLOG\"\n",
    "\n",
    "# Rufe die Funktion auf\n",
    "hyperparameter_data = load_hyperparameter_runs_as_dict(main_folder_hyper)\n",
    "\n",
    "# print(len(hyperparameter_data.keys()))\n",
    "# print(hyperparameter_data[\"deeplabv3_mobilenet_v3_large\"].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " lraspp_mobilenet_v3_large\n",
      "90.23178158656506\n",
      "90.08631829743304\n",
      "89.88125971116598\n",
      "\n",
      " deeplabv3_resnet50\n",
      "91.306187742114\n",
      "91.26811930633008\n",
      "91.2159976161083\n",
      "\n",
      " fcn_resnet50\n",
      "91.02365533608616\n",
      "90.8788871269848\n",
      "90.56566876676172\n",
      "\n",
      " deeplabv3_mobilenet_v3_large\n",
      "89.87057476799626\n",
      "89.85961711400111\n",
      "89.78379219701162\n",
      "\n",
      " deeplabv3_resnet101\n",
      "91.31428991422248\n",
      "91.26546602954323\n",
      "91.22656150227746\n"
     ]
    }
   ],
   "source": [
    "# Neuer Dictionary für sortierte Daten\n",
    "sorted_hyperparameter_data = {}\n",
    "\n",
    "# Iteriere über jedes Modell\n",
    "for model_name, runs in hyperparameter_data.items():\n",
    "    # Extrahiere die Runs und deren Validation Accuracy\n",
    "    runs_list = []\n",
    "    for run_name, run_data in runs.items():\n",
    "        # Finde den besten Validation Accuracy-Wert im 'progress'\n",
    "        if \"progress\" in run_data and run_data[\"progress\"]:\n",
    "            best_val_acc = max(\n",
    "                (record.get(\"val_acc\", float(\"-inf\")) for record in run_data[\"progress\"]),\n",
    "                default=float(\"-inf\")\n",
    "            )\n",
    "        else:\n",
    "            best_val_acc = float(\"-inf\")\n",
    "        \n",
    "        # Bestimme den Pfad zum letzten Checkpoint\n",
    "        run_folder_path = os.path.join(\n",
    "            \"HyperparameterLOG\", model_name, \"Hyperparameter_Tuning_Deeplabv3\", run_name\n",
    "        )\n",
    "        checkpoint_dirs = [\n",
    "            d for d in os.listdir(run_folder_path)\n",
    "            if d.startswith(\"checkpoint_\") and os.path.isdir(os.path.join(run_folder_path, d))\n",
    "        ]\n",
    "        \n",
    "        if checkpoint_dirs:\n",
    "            # Sortiere die Checkpoints numerisch und finde den letzten\n",
    "            checkpoint_dirs.sort(key=lambda x: int(x.split(\"_\")[1]))\n",
    "            last_checkpoint = os.path.join(run_folder_path, checkpoint_dirs[-1], \"checkpoint.pkl\")\n",
    "        else:\n",
    "            last_checkpoint = None  # Kein Checkpoint verfügbar\n",
    "        \n",
    "        # Füge den Run und seine Validation Accuracy zur Liste hinzu\n",
    "        runs_list.append((run_name, run_data, best_val_acc, last_checkpoint))\n",
    "    \n",
    "    # Sortiere die Runs basierend auf der Validation Accuracy absteigend\n",
    "    sorted_runs = sorted(runs_list, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Erstelle ein neues Dictionary mit neuen Keys (0 bis n-1)\n",
    "    sorted_hyperparameter_data[model_name] = {\n",
    "        str(i): {\n",
    "            **run_data,\n",
    "            \"max_validation_accuracy\": best_val_acc,\n",
    "            \"path\": last_checkpoint\n",
    "        }\n",
    "        for i, (run_name, run_data, best_val_acc, last_checkpoint) in enumerate(sorted_runs)\n",
    "    }\n",
    "\n",
    "# # Überprüfung der Struktur\n",
    "# print(sorted_hyperparameter_data[\"deeplabv3_mobilenet_v3_large\"][\"0\"].keys())\n",
    "\n",
    "# # Überprüfung des Typs von 'result'\n",
    "# print(type(sorted_hyperparameter_data[\"deeplabv3_mobilenet_v3_large\"][\"0\"][\"result\"]))\n",
    "\n",
    "# # Überprüfung der max_validation_accuracy\n",
    "# print(sorted_hyperparameter_data[\"deeplabv3_mobilenet_v3_large\"][\"0\"][\"max_validation_accuracy\"])\n",
    "\n",
    "# print(sorted_hyperparameter_data.keys())\n",
    "\n",
    "# for i in range(len(sorted_hyperparameter_data[\"deeplabv3_resnet50\"].keys())):\n",
    "#     print(sorted_hyperparameter_data[\"deeplabv3_mobilenet_v3_large\"][str(i)][\"max_validation_accuracy\"])\n",
    "\n",
    "\n",
    "for key in sorted_hyperparameter_data.keys():\n",
    "    if key == \"fcn_resnet101\":\n",
    "        continue\n",
    "    print(f'\\n {key}')\n",
    "    for i in range(3):\n",
    "        print(sorted_hyperparameter_data[key][str(i)][\"max_validation_accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'auto_cast', 'batch_size', 'learning_rate', 'max_epochs', 'weight_decay', 'result', 'progress', 'max_validation_accuracy', 'path'])\n"
     ]
    }
   ],
   "source": [
    "print(sorted_hyperparameter_data[\"deeplabv3_resnet50\"][\"0\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-21 15:38:44.038831: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-21 15:38:44.691596: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ray.cloudpickle as pickle  # important for reading the checkpoint.pkl\n",
    "import pandas as pd\n",
    "\n",
    "from Helper.ml_models import TrainedModel, K_Fold_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: DEFINING YOUR METRIC COMPUTATION FUNCTIONS\n",
    "# --------------------------------------------------\n",
    "# We'll reuse your previously defined metrics exactly as given:\n",
    "\n",
    "def compute_confusion_matrix(predicted, ground_truth, num_classes):\n",
    "    mask = (ground_truth >= 0) & (ground_truth < num_classes)\n",
    "    label = num_classes * ground_truth[mask] + predicted[mask]\n",
    "    count = torch.bincount(label, minlength=num_classes**2)\n",
    "    confusion_matrix = count.reshape(num_classes, num_classes)\n",
    "    return confusion_matrix\n",
    "\n",
    "def compute_miou(confusion_matrix):\n",
    "    intersection = torch.diag(confusion_matrix)\n",
    "    ground_truth_set = confusion_matrix.sum(1)\n",
    "    predicted_set = confusion_matrix.sum(0)\n",
    "    union = ground_truth_set + predicted_set - intersection\n",
    "    IoU = intersection / (union + 1e-6)  # Avoid division by zero\n",
    "    mIoU = torch.mean(IoU)\n",
    "    return mIoU.item(), IoU\n",
    "\n",
    "def compute_mean_pixel_accuracy(confusion_matrix):\n",
    "    true_positive = torch.diag(confusion_matrix)\n",
    "    total_pixels = confusion_matrix.sum(1)\n",
    "    pixel_accuracy = true_positive / (total_pixels + 1e-6)\n",
    "    mPA = torch.mean(pixel_accuracy)\n",
    "    return mPA.item(), pixel_accuracy\n",
    "\n",
    "def compute_fwiou(confusion_matrix):\n",
    "    total_pixels = confusion_matrix.sum()\n",
    "    ground_truth_set = confusion_matrix.sum(1)\n",
    "    intersection = torch.diag(confusion_matrix)\n",
    "    union = ground_truth_set + confusion_matrix.sum(0) - intersection\n",
    "    IoU = intersection / (union + 1e-6)\n",
    "    FWIoU = (ground_truth_set * IoU) / total_pixels\n",
    "    FWIoU = FWIoU.sum()\n",
    "    return FWIoU.item()\n",
    "\n",
    "def compute_dice_coefficient(confusion_matrix):\n",
    "    intersection = torch.diag(confusion_matrix)\n",
    "    ground_truth_set = confusion_matrix.sum(1)\n",
    "    predicted_set = confusion_matrix.sum(0)\n",
    "    dice = (2 * intersection) / (ground_truth_set + predicted_set + 1e-6)\n",
    "    mean_dice = torch.mean(dice)\n",
    "    return mean_dice.item(), dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: FUNCTION TO LOAD A CHECKPOINT USING RAY.CLOUDPICKLE\n",
    "# -----------------------------------------------------------\n",
    "# This is the key part: your hyperparameter search used ray.cloudpickle to store\n",
    "# checkpoint.pkl files. Below, we replicate how you loaded them in your training loop.\n",
    "\n",
    "def load_checkpointed_model_ray(model_name: str, checkpoint_path: str) -> TrainedModel:\n",
    "    \"\"\"\n",
    "    Loads a model from the given Ray Tune 'checkpoint.pkl' (created with ray.cloudpickle).\n",
    "    It returns an instance of TrainedModel with its model state_dict restored.\n",
    "\n",
    "    :param model_name: The string identifying which segmentation model to instantiate \n",
    "                       (e.g., 'deeplabv3_resnet101', etc.).\n",
    "    :param checkpoint_path: Full path to the 'checkpoint.pkl' file produced by Ray Tune.\n",
    "    :return: A TrainedModel object with the loaded state.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found at: {checkpoint_path}\")\n",
    "\n",
    "    # 1) Create the TrainedModel object (as you did in your training code).\n",
    "    #    Adjust the constructor arguments if needed. \n",
    "    #    (Here we use placeholders for image size & folder_path, which you can adapt.)\n",
    "    loaded_model = TrainedModel(\n",
    "        model_name=model_name,\n",
    "        height=2048,\n",
    "        width=1024,\n",
    "        weights_name=\"\",  # Not relevant if we load from checkpoint\n",
    "        folder_path=\"\",   # Not relevant here\n",
    "        start_epoch=\"latest\", \n",
    "        skip_local_load=True\n",
    "    )\n",
    "\n",
    "    # 2) Load the checkpoint using ray.cloudpickle\n",
    "    with open(checkpoint_path, \"rb\") as fp:\n",
    "        checkpoint_data = pickle.load(fp)\n",
    "\n",
    "    # 3) Restore the model and optimizer states\n",
    "    #    (If you only need the model for inference, you don't have to restore the optimizer.)\n",
    "    loaded_model.model.load_state_dict(checkpoint_data[\"model_state\"])\n",
    "    if \"optimizer_state\" in checkpoint_data:\n",
    "        loaded_model.optimizer.load_state_dict(checkpoint_data[\"optimizer_state\"])\n",
    "    \n",
    "    # 4) Switch to evaluation mode\n",
    "    loaded_model.model.eval()\n",
    "    \n",
    "    # Optionally, return the checkpointed epoch if you need it:\n",
    "    # epoch = checkpoint_data.get(\"epoch\", 0)\n",
    "    \n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data leaks found.\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: PREPARE (OR LOAD) THE DATASET FOR EVALUATION\n",
    "# ----------------------------------------------------\n",
    "# We'll assume you have a K_Fold_Dataset or similar dataset for evaluating your model.\n",
    "\n",
    "k_fold_dataset_eval = K_Fold_Dataset(\n",
    "    image_dir=\"CityscapesDaten/images\",\n",
    "    annotation_dir=\"CityscapesDaten/semantic\",\n",
    "    k_fold_csv_dir=\"Daten/CityscapesDaten\",\n",
    "    leave_out_fold=0\n",
    ")\n",
    "k_fold_dataset_eval.check_for_data_leaks()\n",
    "\n",
    "# For the final evaluation, let's assume we use the \"test_dataset\"\n",
    "test_dataset = k_fold_dataset_eval.test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: DEFINE AN EVALUATION FUNCTION\n",
    "# -------------------------------------\n",
    "# This function runs inference over the test dataset and computes your desired metrics.\n",
    "\n",
    "def evaluate_model(model: TrainedModel, dataset, num_classes: int) -> dict:\n",
    "    \"\"\"\n",
    "    Runs inference on the given dataset and computes segmentation metrics.\n",
    "    Returns a dictionary with these metrics.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.model.to(device)\n",
    "    \n",
    "    confusion_matrix_total = torch.zeros((num_classes, num_classes), dtype=torch.int64)\n",
    "    \n",
    "    # Inference loop\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            image, annotation = dataset[i]\n",
    "            \n",
    "            # Move data to device\n",
    "            image = image.to(device)\n",
    "            annotation = annotation.to(device)\n",
    "            \n",
    "            # Model inference -> shape [1, num_classes, H, W]\n",
    "            output = model.inference(image)\n",
    "            predicted = output.argmax(1).squeeze(0)\n",
    "            \n",
    "            # Update confusion matrix\n",
    "            conf_mat = compute_confusion_matrix(predicted.cpu(), annotation.cpu(), num_classes)\n",
    "            confusion_matrix_total += conf_mat\n",
    "    \n",
    "    # Compute the final metrics\n",
    "    miou, iou_per_class = compute_miou(confusion_matrix_total)\n",
    "    mpa, pa_per_class = compute_mean_pixel_accuracy(confusion_matrix_total)\n",
    "    fwiou = compute_fwiou(confusion_matrix_total)\n",
    "    dice_mean, dice_per_class = compute_dice_coefficient(confusion_matrix_total)\n",
    "    \n",
    "    metrics = {\n",
    "        \"mIoU\": miou,\n",
    "        \"mPA\": mpa,\n",
    "        \"FWIoU\": fwiou,\n",
    "        \"Dice_Mean\": dice_mean,\n",
    "        \"IoU_per_class\": iou_per_class.tolist(),\n",
    "        \"PA_per_class\": pa_per_class.tolist(),\n",
    "        \"Dice_per_class\": dice_per_class.tolist()\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: lraspp_mobilenet_v3_large\n",
      "Checkpoint path: HyperparameterLOG/lraspp_mobilenet_v3_large/Hyperparameter_Tuning_Deeplabv3/train_hyper_f045fc27_12_auto_cast=True,batch_size=8,learning_rate=0.0008,max_epochs=100,weight_decay=0.0000_2025-01-19_15-06-55/checkpoint_000099/checkpoint.pkl\n",
      "Using CUDA GPU\n",
      "Model loaded: lraspp_mobilenet_v3_large | Device: cuda \n",
      "Checkpoint file not found at HyperparameterLOG/lraspp_mobilenet_v3_large/Hyperparameter_Tuning_Deeplabv3/train_hyper_f045fc27_12_auto_cast=True,batch_size=8,learning_rate=0.0008,max_epochs=100,weight_decay=0.0000_2025-01-19_15-06-55/checkpoint_000099/checkpoint.pkl. Skipping.\n",
      "\n",
      "Evaluating model: deeplabv3_resnet50\n",
      "Checkpoint path: HyperparameterLOG/deeplabv3_resnet50/Hyperparameter_Tuning_Deeplabv3/train_hyper_339081a1_34_auto_cast=True,batch_size=8,learning_rate=0.0001,max_epochs=100,weight_decay=0.0001_2025-01-15_14-54-45/checkpoint_000099/checkpoint.pkl\n",
      "Using CUDA GPU\n",
      "Model loaded: deeplabv3_resnet50 | Device: cuda \n",
      "Checkpoint file not found at HyperparameterLOG/deeplabv3_resnet50/Hyperparameter_Tuning_Deeplabv3/train_hyper_339081a1_34_auto_cast=True,batch_size=8,learning_rate=0.0001,max_epochs=100,weight_decay=0.0001_2025-01-15_14-54-45/checkpoint_000099/checkpoint.pkl. Skipping.\n",
      "\n",
      "Evaluating model: fcn_resnet50\n",
      "Checkpoint path: HyperparameterLOG/fcn_resnet50/Hyperparameter_Tuning_Deeplabv3/train_hyper_324adb25_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-01-20_10-35-40/checkpoint_000099/checkpoint.pkl\n",
      "Using CUDA GPU\n",
      "Model loaded: fcn_resnet50 | Device: cuda \n",
      "Checkpoint file not found at HyperparameterLOG/fcn_resnet50/Hyperparameter_Tuning_Deeplabv3/train_hyper_324adb25_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-01-20_10-35-40/checkpoint_000099/checkpoint.pkl. Skipping.\n",
      "\n",
      "Evaluating model: deeplabv3_mobilenet_v3_large\n",
      "Checkpoint path: HyperparameterLOG/deeplabv3_mobilenet_v3_large/Hyperparameter_Tuning_Deeplabv3/train_hyper_2dd3ea95_23_auto_cast=True,batch_size=8,learning_rate=0.0006,max_epochs=100,weight_decay=0.0000_2025-01-18_20-26-43/checkpoint_000099/checkpoint.pkl\n",
      "Using CUDA GPU\n",
      "Model loaded: deeplabv3_mobilenet_v3_large | Device: cuda \n",
      "Checkpoint file not found at HyperparameterLOG/deeplabv3_mobilenet_v3_large/Hyperparameter_Tuning_Deeplabv3/train_hyper_2dd3ea95_23_auto_cast=True,batch_size=8,learning_rate=0.0006,max_epochs=100,weight_decay=0.0000_2025-01-18_20-26-43/checkpoint_000099/checkpoint.pkl. Skipping.\n",
      "\n",
      "Evaluating model: deeplabv3_resnet101\n",
      "Checkpoint path: HyperparameterLOG/deeplabv3_resnet101/Hyperparameter_Tuning_Deeplabv3/train_hyper_d44efb15_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-01-16_10-31-56/checkpoint_000099/checkpoint.pkl\n",
      "Using CUDA GPU\n",
      "Model loaded: deeplabv3_resnet101 | Device: cuda \n",
      "Checkpoint file not found at HyperparameterLOG/deeplabv3_resnet101/Hyperparameter_Tuning_Deeplabv3/train_hyper_d44efb15_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-01-16_10-31-56/checkpoint_000099/checkpoint.pkl. Skipping.\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: USE YOUR 'sorted_hyperparameter_data' DICTIONARY\n",
    "# ---------------------------------------------------------\n",
    "# We'll assume that 'sorted_hyperparameter_data' is exactly as you described:\n",
    "# {\n",
    "#   \"deeplabv3_mobilenet_v3_large\": {\n",
    "#       \"0\": {\n",
    "#           \"path\": \".../checkpoint.pkl\",\n",
    "#           \"max_validation_accuracy\": ...,\n",
    "#           ...\n",
    "#       },\n",
    "#       \"1\": { ... },\n",
    "#       ...\n",
    "#   },\n",
    "#   \"deeplabv3_resnet50\": { ... },\n",
    "#   ...\n",
    "# }\n",
    "#\n",
    "# We'll now iterate through the dictionary, load each model checkpoint, and evaluate it.\n",
    "\n",
    "# Suppose you have:\n",
    "# NUM_CLASSES in Cityscapes typically is 19 or 20\n",
    "NUM_CLASSES = 20\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "for model_name, runs_dict in sorted_hyperparameter_data.items():\n",
    "    # If you want to skip certain models (like \"fcn_resnet101\"), do it here:\n",
    "    if model_name == \"fcn_resnet101\":\n",
    "        continue\n",
    "    \n",
    "    # We'll evaluate just the best run (key = \"0\") for demonstration,\n",
    "    # but you can easily loop over runs_dict.keys() to evaluate all.\n",
    "    best_run_info = runs_dict[\"0\"]  # Best run is at index \"0\"\n",
    "    checkpoint_path = best_run_info.get(\"path\", None)\n",
    "    if not checkpoint_path:\n",
    "        print(f\"No checkpoint path found for {model_name} in run '0'. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    print(f\"Checkpoint path: {checkpoint_path}\")\n",
    "    \n",
    "    # STEP 6a: Load the model\n",
    "    try:\n",
    "        model_loaded = load_checkpointed_model_ray(model_name, checkpoint_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Checkpoint file not found at {checkpoint_path}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # STEP 6b: Evaluate the model\n",
    "    metrics = evaluate_model(model_loaded, test_dataset, NUM_CLASSES)\n",
    "    \n",
    "    # STEP 6c: Print or store the results\n",
    "    evaluation_results[model_name] = metrics\n",
    "    print(f\"Results for {model_name}:\")\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, list):\n",
    "            # For lists (per-class metrics), you might just show length:\n",
    "            print(f\"  {k}: [list of length {len(v)}]\")\n",
    "        else:\n",
    "            print(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
