{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2025-03-04 12:16:06.703947: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-04 12:16:07.250703: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ray.cloudpickle as pickle\n",
    "from Helper.ml_models import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subfolder name: fcn_resnet101\n",
      "Subfolder name: deeplabv3_resnet50\n",
      "Subfolder name: fcn_resnet50\n",
      "Subfolder name: deeplabv3_resnet101\n"
     ]
    }
   ],
   "source": [
    "main_folder_hyper = \"HyperparameterLOG\"\n",
    "\n",
    "for folder_name in os.listdir(main_folder_hyper):\n",
    "    folder_path = os.path.join(main_folder_hyper, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f\"Subfolder name: {folder_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: main_folder_hyper = /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG\n",
      "DEBUG: Exists on disk?   = True\n",
      "\n",
      "DEBUG: Subfolders in HyperparameterLOG:\n",
      " - fcn_resnet101\n",
      " - deeplabv3_resnet50\n",
      " - fcn_resnet50\n",
      " - deeplabv3_resnet101\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/fcn_resnet101/tuner.pkl\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/fcn_resnet101/search_gen_state-2025-02-22_22-23-44.json\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/fcn_resnet101/searcher-state-2025-02-22_22-23-44.pkl\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/fcn_resnet101/.validate_storage_marker\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/fcn_resnet101/experiment_state-2025-02-22_22-23-44.json\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet50/tuner.pkl\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet50/searcher-state-2025-02-16_19-30-49.pkl\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet50/experiment_state-2025-02-16_19-30-49.json\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet50/.validate_storage_marker\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet50/search_gen_state-2025-02-16_19-30-49.json\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/fcn_resnet50/tuner.pkl\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/fcn_resnet50/searcher-state-2025-02-20_17-08-56.pkl\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/fcn_resnet50/experiment_state-2025-02-20_17-08-56.json\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/fcn_resnet50/search_gen_state-2025-02-20_17-08-56.json\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/fcn_resnet50/.validate_storage_marker\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet101/tuner.pkl\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet101/search_gen_state-2025-02-19_04-23-10.json\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet101/experiment_state-2025-02-19_04-23-10.json\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet101/searcher-state-2025-02-19_04-23-10.pkl\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet101/.validate_storage_marker\n"
     ]
    }
   ],
   "source": [
    "# Suchen von Checkpoints in der Ordnerstruktur\n",
    "BASE_PATH = \"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation\"\n",
    "# Construct absolute path to your HyperparameterLOG\n",
    "main_folder_hyper = os.path.join(BASE_PATH, \"HyperparameterLOG\")\n",
    "main_folder_hyper = os.path.abspath(main_folder_hyper)\n",
    "print(\"DEBUG: main_folder_hyper =\", main_folder_hyper)\n",
    "print(\"DEBUG: Exists on disk?   =\", os.path.isdir(main_folder_hyper))\n",
    "\n",
    "# (Optional) Print top-level subfolders for confirmation:\n",
    "print(\"\\nDEBUG: Subfolders in HyperparameterLOG:\")\n",
    "for item in os.listdir(main_folder_hyper):\n",
    "    if os.path.isdir(os.path.join(main_folder_hyper, item)):\n",
    "        print(\" -\", item)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) LOAD HYPERPARAMETER RUNS\n",
    "# -----------------------------\n",
    "def load_hyperparameter_runs_as_dict(base_folder: str):\n",
    "    runs_data = {}\n",
    "\n",
    "    if not os.path.isdir(base_folder):\n",
    "        print(f\"ERROR: Base folder does not exist: {base_folder}\")\n",
    "        return runs_data\n",
    "\n",
    "    for model_folder in os.listdir(base_folder):\n",
    "        model_path = os.path.join(base_folder, model_folder)\n",
    "        \n",
    "        if not os.path.isdir(model_path):\n",
    "            print(f\"Skipping invalid model path: {model_path}\")\n",
    "            continue\n",
    "\n",
    "        runs_data[model_folder] = {}\n",
    "\n",
    "        for train_folder in os.listdir(model_path):\n",
    "            train_folder_path = os.path.join(model_path, train_folder)\n",
    "\n",
    "            # Sicherstellen, dass es sich um einen gÃ¼ltigen Trainingsordner handelt\n",
    "            if not train_folder.startswith(\"train_hyper_\") or not os.path.isdir(train_folder_path):\n",
    "                print(f\"Skipping non-training folder: {train_folder_path}\")\n",
    "                continue\n",
    "\n",
    "            # Check if necessary files exist\n",
    "            params_file = os.path.join(train_folder_path, \"params.json\")\n",
    "            progress_file = os.path.join(train_folder_path, \"progress.csv\")\n",
    "            result_file = os.path.join(train_folder_path, \"result.json\")\n",
    "\n",
    "            if not (os.path.isfile(params_file) and os.path.isfile(progress_file) and os.path.isfile(result_file)):\n",
    "                print(f\"Skipping incomplete run: {train_folder_path}\")\n",
    "                continue\n",
    "\n",
    "            # Laden der Dateien\n",
    "            with open(params_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                params_dict = json.load(f)\n",
    "\n",
    "            progress_df = pd.read_csv(progress_file)\n",
    "            progress_records = progress_df.to_dict(orient=\"records\")\n",
    "\n",
    "            result_records = []\n",
    "            with open(result_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        result_records.append(json.loads(line))\n",
    "\n",
    "            run_dict = {\n",
    "                \"id\": train_folder,\n",
    "                **params_dict,\n",
    "                \"result\": result_records,\n",
    "                \"progress\": progress_records,\n",
    "            }\n",
    "\n",
    "            runs_data[model_folder][train_folder] = run_dict\n",
    "\n",
    "    return runs_data\n",
    "\n",
    "# Neuladen der Daten mit verbesserter Fehlerbehandlung\n",
    "hyperparameter_data = load_hyperparameter_runs_as_dict(main_folder_hyper)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] For model='fcn_resnet101' run='train_hyper_c75247d6_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-02-22_22-23-45', found checkpoint dirs:\n",
      "        ['checkpoint_000079', 'checkpoint_000099', 'checkpoint_000068', 'checkpoint_000091', 'checkpoint_000074', 'checkpoint_000075']\n",
      "\n",
      "[DEBUG] For model='fcn_resnet101' run='train_hyper_06021618_2_auto_cast=True,batch_size=8,learning_rate=0.0000,max_epochs=100,weight_decay=0.0007_2025-02-22_22-24-09', found checkpoint dirs:\n",
      "        []\n",
      "\n",
      "[DEBUG] For model='deeplabv3_resnet50' run='train_hyper_56ebb659_2_auto_cast=True,batch_size=8,learning_rate=0.0000,max_epochs=100,weight_decay=0.0007_2025-02-16_19-30-53', found checkpoint dirs:\n",
      "        ['checkpoint_000099', 'checkpoint_000095', 'checkpoint_000084', 'checkpoint_000092', 'checkpoint_000089', 'checkpoint_000069']\n",
      "\n",
      "[DEBUG] For model='deeplabv3_resnet50' run='train_hyper_430aff19_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-02-16_19-30-49', found checkpoint dirs:\n",
      "        ['checkpoint_000096', 'checkpoint_000099', 'checkpoint_000095', 'checkpoint_000084', 'checkpoint_000094', 'checkpoint_000098']\n",
      "\n",
      "[DEBUG] For model='fcn_resnet50' run='train_hyper_4b822bcf_2_auto_cast=True,batch_size=8,learning_rate=0.0000,max_epochs=100,weight_decay=0.0007_2025-02-20_17-09-00', found checkpoint dirs:\n",
      "        ['checkpoint_000099', 'checkpoint_000084', 'checkpoint_000067', 'checkpoint_000063', 'checkpoint_000083', 'checkpoint_000098']\n",
      "\n",
      "[DEBUG] For model='fcn_resnet50' run='train_hyper_dee6a3b4_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-02-20_17-08-56', found checkpoint dirs:\n",
      "        ['checkpoint_000065', 'checkpoint_000099', 'checkpoint_000067', 'checkpoint_000066', 'checkpoint_000094', 'checkpoint_000097']\n",
      "\n",
      "[DEBUG] For model='deeplabv3_resnet101' run='train_hyper_791244e2_2_auto_cast=True,batch_size=8,learning_rate=0.0000,max_epochs=100,weight_decay=0.0007_2025-02-19_04-23-24', found checkpoint dirs:\n",
      "        []\n",
      "\n",
      "[DEBUG] For model='deeplabv3_resnet101' run='train_hyper_0951e564_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-02-19_04-23-11', found checkpoint dirs:\n",
      "        ['checkpoint_000096', 'checkpoint_000093', 'checkpoint_000099', 'checkpoint_000067', 'checkpoint_000078', 'checkpoint_000071']\n"
     ]
    }
   ],
   "source": [
    "# Finden der Checkpoints\n",
    "sorted_hyperparameter_data = {}\n",
    "\n",
    "for model_name, runs_dict in hyperparameter_data.items():\n",
    "    runs_list = []\n",
    "    \n",
    "    for run_name, run_data in runs_dict.items():\n",
    "        # Determine best validation accuracy from the 'progress' CSV\n",
    "        if \"progress\" in run_data and run_data[\"progress\"]:\n",
    "            best_val_acc = max(\n",
    "                (r.get(\"val_acc\", float(\"-inf\")) for r in run_data[\"progress\"]),\n",
    "                default=float(\"-inf\")\n",
    "            )\n",
    "        else:\n",
    "            best_val_acc = float(\"-inf\")\n",
    "        \n",
    "        # Build absolute path to this particular run folder\n",
    "        run_folder_path = os.path.join(\n",
    "            main_folder_hyper, model_name, run_name\n",
    "        )\n",
    "        run_folder_path = os.path.abspath(run_folder_path)\n",
    "\n",
    "        # DEBUG: Print out the run folder path\n",
    "        # print(f\"DEBUG: run_folder_path for {run_name} = {run_folder_path}\")\n",
    "        \n",
    "        # Gather checkpoint directories\n",
    "        if os.path.isdir(run_folder_path):\n",
    "            checkpoint_dirs = [\n",
    "                d for d in os.listdir(run_folder_path)\n",
    "                if d.startswith(\"checkpoint_\") \n",
    "                   and os.path.isdir(os.path.join(run_folder_path, d))\n",
    "            ]\n",
    "        else:\n",
    "            checkpoint_dirs = []\n",
    "        \n",
    "        # Debug printing\n",
    "        print(f\"\\n[DEBUG] For model='{model_name}' run='{run_name}', found checkpoint dirs:\")\n",
    "        print(\"       \", checkpoint_dirs)\n",
    "\n",
    "        # Pick the *last* checkpoint folder numerically\n",
    "        if checkpoint_dirs:\n",
    "            checkpoint_dirs.sort(key=lambda x: int(x.split(\"_\")[1]))  # numeric sort by the suffix\n",
    "            last_checkpoint_dir = checkpoint_dirs[-1]\n",
    "            last_checkpoint = os.path.join(run_folder_path, last_checkpoint_dir, \"checkpoint.pkl\")\n",
    "            last_checkpoint = os.path.abspath(last_checkpoint)\n",
    "        else:\n",
    "            last_checkpoint = None\n",
    "        \n",
    "        runs_list.append((run_name, run_data, best_val_acc, last_checkpoint))\n",
    "    \n",
    "    # Sort all runs by best_val_acc descending\n",
    "    sorted_runs = sorted(runs_list, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Build a new dictionary with simple int-string keys (\"0\", \"1\", ...)\n",
    "    sorted_hyperparameter_data[model_name] = {\n",
    "        str(i): {\n",
    "            **run_data,\n",
    "            \"max_validation_accuracy\": best_val_acc,\n",
    "            \"path\": last_checkpoint\n",
    "        }\n",
    "        for i, (run_name, run_data, best_val_acc, last_checkpoint) in enumerate(sorted_runs)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEBUG: 'deeplabv3_resnet101' run #1 checkpoint path:\n",
      "/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet101/train_hyper_0951e564_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-02-19_04-23-11/checkpoint_000099/checkpoint.pkl\n",
      "dict_keys(['id', 'auto_cast', 'batch_size', 'learning_rate', 'max_epochs', 'weight_decay', 'result', 'progress', 'max_validation_accuracy', 'path'])\n",
      "/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet50/train_hyper_56ebb659_2_auto_cast=True,batch_size=8,learning_rate=0.0000,max_epochs=100,weight_decay=0.0007_2025-02-16_19-30-53/checkpoint_000099/checkpoint.pkl\n",
      "dict_keys(['fcn_resnet101', 'deeplabv3_resnet50', 'fcn_resnet50', 'deeplabv3_resnet101'])\n"
     ]
    }
   ],
   "source": [
    "# Validatie Dict 1\n",
    "if \"deeplabv3_resnet50\" in sorted_hyperparameter_data:\n",
    "    if \"1\" in sorted_hyperparameter_data[\"deeplabv3_resnet101\"]:\n",
    "        print(\"\\nDEBUG: 'deeplabv3_resnet101' run #1 checkpoint path:\")\n",
    "        print(sorted_hyperparameter_data['deeplabv3_resnet101']['0']['path'])\n",
    "    else:\n",
    "        print(\"\\nDEBUG: 'deeplabv3_resnet101' run #1 does not exist in dictionary.\")\n",
    "\n",
    "# You can print or debug other model-run combos similarly:\n",
    "# print(sorted_hyperparameter_data[\"deeplabv3_resnet50\"][\"0\"][\"path\"])\n",
    "\n",
    "print(sorted_hyperparameter_data[\"deeplabv3_resnet50\"]['0'].keys())\n",
    "print(sorted_hyperparameter_data[\"deeplabv3_resnet50\"]['0']['path'])\n",
    "\n",
    "print(sorted_hyperparameter_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'auto_cast', 'batch_size', 'learning_rate', 'max_epochs', 'weight_decay', 'result', 'progress', 'max_validation_accuracy', 'path'])\n"
     ]
    }
   ],
   "source": [
    "# Validate Dict 2\n",
    "print(sorted_hyperparameter_data[\"deeplabv3_resnet50\"][\"0\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEWERTUNGSFUNKTIONEN'\n",
    "import pandas as pd\n",
    "from Helper.ml_models import TrainedModel, K_Fold_Dataset\n",
    "\n",
    "def compute_confusion_matrix(predicted, ground_truth, num_classes):\n",
    "    mask = (ground_truth >= 0) & (ground_truth < num_classes)\n",
    "    label = num_classes * ground_truth[mask] + predicted[mask]\n",
    "    count = torch.bincount(label, minlength=num_classes**2)\n",
    "    confusion_matrix = count.reshape(num_classes, num_classes)\n",
    "    return confusion_matrix\n",
    "\n",
    "def compute_miou(confusion_matrix):\n",
    "    intersection = torch.diag(confusion_matrix)\n",
    "    ground_truth_set = confusion_matrix.sum(1)\n",
    "    predicted_set = confusion_matrix.sum(0)\n",
    "    union = ground_truth_set + predicted_set - intersection\n",
    "    IoU = intersection / (union + 1e-6)\n",
    "    mIoU = torch.mean(IoU)\n",
    "    return mIoU.item(), IoU\n",
    "\n",
    "def compute_mean_pixel_accuracy(confusion_matrix):\n",
    "    true_positive = torch.diag(confusion_matrix)\n",
    "    total_pixels = confusion_matrix.sum(1)\n",
    "    pixel_accuracy = true_positive / (total_pixels + 1e-6)\n",
    "    mPA = torch.mean(pixel_accuracy)\n",
    "    return mPA.item(), pixel_accuracy\n",
    "\n",
    "def compute_fwiou(confusion_matrix):\n",
    "    total_pixels = confusion_matrix.sum()\n",
    "    ground_truth_set = confusion_matrix.sum(1)\n",
    "    intersection = torch.diag(confusion_matrix)\n",
    "    union = ground_truth_set + confusion_matrix.sum(0) - intersection\n",
    "    IoU = intersection / (union + 1e-6)\n",
    "    FWIoU = (ground_truth_set * IoU) / total_pixels\n",
    "    FWIoU = FWIoU.sum()\n",
    "    return FWIoU.item()\n",
    "\n",
    "def compute_dice_coefficient(confusion_matrix):\n",
    "    intersection = torch.diag(confusion_matrix)\n",
    "    ground_truth_set = confusion_matrix.sum(1)\n",
    "    predicted_set = confusion_matrix.sum(0)\n",
    "    dice = (2 * intersection) / (ground_truth_set + predicted_set + 1e-6)\n",
    "    mean_dice = torch.mean(dice)\n",
    "    return mean_dice.item(), dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading MOdel Function\n",
    "def load_checkpointed_model_ray(model_name, checkpoint_path, num_classes=None):\n",
    "    # Hier sicherstellen, dass skip_local_load Ã¼bergeben wird:\n",
    "    loaded_model = MapillaryTrainedModel(\n",
    "        model_name=model_name,\n",
    "        width=520,\n",
    "        height=520,\n",
    "        weights_name='',\n",
    "        skip_local_load=True  # WICHTIG!\n",
    "    )\n",
    "    with open(checkpoint_path, \"rb\") as fp:\n",
    "        checkpoint_data = pickle.load(fp)\n",
    "    loaded_model.model.load_state_dict(checkpoint_data[\"model_state\"], strict=True)\n",
    "    if \"optimizer_state\" in checkpoint_data:\n",
    "        loaded_model.optimizer.load_state_dict(checkpoint_data[\"optimizer_state\"])\n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with length 2000\n"
     ]
    }
   ],
   "source": [
    "# Loading Dataset\n",
    "from Helper.ml_models import MapillaryDataLoader\n",
    "\n",
    "mapillary_loader = MapillaryDataLoader(\n",
    "    train_images_dir=\"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Mapillary_Vistas/training/images\",\n",
    "    train_annotations_dir=\"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Mapillary_Vistas/training_own\",\n",
    "    val_images_dir=\"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Mapillary_Vistas/validation/images\",\n",
    "    val_annotations_dir=\"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Mapillary_Vistas/validation_own\"\n",
    ")\n",
    "\n",
    "test_dataset = mapillary_loader.test_dataset\n",
    "print(f'Dataset loaded with length {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalutation Function using all above defined evaluation functions (\"Berwertungsfunktionen\")\n",
    "def evaluate_model(model: MapillaryTrainedModel, dataset, num_classes: int) -> dict:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.model.to(device)\n",
    "    confusion_matrix_total = torch.zeros((num_classes, num_classes), dtype=torch.int64).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(dataset)), desc=\"Evaluating Dataset\"):\n",
    "            image, annotation = dataset[i]\n",
    "            image = image.to(device)\n",
    "            annotation = annotation.to(device)\n",
    "            output = model.inference(image)\n",
    "            predicted = output.argmax(1).squeeze(0)\n",
    "            conf_mat = compute_confusion_matrix(predicted.cpu(), annotation.cpu(), num_classes)\n",
    "            # Ãndere hier:\n",
    "            confusion_matrix_total += conf_mat.to(device)\n",
    "    \n",
    "    miou, iou_per_class = compute_miou(confusion_matrix_total)\n",
    "    mpa, pa_per_class = compute_mean_pixel_accuracy(confusion_matrix_total)\n",
    "    fwiou = compute_fwiou(confusion_matrix_total)\n",
    "    dice_mean, dice_per_class = compute_dice_coefficient(confusion_matrix_total)\n",
    "    \n",
    "    metrics = {\n",
    "        \"mIoU\": miou,\n",
    "        \"mPA\": mpa,\n",
    "        \"FWIoU\": fwiou,\n",
    "        \"Dice_Mean\": dice_mean,\n",
    "        \"IoU_per_class\": iou_per_class.tolist(),\n",
    "        \"PA_per_class\": pa_per_class.tolist(),\n",
    "        \"Dice_per_class\": dice_per_class.tolist()\n",
    "    }\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: fcn_resnet101\n",
      "Checkpoint path:  /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/fcn_resnet101/train_hyper_c75247d6_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-02-22_22-23-45/checkpoint_000099/checkpoint.pkl\n",
      "Using CUDA GPU\n",
      "Model loaded: fcn_resnet101 | Device: cuda \n",
      "Error loading Model with Epoch latest: Error(s) in loading state_dict for FCN:\n",
      "\tsize mismatch for classifier.4.weight: copying a param with shape torch.Size([20, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([124, 512, 1, 1]).\n",
      "\tsize mismatch for classifier.4.bias: copying a param with shape torch.Size([20]) from checkpoint, the shape in current model is torch.Size([124]).\n",
      "Skipping local .pth load due to error above.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Dataset: 100%|ââââââââââ| 2000/2000 [04:10<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for fcn_resnet101:\n",
      "  mIoU: 0.1921\n",
      "  mPA: 0.2344\n",
      "  FWIoU: 0.7838\n",
      "  Dice_Mean: 0.2648\n",
      "  IoU_per_class: [list of length 124]\n",
      "  PA_per_class: [list of length 124]\n",
      "  Dice_per_class: [list of length 124]\n",
      "\n",
      "Evaluating model: deeplabv3_resnet50\n",
      "Checkpoint path:  /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet50/train_hyper_56ebb659_2_auto_cast=True,batch_size=8,learning_rate=0.0000,max_epochs=100,weight_decay=0.0007_2025-02-16_19-30-53/checkpoint_000099/checkpoint.pkl\n",
      "Using CUDA GPU\n",
      "Model loaded: deeplabv3_resnet50 | Device: cuda \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Dataset: 100%|ââââââââââ| 2000/2000 [03:21<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for deeplabv3_resnet50:\n",
      "  mIoU: 0.1847\n",
      "  mPA: 0.2335\n",
      "  FWIoU: 0.7961\n",
      "  Dice_Mean: 0.2458\n",
      "  IoU_per_class: [list of length 124]\n",
      "  PA_per_class: [list of length 124]\n",
      "  Dice_per_class: [list of length 124]\n",
      "\n",
      "Evaluating model: fcn_resnet50\n",
      "Checkpoint path:  /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/fcn_resnet50/train_hyper_4b822bcf_2_auto_cast=True,batch_size=8,learning_rate=0.0000,max_epochs=100,weight_decay=0.0007_2025-02-20_17-09-00/checkpoint_000099/checkpoint.pkl\n",
      "Using CUDA GPU\n",
      "Model loaded: fcn_resnet50 | Device: cuda \n",
      "No local .pth found; initializing a new model save.\n",
      "Saved Model\n",
      "Successfully loaded a fresh model checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Dataset: 100%|ââââââââââ| 2000/2000 [03:08<00:00, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for fcn_resnet50:\n",
      "  mIoU: 0.1927\n",
      "  mPA: 0.2379\n",
      "  FWIoU: 0.7903\n",
      "  Dice_Mean: 0.2583\n",
      "  IoU_per_class: [list of length 124]\n",
      "  PA_per_class: [list of length 124]\n",
      "  Dice_per_class: [list of length 124]\n",
      "\n",
      "Evaluating model: deeplabv3_resnet101\n",
      "Checkpoint path:  /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet101/train_hyper_0951e564_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-02-19_04-23-11/checkpoint_000099/checkpoint.pkl\n",
      "Using CUDA GPU\n",
      "Model loaded: deeplabv3_resnet101 | Device: cuda \n",
      "No local .pth found; initializing a new model save.\n",
      "Saved Model\n",
      "Successfully loaded a fresh model checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Dataset: 100%|ââââââââââ| 2000/2000 [03:38<00:00,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for deeplabv3_resnet101:\n",
      "  mIoU: 0.2055\n",
      "  mPA: 0.2607\n",
      "  FWIoU: 0.8023\n",
      "  Dice_Mean: 0.2764\n",
      "  IoU_per_class: [list of length 124]\n",
      "  PA_per_class: [list of length 124]\n",
      "  Dice_per_class: [list of length 124]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 124  # Neue Klassenanzahl\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "for model_name, runs_dict in sorted_hyperparameter_data.items():\n",
    "    best_run_info = runs_dict[\"0\"]\n",
    "    checkpoint_path = best_run_info.get(\"path\", None)\n",
    "    if not checkpoint_path:\n",
    "        print(f\"\\n[WARNING] No checkpoint path found for {model_name} run '0'. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    print(f\"Checkpoint path:  {checkpoint_path}\")\n",
    "\n",
    "    try:\n",
    "        model_loaded = load_checkpointed_model_ray(model_name, checkpoint_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(\"[WARNING] The exact error was:\", e)\n",
    "        continue\n",
    "\n",
    "    # Evaluierung auf dem Testset\n",
    "    metrics = evaluate_model(model_loaded, test_dataset, NUM_CLASSES)\n",
    "    evaluation_results[model_name] = metrics\n",
    "    print(f\"Results for {model_name}:\")\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, list):\n",
    "            print(f\"  {k}: [list of length {len(v)}]\")\n",
    "        else:\n",
    "            print(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "\n",
    "# Optional: Speichere die Ergebnisse\n",
    "with open(\"evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(evaluation_results, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results saved to: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/evaluation_hyperparameter_MapillaryV1.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "save_path = \"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/evaluation_hyperparameter_MapillaryV1.json\"\n",
    "\n",
    "# 2) Ensure the directory exists. If not, create it.\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "# 3) Write the evaluation_results dictionary to JSON.\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(evaluation_results, f, indent=4)\n",
    "\n",
    "print(f\"Evaluation results saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint paths saved to: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/best_checkpoints_Mapillary.json\n"
     ]
    }
   ],
   "source": [
    "best_checkpoints = {}\n",
    "for model_name, runs_dict in sorted_hyperparameter_data.items():\n",
    "    # The best run is at index \"0\"\n",
    "    best_run_info = runs_dict[\"0\"]\n",
    "    best_checkpoint_path = best_run_info.get(\"path\", None)\n",
    "    \n",
    "    # Store it in our dictionary\n",
    "    best_checkpoints[model_name] = best_checkpoint_path\n",
    "\n",
    "# 2) Specify where you want to save the JSON\n",
    "save_path = \"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/best_checkpoints_Mapillary.json\"\n",
    "\n",
    "# 3) Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "# 4) Write the dictionary to JSON\n",
    "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_checkpoints, f, indent=4)\n",
    "\n",
    "print(f\"Best checkpoint paths saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â Checkpoint fÃ¼r fcn_resnet101 gesichert: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/hyper_map/fcn_resnet101_best_checkpoint.pkl\n",
      "â Checkpoint fÃ¼r deeplabv3_resnet50 gesichert: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/hyper_map/deeplabv3_resnet50_best_checkpoint.pkl\n",
      "â Checkpoint fÃ¼r fcn_resnet50 gesichert: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/hyper_map/fcn_resnet50_best_checkpoint.pkl\n",
      "â Checkpoint fÃ¼r deeplabv3_resnet101 gesichert: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/hyper_map/deeplabv3_resnet101_best_checkpoint.pkl\n",
      "â Alle Checkpoints wurden gesichert!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Definiere das Zielverzeichnis\n",
    "target_dir = \"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/hyper_map\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Lade die JSON-Datei mit den besten Checkpoints\n",
    "best_checkpoints_path = \"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/best_checkpoints_Mapillary.json\"\n",
    "\n",
    "with open(best_checkpoints_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    best_checkpoints = json.load(f)\n",
    "\n",
    "# Kopiere die besten Checkpoints in das Zielverzeichnis\n",
    "for model_name, checkpoint_path in best_checkpoints.items():\n",
    "    if checkpoint_path and os.path.isfile(checkpoint_path):\n",
    "        # Bestimme den Zielpfad\n",
    "        dest_checkpoint = os.path.join(target_dir, f\"{model_name}_best_checkpoint.pkl\")\n",
    "\n",
    "        # Kopiere die Datei\n",
    "        shutil.copy2(checkpoint_path, dest_checkpoint)\n",
    "        print(f\"â Checkpoint fÃ¼r {model_name} gesichert: {dest_checkpoint}\")\n",
    "    else:\n",
    "        print(f\"â ï¸ Kein gÃ¼ltiger Checkpoint fÃ¼r {model_name} gefunden!\")\n",
    "\n",
    "print(\"â Alle Checkpoints wurden gesichert!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
