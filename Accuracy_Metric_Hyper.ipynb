{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ray.cloudpickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subfolder name: deeplabv3_resnet50\n",
      "Subfolder name: deeplabv3_resnet101\n"
     ]
    }
   ],
   "source": [
    "main_folder_hyper = \"HyperparameterLOG\"\n",
    "\n",
    "for folder_name in os.listdir(main_folder_hyper):\n",
    "    folder_path = os.path.join(main_folder_hyper, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f\"Subfolder name: {folder_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: main_folder_hyper = /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG\n",
      "DEBUG: Exists on disk?   = True\n",
      "\n",
      "DEBUG: Subfolders in HyperparameterLOG:\n",
      " - deeplabv3_resnet50\n",
      " - deeplabv3_resnet101\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet50/tuner.pkl\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet50/experiment_state-2025-02-06_19-30-16.json\n",
      "Skipping incomplete run: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet50/train_hyper_ff4383cb_8_auto_cast=False,batch_size=4,learning_rate=0.0000,max_epochs=100,weight_decay=0.0038_2025-02-08_01-33-00\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet50/.validate_storage_marker\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet50/searcher-state-2025-02-06_19-30-16.pkl\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet50/search_gen_state-2025-02-06_19-30-16.json\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet101/tuner.pkl\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet101/experiment_state-2025-02-08_07-36-17.json\n",
      "Skipping incomplete run: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet101/train_hyper_6e2abc3c_3_auto_cast=False,batch_size=16,learning_rate=0.0000,max_epochs=100,weight_decay=0.0001_2025-02-09_19-52-26\n",
      "Skipping incomplete run: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet101/train_hyper_7c22c5c7_4_auto_cast=False,batch_size=16,learning_rate=0.0000,max_epochs=100,weight_decay=0.0014_2025-02-09_19-52-36\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet101/search_gen_state-2025-02-08_07-36-17.json\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet101/.validate_storage_marker\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet101/searcher-state-2025-02-08_07-36-17.pkl\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1) DEFINE ABSOLUTE BASE PATH\n",
    "# -----------------------------\n",
    "BASE_PATH = \"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation\"\n",
    "# Construct absolute path to your HyperparameterLOG\n",
    "main_folder_hyper = os.path.join(BASE_PATH, \"HyperparameterLOG\")\n",
    "main_folder_hyper = os.path.abspath(main_folder_hyper)\n",
    "print(\"DEBUG: main_folder_hyper =\", main_folder_hyper)\n",
    "print(\"DEBUG: Exists on disk?   =\", os.path.isdir(main_folder_hyper))\n",
    "\n",
    "# (Optional) Print top-level subfolders for confirmation:\n",
    "print(\"\\nDEBUG: Subfolders in HyperparameterLOG:\")\n",
    "for item in os.listdir(main_folder_hyper):\n",
    "    if os.path.isdir(os.path.join(main_folder_hyper, item)):\n",
    "        print(\" -\", item)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) LOAD HYPERPARAMETER RUNS\n",
    "# -----------------------------\n",
    "def load_hyperparameter_runs_as_dict(base_folder: str):\n",
    "    runs_data = {}\n",
    "\n",
    "    if not os.path.isdir(base_folder):\n",
    "        print(f\"ERROR: Base folder does not exist: {base_folder}\")\n",
    "        return runs_data\n",
    "\n",
    "    for model_folder in os.listdir(base_folder):\n",
    "        model_path = os.path.join(base_folder, model_folder)\n",
    "        \n",
    "        if not os.path.isdir(model_path):\n",
    "            print(f\"Skipping invalid model path: {model_path}\")\n",
    "            continue\n",
    "\n",
    "        runs_data[model_folder] = {}\n",
    "\n",
    "        for train_folder in os.listdir(model_path):\n",
    "            train_folder_path = os.path.join(model_path, train_folder)\n",
    "\n",
    "            # Sicherstellen, dass es sich um einen gültigen Trainingsordner handelt\n",
    "            if not train_folder.startswith(\"train_hyper_\") or not os.path.isdir(train_folder_path):\n",
    "                print(f\"Skipping non-training folder: {train_folder_path}\")\n",
    "                continue\n",
    "\n",
    "            # Check if necessary files exist\n",
    "            params_file = os.path.join(train_folder_path, \"params.json\")\n",
    "            progress_file = os.path.join(train_folder_path, \"progress.csv\")\n",
    "            result_file = os.path.join(train_folder_path, \"result.json\")\n",
    "\n",
    "            if not (os.path.isfile(params_file) and os.path.isfile(progress_file) and os.path.isfile(result_file)):\n",
    "                print(f\"Skipping incomplete run: {train_folder_path}\")\n",
    "                continue\n",
    "\n",
    "            # Laden der Dateien\n",
    "            with open(params_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                params_dict = json.load(f)\n",
    "\n",
    "            progress_df = pd.read_csv(progress_file)\n",
    "            progress_records = progress_df.to_dict(orient=\"records\")\n",
    "\n",
    "            result_records = []\n",
    "            with open(result_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        result_records.append(json.loads(line))\n",
    "\n",
    "            run_dict = {\n",
    "                \"id\": train_folder,\n",
    "                **params_dict,\n",
    "                \"result\": result_records,\n",
    "                \"progress\": progress_records,\n",
    "            }\n",
    "\n",
    "            runs_data[model_folder][train_folder] = run_dict\n",
    "\n",
    "    return runs_data\n",
    "\n",
    "# Neuladen der Daten mit verbesserter Fehlerbehandlung\n",
    "hyperparameter_data = load_hyperparameter_runs_as_dict(main_folder_hyper)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] For model='deeplabv3_resnet50' run='train_hyper_533faa74_5_auto_cast=False,batch_size=8,learning_rate=0.0006,max_epochs=100,weight_decay=0.0000_2025-02-08_01-32-26', found checkpoint dirs:\n",
      "        []\n",
      "\n",
      "[DEBUG] For model='deeplabv3_resnet50' run='train_hyper_1084e8d6_2_auto_cast=True,batch_size=8,learning_rate=0.0040,max_epochs=100,weight_decay=0.0076_2025-02-06_19-30-20', found checkpoint dirs:\n",
      "        ['checkpoint_000002', 'checkpoint_000004']\n",
      "\n",
      "[DEBUG] For model='deeplabv3_resnet50' run='train_hyper_ac0b31c1_6_auto_cast=False,batch_size=16,learning_rate=0.0027,max_epochs=100,weight_decay=0.0001_2025-02-08_01-32-38', found checkpoint dirs:\n",
      "        []\n",
      "\n",
      "[DEBUG] For model='deeplabv3_resnet50' run='train_hyper_8cdfeb36_4_auto_cast=False,batch_size=16,learning_rate=0.0000,max_epochs=100,weight_decay=0.0014_2025-02-08_01-32-14', found checkpoint dirs:\n",
      "        []\n",
      "\n",
      "[DEBUG] For model='deeplabv3_resnet50' run='train_hyper_e12c1511_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-02-06_19-30-16', found checkpoint dirs:\n",
      "        ['checkpoint_000099', 'checkpoint_000075']\n",
      "\n",
      "[DEBUG] For model='deeplabv3_resnet50' run='train_hyper_0fc0e398_3_auto_cast=False,batch_size=16,learning_rate=0.0000,max_epochs=100,weight_decay=0.0001_2025-02-08_00-07-07', found checkpoint dirs:\n",
      "        []\n",
      "\n",
      "[DEBUG] For model='deeplabv3_resnet50' run='train_hyper_3662cd6a_7_auto_cast=False,batch_size=4,learning_rate=0.0000,max_epochs=100,weight_decay=0.0000_2025-02-08_01-32-47', found checkpoint dirs:\n",
      "        ['checkpoint_000015', 'checkpoint_000014', 'checkpoint_000012']\n",
      "\n",
      "[DEBUG] For model='deeplabv3_resnet101' run='train_hyper_1e767146_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-02-08_07-36-18', found checkpoint dirs:\n",
      "        ['checkpoint_000099', 'checkpoint_000091', 'checkpoint_000094']\n",
      "\n",
      "[DEBUG] For model='deeplabv3_resnet101' run='train_hyper_47066be6_2_auto_cast=True,batch_size=8,learning_rate=0.0040,max_epochs=100,weight_decay=0.0076_2025-02-08_07-36-30', found checkpoint dirs:\n",
      "        []\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 3) SORT + FIND CHECKPOINTS\n",
    "# -----------------------------\n",
    "sorted_hyperparameter_data = {}\n",
    "\n",
    "for model_name, runs_dict in hyperparameter_data.items():\n",
    "    runs_list = []\n",
    "    \n",
    "    for run_name, run_data in runs_dict.items():\n",
    "        # Determine best validation accuracy from the 'progress' CSV\n",
    "        if \"progress\" in run_data and run_data[\"progress\"]:\n",
    "            best_val_acc = max(\n",
    "                (r.get(\"val_acc\", float(\"-inf\")) for r in run_data[\"progress\"]),\n",
    "                default=float(\"-inf\")\n",
    "            )\n",
    "        else:\n",
    "            best_val_acc = float(\"-inf\")\n",
    "        \n",
    "        # Build absolute path to this particular run folder\n",
    "        run_folder_path = os.path.join(\n",
    "            main_folder_hyper, model_name, run_name\n",
    "        )\n",
    "        run_folder_path = os.path.abspath(run_folder_path)\n",
    "\n",
    "        # DEBUG: Print out the run folder path\n",
    "        # print(f\"DEBUG: run_folder_path for {run_name} = {run_folder_path}\")\n",
    "        \n",
    "        # Gather checkpoint directories\n",
    "        if os.path.isdir(run_folder_path):\n",
    "            checkpoint_dirs = [\n",
    "                d for d in os.listdir(run_folder_path)\n",
    "                if d.startswith(\"checkpoint_\") \n",
    "                   and os.path.isdir(os.path.join(run_folder_path, d))\n",
    "            ]\n",
    "        else:\n",
    "            checkpoint_dirs = []\n",
    "        \n",
    "        # Debug printing\n",
    "        print(f\"\\n[DEBUG] For model='{model_name}' run='{run_name}', found checkpoint dirs:\")\n",
    "        print(\"       \", checkpoint_dirs)\n",
    "\n",
    "        # Pick the *last* checkpoint folder numerically\n",
    "        if checkpoint_dirs:\n",
    "            checkpoint_dirs.sort(key=lambda x: int(x.split(\"_\")[1]))  # numeric sort by the suffix\n",
    "            last_checkpoint_dir = checkpoint_dirs[-1]\n",
    "            last_checkpoint = os.path.join(run_folder_path, last_checkpoint_dir, \"checkpoint.pkl\")\n",
    "            last_checkpoint = os.path.abspath(last_checkpoint)\n",
    "        else:\n",
    "            last_checkpoint = None\n",
    "        \n",
    "        runs_list.append((run_name, run_data, best_val_acc, last_checkpoint))\n",
    "    \n",
    "    # Sort all runs by best_val_acc descending\n",
    "    sorted_runs = sorted(runs_list, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Build a new dictionary with simple int-string keys (\"0\", \"1\", ...)\n",
    "    sorted_hyperparameter_data[model_name] = {\n",
    "        str(i): {\n",
    "            **run_data,\n",
    "            \"max_validation_accuracy\": best_val_acc,\n",
    "            \"path\": last_checkpoint\n",
    "        }\n",
    "        for i, (run_name, run_data, best_val_acc, last_checkpoint) in enumerate(sorted_runs)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEBUG: 'deeplabv3_resnet101' run #1 checkpoint path:\n",
      "/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet101/train_hyper_1e767146_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-02-08_07-36-18/checkpoint_000099/checkpoint.pkl\n",
      "dict_keys(['id', 'auto_cast', 'batch_size', 'learning_rate', 'max_epochs', 'weight_decay', 'result', 'progress', 'max_validation_accuracy', 'path'])\n",
      "/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet50/train_hyper_e12c1511_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-02-06_19-30-16/checkpoint_000099/checkpoint.pkl\n",
      "dict_keys(['deeplabv3_resnet50', 'deeplabv3_resnet101'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 4) PRINT EXAMPLE CHECKPOINT PATH\n",
    "# -----------------------------\n",
    "# Example: print the best run's path for 'lraspp_mobilenet_v3_large'\n",
    "# (Change \"1\" to \"0\" or \"2\" as needed, depending on how many runs you have.)\n",
    "if \"deeplabv3_resnet50\" in sorted_hyperparameter_data:\n",
    "    if \"1\" in sorted_hyperparameter_data[\"deeplabv3_resnet101\"]:\n",
    "        print(\"\\nDEBUG: 'deeplabv3_resnet101' run #1 checkpoint path:\")\n",
    "        print(sorted_hyperparameter_data['deeplabv3_resnet101']['0']['path'])\n",
    "    else:\n",
    "        print(\"\\nDEBUG: 'deeplabv3_resnet101' run #1 does not exist in dictionary.\")\n",
    "\n",
    "# You can print or debug other model-run combos similarly:\n",
    "# print(sorted_hyperparameter_data[\"deeplabv3_resnet50\"][\"0\"][\"path\"])\n",
    "\n",
    "print(sorted_hyperparameter_data[\"deeplabv3_resnet50\"]['0'].keys())\n",
    "print(sorted_hyperparameter_data[\"deeplabv3_resnet50\"]['0']['path'])\n",
    "\n",
    "print(sorted_hyperparameter_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'auto_cast', 'batch_size', 'learning_rate', 'max_epochs', 'weight_decay', 'result', 'progress', 'max_validation_accuracy', 'path'])\n"
     ]
    }
   ],
   "source": [
    "print(sorted_hyperparameter_data[\"deeplabv3_resnet50\"][\"0\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 08:21:10.118446: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-10 08:21:10.775790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 5) REMAINDER OF YOUR EVAL CODE\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "from Helper.ml_models import TrainedModel, K_Fold_Dataset\n",
    "\n",
    "def compute_confusion_matrix(predicted, ground_truth, num_classes):\n",
    "    mask = (ground_truth >= 0) & (ground_truth < num_classes)\n",
    "    label = num_classes * ground_truth[mask] + predicted[mask]\n",
    "    count = torch.bincount(label, minlength=num_classes**2)\n",
    "    confusion_matrix = count.reshape(num_classes, num_classes)\n",
    "    return confusion_matrix\n",
    "\n",
    "def compute_miou(confusion_matrix):\n",
    "    intersection = torch.diag(confusion_matrix)\n",
    "    ground_truth_set = confusion_matrix.sum(1)\n",
    "    predicted_set = confusion_matrix.sum(0)\n",
    "    union = ground_truth_set + predicted_set - intersection\n",
    "    IoU = intersection / (union + 1e-6)\n",
    "    mIoU = torch.mean(IoU)\n",
    "    return mIoU.item(), IoU\n",
    "\n",
    "def compute_mean_pixel_accuracy(confusion_matrix):\n",
    "    true_positive = torch.diag(confusion_matrix)\n",
    "    total_pixels = confusion_matrix.sum(1)\n",
    "    pixel_accuracy = true_positive / (total_pixels + 1e-6)\n",
    "    mPA = torch.mean(pixel_accuracy)\n",
    "    return mPA.item(), pixel_accuracy\n",
    "\n",
    "def compute_fwiou(confusion_matrix):\n",
    "    total_pixels = confusion_matrix.sum()\n",
    "    ground_truth_set = confusion_matrix.sum(1)\n",
    "    intersection = torch.diag(confusion_matrix)\n",
    "    union = ground_truth_set + confusion_matrix.sum(0) - intersection\n",
    "    IoU = intersection / (union + 1e-6)\n",
    "    FWIoU = (ground_truth_set * IoU) / total_pixels\n",
    "    FWIoU = FWIoU.sum()\n",
    "    return FWIoU.item()\n",
    "\n",
    "def compute_dice_coefficient(confusion_matrix):\n",
    "    intersection = torch.diag(confusion_matrix)\n",
    "    ground_truth_set = confusion_matrix.sum(1)\n",
    "    predicted_set = confusion_matrix.sum(0)\n",
    "    dice = (2 * intersection) / (ground_truth_set + predicted_set + 1e-6)\n",
    "    mean_dice = torch.mean(dice)\n",
    "    return mean_dice.item(), dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpointed_model_ray(model_name, checkpoint_path, num_classes=None):\n",
    "    \"\"\"\n",
    "    Lädt ein Modell mit Checkpoint ohne Anpassung des Klassifikationslayers.\n",
    "    \"\"\"\n",
    "    # Lade das Modell und den Checkpoint\n",
    "    loaded_model = TrainedModel(model_name=model_name, width=520, height=520)\n",
    "    with open(checkpoint_path, \"rb\") as fp:\n",
    "        checkpoint_data = pickle.load(fp)\n",
    "\n",
    "    # Checkpoint in das Modell laden\n",
    "    loaded_model.model.load_state_dict(checkpoint_data[\"model_state\"], strict=True)\n",
    "\n",
    "    # (Optional) Optimizer-Zustand laden, falls benötigt\n",
    "    if \"optimizer_state\" in checkpoint_data:\n",
    "        loaded_model.optimizer.load_state_dict(checkpoint_data[\"optimizer_state\"])\n",
    "\n",
    "    return loaded_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helper.ml_models import MapillaryDataLoader\n",
    "\n",
    "mapillary_loader = MapillaryDataLoader(\n",
    "    train_images_dir=\"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Mapilarry_Vistas/training/images\",\n",
    "    train_annotations_dir=\"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Mapilarry_Vistas/training_own\",\n",
    "    val_images_dir=\"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Mapilarry_Vistas/validation/images\",\n",
    "    val_annotations_dir=\"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Mapilarry_Vistas/validation_own\"\n",
    ")\n",
    "\n",
    "test_dataset = mapillary_loader.test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(model: TrainedModel, dataset, num_classes: int) -> dict:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.model.to(device)\n",
    "    confusion_matrix_total = torch.zeros((num_classes, num_classes), dtype=torch.int64).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(dataset)), desc=\"Evaluating Dataset\"):\n",
    "            image, annotation = dataset[i]\n",
    "            image = image.to(device)\n",
    "            annotation = annotation.to(device)\n",
    "            output = model.inference(image)\n",
    "            predicted = output.argmax(1).squeeze(0)\n",
    "            conf_mat = compute_confusion_matrix(predicted.cpu(), annotation.cpu(), num_classes)\n",
    "            confusion_matrix_total += conf_mat.cpu()\n",
    "    \n",
    "    miou, iou_per_class = compute_miou(confusion_matrix_total)\n",
    "    mpa, pa_per_class = compute_mean_pixel_accuracy(confusion_matrix_total)\n",
    "    fwiou = compute_fwiou(confusion_matrix_total)\n",
    "    dice_mean, dice_per_class = compute_dice_coefficient(confusion_matrix_total)\n",
    "    \n",
    "    metrics = {\n",
    "        \"mIoU\": miou,\n",
    "        \"mPA\": mpa,\n",
    "        \"FWIoU\": fwiou,\n",
    "        \"Dice_Mean\": dice_mean,\n",
    "        \"IoU_per_class\": iou_per_class.tolist(),\n",
    "        \"PA_per_class\": pa_per_class.tolist(),\n",
    "        \"Dice_per_class\": dice_per_class.tolist()\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: deeplabv3_resnet50\n",
      "Checkpoint path:  /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet50/train_hyper_e12c1511_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-02-06_19-30-16/checkpoint_000099/checkpoint.pkl\n",
      "Using CUDA GPU\n",
      "Model loaded: deeplabv3_resnet50 | Device: cuda \n",
      "Error loading Model with Epoch latest: Error(s) in loading state_dict for DeepLabV3:\n",
      "\tsize mismatch for classifier.4.weight: copying a param with shape torch.Size([124, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([20, 256, 1, 1]).\n",
      "\tsize mismatch for classifier.4.bias: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([20]).\n",
      "Skipping local .pth load due to error above.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DeepLabV3:\n\tsize mismatch for classifier.4.weight: copying a param with shape torch.Size([124, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([20, 256, 1, 1]).\n\tsize mismatch for classifier.4.bias: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([20]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint path:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     model_loaded \u001b[38;5;241m=\u001b[39m \u001b[43mload_checkpointed_model_ray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_CLASSES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[WARNING] The exact error was:\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m, in \u001b[0;36mload_checkpointed_model_ray\u001b[0;34m(model_name, checkpoint_path, num_classes)\u001b[0m\n\u001b[1;32m      8\u001b[0m     checkpoint_data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(fp)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Checkpoint in das Modell laden\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_state\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# (Optional) Optimizer-Zustand laden, falls benötigt\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_state\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m checkpoint_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/studi/lib/python3.9/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DeepLabV3:\n\tsize mismatch for classifier.4.weight: copying a param with shape torch.Size([124, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([20, 256, 1, 1]).\n\tsize mismatch for classifier.4.bias: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([20])."
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 124  # Neue Klassenanzahl\n",
    "\n",
    "for model_name, runs_dict in sorted_hyperparameter_data.items():\n",
    "    best_run_info = runs_dict[\"0\"]\n",
    "    checkpoint_path = best_run_info.get(\"path\", None)\n",
    "    if not checkpoint_path:\n",
    "        print(f\"\\n[WARNING] No checkpoint path found for {model_name} run '0'. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    print(f\"Checkpoint path:  {checkpoint_path}\")\n",
    "\n",
    "    try:\n",
    "        model_loaded = load_checkpointed_model_ray(model_name, checkpoint_path, NUM_CLASSES)\n",
    "    except FileNotFoundError as e:\n",
    "        print(\"[WARNING] The exact error was:\", e)\n",
    "        continue\n",
    "\n",
    "    # Evaluierung auf dem Testset\n",
    "    metrics = evaluate_model(model_loaded, test_dataset, NUM_CLASSES)\n",
    "    evaluation_results[model_name] = metrics\n",
    "    print(f\"Results for {model_name}:\")\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, list):\n",
    "            print(f\"  {k}: [list of length {len(v)}]\")\n",
    "        else:\n",
    "            print(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "\n",
    "# Optional: Speichere die Ergebnisse\n",
    "with open(\"evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(evaluation_results, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results saved to: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/evaluation_hyperparameter_cityscapes.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "save_path = \"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/evaluation_hyperparameter_MapillaryV1.json\"\n",
    "\n",
    "# 2) Ensure the directory exists. If not, create it.\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "# 3) Write the evaluation_results dictionary to JSON.\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(evaluation_results, f, indent=4)\n",
    "\n",
    "print(f\"Evaluation results saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint paths saved to: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/best_checkpoints.json\n"
     ]
    }
   ],
   "source": [
    "best_checkpoints = {}\n",
    "for model_name, runs_dict in sorted_hyperparameter_data.items():\n",
    "    # The best run is at index \"0\"\n",
    "    best_run_info = runs_dict[\"0\"]\n",
    "    best_checkpoint_path = best_run_info.get(\"path\", None)\n",
    "    \n",
    "    # Store it in our dictionary\n",
    "    best_checkpoints[model_name] = best_checkpoint_path\n",
    "\n",
    "# 2) Specify where you want to save the JSON\n",
    "save_path = \"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/best_checkpoints.json\"\n",
    "\n",
    "# 3) Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "# 4) Write the dictionary to JSON\n",
    "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_checkpoints, f, indent=4)\n",
    "\n",
    "print(f\"Best checkpoint paths saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checkpoint für lraspp_mobilenet_v3_large gesichert: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/hyper_city/lraspp_mobilenet_v3_large_best_checkpoint.pkl\n",
      "✅ Checkpoint für fcn_resnet101 gesichert: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/hyper_city/fcn_resnet101_best_checkpoint.pkl\n",
      "✅ Checkpoint für deeplabv3_resnet50 gesichert: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/hyper_city/deeplabv3_resnet50_best_checkpoint.pkl\n",
      "✅ Checkpoint für fcn_resnet50 gesichert: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/hyper_city/fcn_resnet50_best_checkpoint.pkl\n",
      "✅ Checkpoint für deeplabv3_mobilenet_v3_large gesichert: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/hyper_city/deeplabv3_mobilenet_v3_large_best_checkpoint.pkl\n",
      "✅ Checkpoint für deeplabv3_resnet101 gesichert: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/hyper_city/deeplabv3_resnet101_best_checkpoint.pkl\n",
      "✅ Alle Checkpoints wurden gesichert!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Definiere das Zielverzeichnis\n",
    "target_dir = \"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/hyper_city\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Lade die JSON-Datei mit den besten Checkpoints\n",
    "best_checkpoints_path = \"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/best_checkpoints.json\"\n",
    "\n",
    "with open(best_checkpoints_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    best_checkpoints = json.load(f)\n",
    "\n",
    "# Kopiere die besten Checkpoints in das Zielverzeichnis\n",
    "for model_name, checkpoint_path in best_checkpoints.items():\n",
    "    if checkpoint_path and os.path.isfile(checkpoint_path):\n",
    "        # Bestimme den Zielpfad\n",
    "        dest_checkpoint = os.path.join(target_dir, f\"{model_name}_best_checkpoint.pkl\")\n",
    "\n",
    "        # Kopiere die Datei\n",
    "        shutil.copy2(checkpoint_path, dest_checkpoint)\n",
    "        print(f\"✅ Checkpoint für {model_name} gesichert: {dest_checkpoint}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Kein gültiger Checkpoint für {model_name} gefunden!\")\n",
    "\n",
    "print(\"✅ Alle Checkpoints wurden gesichert!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
