{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: MAIN_FOLDER_HYP = /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small\n",
      "DEBUG: Exists on disk? = True\n",
      "\n",
      "DEBUG: Subfolders in HyperparameterLOG:\n",
      " - fcn_resnet101\n",
      " - deeplabv3_resnet50\n",
      " - fcn_resnet50\n",
      " - deeplabv3_resnet101\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 1: Imports & User Inputs\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import shutil\n",
    "import ray.cloudpickle as pickle\n",
    "from tqdm import tqdm\n",
    "from Helper.ml_models import *  # Importiert alle benötigten ML-Modelle (z. B. MapillaryTrainedModel, MapillaryDataLoader, …)\n",
    "\n",
    "# === Basis-Pfade und Parameter ===\n",
    "BASE_PATH = \"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation\"\n",
    "MAIN_FOLDER_HYP = os.path.abspath(os.path.join(BASE_PATH, \"HyperparameterLOG_small\"))\n",
    "TARGET_DIR = \"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/hyper_map_small\"\n",
    "\n",
    "# Evaluationseinstellungen\n",
    "NUM_CLASSES = 66  # Neue Klassenanzahl\n",
    "\n",
    "# Speicherpfade\n",
    "EVAL_RESULTS_SAVE_PATH = os.path.join(BASE_PATH, \"FINAL_DATEN\", \"evaluation_hyperparameter_Mapillary_small.json\")\n",
    "BEST_CHECKPOINTS_SAVE_PATH = os.path.join(BASE_PATH, \"FINAL_DATEN\", \"best_checkpoints_Mapillary_small.json\")\n",
    "\n",
    "# Debug: Ausgabe der HyperparameterLOG-Unterordner\n",
    "print(\"DEBUG: MAIN_FOLDER_HYP =\", MAIN_FOLDER_HYP)\n",
    "print(\"DEBUG: Exists on disk? =\", os.path.isdir(MAIN_FOLDER_HYP))\n",
    "print(\"\\nDEBUG: Subfolders in HyperparameterLOG:\")\n",
    "for item in os.listdir(MAIN_FOLDER_HYP):\n",
    "    if os.path.isdir(os.path.join(MAIN_FOLDER_HYP, item)):\n",
    "        print(\" -\", item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2: Funktionendefinitionen (kollabierbarer Bereich)\n",
    "def load_hyperparameter_runs_as_dict(base_folder: str):\n",
    "    \"\"\"\n",
    "    Lädt alle Hyperparameter-Runs aus der Ordnerstruktur als Dictionary.\n",
    "    \"\"\"\n",
    "    runs_data = {}\n",
    "    if not os.path.isdir(base_folder):\n",
    "        print(f\"ERROR: Base folder does not exist: {base_folder}\")\n",
    "        return runs_data\n",
    "\n",
    "    for model_folder in os.listdir(base_folder):\n",
    "        model_path = os.path.join(base_folder, model_folder)\n",
    "        if not os.path.isdir(model_path):\n",
    "            print(f\"Skipping invalid model path: {model_path}\")\n",
    "            continue\n",
    "\n",
    "        runs_data[model_folder] = {}\n",
    "        for train_folder in os.listdir(model_path):\n",
    "            train_folder_path = os.path.join(model_path, train_folder)\n",
    "            if not train_folder.startswith(\"train_hyper_\") or not os.path.isdir(train_folder_path):\n",
    "                print(f\"Skipping non-training folder: {train_folder_path}\")\n",
    "                continue\n",
    "\n",
    "            params_file = os.path.join(train_folder_path, \"params.json\")\n",
    "            progress_file = os.path.join(train_folder_path, \"progress.csv\")\n",
    "            result_file = os.path.join(train_folder_path, \"result.json\")\n",
    "\n",
    "            if not (os.path.isfile(params_file) and os.path.isfile(progress_file) and os.path.isfile(result_file)):\n",
    "                print(f\"Skipping incomplete run: {train_folder_path}\")\n",
    "                continue\n",
    "\n",
    "            with open(params_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                params_dict = json.load(f)\n",
    "            progress_df = pd.read_csv(progress_file)\n",
    "            progress_records = progress_df.to_dict(orient=\"records\")\n",
    "            result_records = []\n",
    "            with open(result_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        result_records.append(json.loads(line))\n",
    "\n",
    "            run_dict = {\n",
    "                \"id\": train_folder,\n",
    "                **params_dict,\n",
    "                \"result\": result_records,\n",
    "                \"progress\": progress_records,\n",
    "            }\n",
    "            runs_data[model_folder][train_folder] = run_dict\n",
    "\n",
    "    return runs_data\n",
    "\n",
    "\n",
    "def compute_confusion_matrix(predicted, ground_truth, num_classes):\n",
    "    mask = (ground_truth >= 0) & (ground_truth < num_classes)\n",
    "    label = num_classes * ground_truth[mask] + predicted[mask]\n",
    "    count = torch.bincount(label, minlength=num_classes**2)\n",
    "    confusion_matrix = count.reshape(num_classes, num_classes)\n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "def compute_miou(confusion_matrix):\n",
    "    intersection = torch.diag(confusion_matrix)\n",
    "    ground_truth_set = confusion_matrix.sum(1)\n",
    "    predicted_set = confusion_matrix.sum(0)\n",
    "    union = ground_truth_set + predicted_set - intersection\n",
    "    IoU = intersection / (union + 1e-6)\n",
    "    mIoU = torch.mean(IoU)\n",
    "    return mIoU.item(), IoU\n",
    "\n",
    "\n",
    "def compute_mean_pixel_accuracy(confusion_matrix):\n",
    "    true_positive = torch.diag(confusion_matrix)\n",
    "    total_pixels = confusion_matrix.sum(1)\n",
    "    pixel_accuracy = true_positive / (total_pixels + 1e-6)\n",
    "    mPA = torch.mean(pixel_accuracy)\n",
    "    return mPA.item(), pixel_accuracy\n",
    "\n",
    "\n",
    "def compute_fwiou(confusion_matrix):\n",
    "    total_pixels = confusion_matrix.sum()\n",
    "    ground_truth_set = confusion_matrix.sum(1)\n",
    "    intersection = torch.diag(confusion_matrix)\n",
    "    union = ground_truth_set + confusion_matrix.sum(0) - intersection\n",
    "    IoU = intersection / (union + 1e-6)\n",
    "    FWIoU = (ground_truth_set * IoU) / total_pixels\n",
    "    return FWIoU.sum().item()\n",
    "\n",
    "\n",
    "def compute_dice_coefficient(confusion_matrix):\n",
    "    intersection = torch.diag(confusion_matrix)\n",
    "    ground_truth_set = confusion_matrix.sum(1)\n",
    "    predicted_set = confusion_matrix.sum(0)\n",
    "    dice = (2 * intersection) / (ground_truth_set + predicted_set + 1e-6)\n",
    "    mean_dice = torch.mean(dice)\n",
    "    return mean_dice.item(), dice\n",
    "\n",
    "\n",
    "def load_checkpointed_model_ray(model_name, checkpoint_path, num_classes=None):\n",
    "    \"\"\"\n",
    "    Lädt ein checkpointed Model mithilfe von Ray Cloudpickle.\n",
    "    \"\"\"\n",
    "    loaded_model = MapillaryTrainedModel(\n",
    "        model_name=model_name,\n",
    "        width=520,\n",
    "        height=520,\n",
    "        weights_name='',\n",
    "        skip_local_load=True  # WICHTIG!\n",
    "    )\n",
    "    with open(checkpoint_path, \"rb\") as fp:\n",
    "        checkpoint_data = pickle.load(fp)\n",
    "    loaded_model.model.load_state_dict(checkpoint_data[\"model_state\"], strict=True)\n",
    "    if \"optimizer_state\" in checkpoint_data:\n",
    "        loaded_model.optimizer.load_state_dict(checkpoint_data[\"optimizer_state\"])\n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "def evaluate_model(model: MapillaryTrainedModel, dataset, num_classes: int) -> dict:\n",
    "    \"\"\"\n",
    "    Bewertet das Modell auf dem angegebenen Datensatz und berechnet diverse Metriken.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.model.to(device)\n",
    "    confusion_matrix_total = torch.zeros((num_classes, num_classes), dtype=torch.int64).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(dataset)), desc=\"Evaluating Dataset\"):\n",
    "            image, annotation = dataset[i]\n",
    "            image = image.to(device)\n",
    "            annotation = annotation.to(device)\n",
    "            output = model.inference(image)\n",
    "            predicted = output.argmax(1).squeeze(0)\n",
    "            conf_mat = compute_confusion_matrix(predicted.cpu(), annotation.cpu(), num_classes)\n",
    "            confusion_matrix_total += conf_mat.to(device)\n",
    "    \n",
    "    miou, iou_per_class = compute_miou(confusion_matrix_total)\n",
    "    mpa, pa_per_class = compute_mean_pixel_accuracy(confusion_matrix_total)\n",
    "    fwiou = compute_fwiou(confusion_matrix_total)\n",
    "    dice_mean, dice_per_class = compute_dice_coefficient(confusion_matrix_total)\n",
    "    \n",
    "    metrics = {\n",
    "        \"mIoU\": miou,\n",
    "        \"mPA\": mpa,\n",
    "        \"FWIoU\": fwiou,\n",
    "        \"Dice_Mean\": dice_mean,\n",
    "        \"IoU_per_class\": iou_per_class.tolist(),\n",
    "        \"PA_per_class\": pa_per_class.tolist(),\n",
    "        \"Dice_per_class\": dice_per_class.tolist()\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/fcn_resnet101/tuner.pkl\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/fcn_resnet101/searcher-state-2025-03-11_12-03-54.pkl\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/fcn_resnet101/search_gen_state-2025-03-11_12-03-54.json\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/fcn_resnet101/experiment_state-2025-03-11_12-03-54.json\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/fcn_resnet101/.validate_storage_marker\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/deeplabv3_resnet50/tuner.pkl\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/deeplabv3_resnet50/searcher-state-2025-03-07_20-02-56.pkl\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/deeplabv3_resnet50/search_gen_state-2025-03-07_20-02-56.json\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/deeplabv3_resnet50/experiment_state-2025-03-07_20-02-56.json\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/deeplabv3_resnet50/.validate_storage_marker\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/fcn_resnet50/search_gen_state-2025-03-10_11-25-21.json\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/fcn_resnet50/tuner.pkl\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/fcn_resnet50/searcher-state-2025-03-10_11-25-21.pkl\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/fcn_resnet50/.validate_storage_marker\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/fcn_resnet50/experiment_state-2025-03-10_11-25-21.json\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/deeplabv3_resnet101/searcher-state-2025-03-09_00-01-13.pkl\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/deeplabv3_resnet101/tuner.pkl\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/deeplabv3_resnet101/search_gen_state-2025-03-09_00-01-13.json\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/deeplabv3_resnet101/experiment_state-2025-03-09_00-01-13.json\n",
      "Skipping non-training folder: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/deeplabv3_resnet101/.validate_storage_marker\n",
      "\n",
      "[DEBUG] For model='fcn_resnet101' run='train_hyper_7adb91eb_1_auto_cast=True,batch_size=6,learning_rate=0.0001,max_epochs=100,weight_decay=0_2025-03-11_12-03-54', found checkpoint dirs:\n",
      "        ['checkpoint_000099', 'checkpoint_000095', 'checkpoint_000080', 'checkpoint_000074', 'checkpoint_000082', 'checkpoint_000062']\n",
      "\n",
      "[DEBUG] For model='deeplabv3_resnet50' run='train_hyper_a8ae06ed_1_auto_cast=True,batch_size=6,learning_rate=0.0001,max_epochs=100,weight_decay=0_2025-03-07_20-02-56', found checkpoint dirs:\n",
      "        ['checkpoint_000093', 'checkpoint_000099', 'checkpoint_000094', 'checkpoint_000073', 'checkpoint_000097']\n",
      "\n",
      "[DEBUG] For model='fcn_resnet50' run='train_hyper_bc32f73a_1_auto_cast=True,batch_size=6,learning_rate=0.0001,max_epochs=100,weight_decay=0_2025-03-10_11-25-21', found checkpoint dirs:\n",
      "        ['checkpoint_000096', 'checkpoint_000099', 'checkpoint_000068', 'checkpoint_000082', 'checkpoint_000076', 'checkpoint_000044']\n",
      "\n",
      "[DEBUG] For model='deeplabv3_resnet101' run='train_hyper_ae167f80_1_auto_cast=True,batch_size=6,learning_rate=0.0001,max_epochs=100,weight_decay=0_2025-03-09_00-01-13', found checkpoint dirs:\n",
      "        ['checkpoint_000099', 'checkpoint_000095', 'checkpoint_000080', 'checkpoint_000090', 'checkpoint_000097']\n",
      "\n",
      "DEBUG: Keys for deeplabv3_resnet50 run '0': dict_keys(['id', 'auto_cast', 'batch_size', 'learning_rate', 'max_epochs', 'weight_decay', 'result', 'progress', 'max_validation_accuracy', 'path'])\n",
      "DEBUG: Checkpoint path: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/deeplabv3_resnet50/train_hyper_a8ae06ed_1_auto_cast=True,batch_size=6,learning_rate=0.0001,max_epochs=100,weight_decay=0_2025-03-07_20-02-56/checkpoint_000099/checkpoint.pkl\n",
      "DEBUG: Models found: ['fcn_resnet101', 'deeplabv3_resnet50', 'fcn_resnet50', 'deeplabv3_resnet101']\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 3A: Daten laden & Checkpoint-Extraktion\n",
    "# 1. Hyperparameter-Runs laden\n",
    "hyperparameter_data = load_hyperparameter_runs_as_dict(MAIN_FOLDER_HYP)\n",
    "\n",
    "# 2. Checkpoints finden und nach bestmöglicher Validierungsgenauigkeit sortieren\n",
    "sorted_hyperparameter_data = {}\n",
    "\n",
    "for model_name, runs_dict in hyperparameter_data.items():\n",
    "    runs_list = []\n",
    "    for run_name, run_data in runs_dict.items():\n",
    "        # Bestimme die höchste Validierungsgenauigkeit aus der \"progress\"-CSV\n",
    "        if \"progress\" in run_data and run_data[\"progress\"]:\n",
    "            best_val_acc = max((r.get(\"val_acc\", float(\"-inf\")) for r in run_data[\"progress\"]), default=float(\"-inf\"))\n",
    "        else:\n",
    "            best_val_acc = float(\"-inf\")\n",
    "        \n",
    "        # Absoluter Pfad zum Run-Ordner\n",
    "        run_folder_path = os.path.abspath(os.path.join(MAIN_FOLDER_HYP, model_name, run_name))\n",
    "        \n",
    "        # Checkpoint-Unterordner suchen\n",
    "        if os.path.isdir(run_folder_path):\n",
    "            checkpoint_dirs = [d for d in os.listdir(run_folder_path)\n",
    "                               if d.startswith(\"checkpoint_\") and os.path.isdir(os.path.join(run_folder_path, d))]\n",
    "        else:\n",
    "            checkpoint_dirs = []\n",
    "        \n",
    "        print(f\"\\n[DEBUG] For model='{model_name}' run='{run_name}', found checkpoint dirs:\")\n",
    "        print(\"       \", checkpoint_dirs)\n",
    "        \n",
    "        # Numerische Sortierung der Checkpoint-Ordner und Auswahl des letzten\n",
    "        if checkpoint_dirs:\n",
    "            checkpoint_dirs.sort(key=lambda x: int(x.split(\"_\")[1]))\n",
    "            last_checkpoint_dir = checkpoint_dirs[-1]\n",
    "            last_checkpoint = os.path.abspath(os.path.join(run_folder_path, last_checkpoint_dir, \"checkpoint.pkl\"))\n",
    "        else:\n",
    "            last_checkpoint = None\n",
    "        \n",
    "        runs_list.append((run_name, run_data, best_val_acc, last_checkpoint))\n",
    "    \n",
    "    # Sortiere Runs absteigend nach Validierungsgenauigkeit\n",
    "    sorted_runs = sorted(runs_list, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Erstelle ein neues Dictionary mit fortlaufenden Schlüsseln (\"0\", \"1\", …)\n",
    "    sorted_hyperparameter_data[model_name] = {\n",
    "        str(i): {\n",
    "            **run_data,\n",
    "            \"max_validation_accuracy\": best_val_acc,\n",
    "            \"path\": last_checkpoint\n",
    "        }\n",
    "        for i, (run_name, run_data, best_val_acc, last_checkpoint) in enumerate(sorted_runs)\n",
    "    }\n",
    "\n",
    "# Optionaler Debug-Output\n",
    "if \"deeplabv3_resnet50\" in sorted_hyperparameter_data:\n",
    "    print(\"\\nDEBUG: Keys for deeplabv3_resnet50 run '0':\", sorted_hyperparameter_data[\"deeplabv3_resnet50\"]['0'].keys())\n",
    "    print(\"DEBUG: Checkpoint path:\", sorted_hyperparameter_data[\"deeplabv3_resnet50\"]['0']['path'])\n",
    "print(\"DEBUG: Models found:\", list(sorted_hyperparameter_data.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with length 2000\n",
      "\n",
      "Evaluating model: fcn_resnet101\n",
      "Checkpoint path: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/fcn_resnet101/train_hyper_7adb91eb_1_auto_cast=True,batch_size=6,learning_rate=0.0001,max_epochs=100,weight_decay=0_2025-03-11_12-03-54/checkpoint_000099/checkpoint.pkl\n",
      "Using CUDA GPU\n",
      "Model loaded: fcn_resnet101 | Device: cuda \n",
      "Error loading Model with Epoch latest: Error(s) in loading state_dict for FCN:\n",
      "\tsize mismatch for classifier.4.weight: copying a param with shape torch.Size([20, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([124, 512, 1, 1]).\n",
      "\tsize mismatch for classifier.4.bias: copying a param with shape torch.Size([20]) from checkpoint, the shape in current model is torch.Size([124]).\n",
      "Skipping local .pth load due to error above.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Dataset: 100%|██████████| 2000/2000 [03:25<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for fcn_resnet101:\n",
      "  mIoU: 0.2044\n",
      "  mPA: 0.2459\n",
      "  FWIoU: 0.8601\n",
      "  Dice_Mean: 0.2665\n",
      "  IoU_per_class: [list of length 66]\n",
      "  PA_per_class: [list of length 66]\n",
      "  Dice_per_class: [list of length 66]\n",
      "\n",
      "Evaluating model: deeplabv3_resnet50\n",
      "Checkpoint path: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/deeplabv3_resnet50/train_hyper_a8ae06ed_1_auto_cast=True,batch_size=6,learning_rate=0.0001,max_epochs=100,weight_decay=0_2025-03-07_20-02-56/checkpoint_000099/checkpoint.pkl\n",
      "Using CUDA GPU\n",
      "Model loaded: deeplabv3_resnet50 | Device: cuda \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Dataset: 100%|██████████| 2000/2000 [02:51<00:00, 11.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for deeplabv3_resnet50:\n",
      "  mIoU: 0.2009\n",
      "  mPA: 0.2478\n",
      "  FWIoU: 0.8614\n",
      "  Dice_Mean: 0.2604\n",
      "  IoU_per_class: [list of length 66]\n",
      "  PA_per_class: [list of length 66]\n",
      "  Dice_per_class: [list of length 66]\n",
      "\n",
      "Evaluating model: fcn_resnet50\n",
      "Checkpoint path: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/fcn_resnet50/train_hyper_bc32f73a_1_auto_cast=True,batch_size=6,learning_rate=0.0001,max_epochs=100,weight_decay=0_2025-03-10_11-25-21/checkpoint_000099/checkpoint.pkl\n",
      "Using CUDA GPU\n",
      "Model loaded: fcn_resnet50 | Device: cuda \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Dataset: 100%|██████████| 2000/2000 [02:47<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for fcn_resnet50:\n",
      "  mIoU: 0.2012\n",
      "  mPA: 0.2426\n",
      "  FWIoU: 0.8544\n",
      "  Dice_Mean: 0.2632\n",
      "  IoU_per_class: [list of length 66]\n",
      "  PA_per_class: [list of length 66]\n",
      "  Dice_per_class: [list of length 66]\n",
      "\n",
      "Evaluating model: deeplabv3_resnet101\n",
      "Checkpoint path: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_small/deeplabv3_resnet101/train_hyper_ae167f80_1_auto_cast=True,batch_size=6,learning_rate=0.0001,max_epochs=100,weight_decay=0_2025-03-09_00-01-13/checkpoint_000099/checkpoint.pkl\n",
      "Using CUDA GPU\n",
      "Model loaded: deeplabv3_resnet101 | Device: cuda \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Dataset: 100%|██████████| 2000/2000 [03:15<00:00, 10.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for deeplabv3_resnet101:\n",
      "  mIoU: 0.2087\n",
      "  mPA: 0.2562\n",
      "  FWIoU: 0.8661\n",
      "  Dice_Mean: 0.2697\n",
      "  IoU_per_class: [list of length 66]\n",
      "  PA_per_class: [list of length 66]\n",
      "  Dice_per_class: [list of length 66]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 3B: Datensatz laden & Modelle evaluieren\n",
    "# Datensatz laden (Mapillary)\n",
    "mapillary_loader = MapillaryDataLoader(\n",
    "    train_images_dir=os.path.join(BASE_PATH, \"Mapillary_Vistas/training/images\"),\n",
    "    train_annotations_dir=os.path.join(BASE_PATH, \"Mapillary_Vistas/training/v2.0/labels_small\"),\n",
    "    val_images_dir=os.path.join(BASE_PATH, \"Mapillary_Vistas/validation/images\"),\n",
    "    val_annotations_dir=os.path.join(BASE_PATH, \"Mapillary_Vistas/validation/v2.0/labels_small\")\n",
    ")\n",
    "test_dataset = mapillary_loader.test_dataset\n",
    "print(f\"Dataset loaded with length {len(test_dataset)}\")\n",
    "\n",
    "# Modelle evaluieren (verwende jeweils den besten Run \"0\")\n",
    "evaluation_results = {}\n",
    "\n",
    "for model_name, runs_dict in sorted_hyperparameter_data.items():\n",
    "    best_run_info = runs_dict.get(\"0\")\n",
    "    checkpoint_path = best_run_info.get(\"path\", None) if best_run_info else None\n",
    "    if not checkpoint_path:\n",
    "        print(f\"\\n[WARNING] No checkpoint path found for {model_name} run '0'. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    print(f\"Checkpoint path: {checkpoint_path}\")\n",
    "    \n",
    "    try:\n",
    "        model_loaded = load_checkpointed_model_ray(model_name, checkpoint_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(\"[WARNING] File not found error:\", e)\n",
    "        continue\n",
    "    \n",
    "    metrics = evaluate_model(model_loaded, test_dataset, NUM_CLASSES)\n",
    "    evaluation_results[model_name] = metrics\n",
    "    \n",
    "    print(f\"Results for {model_name}:\")\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, list):\n",
    "            print(f\"  {k}: [list of length {len(v)}]\")\n",
    "        else:\n",
    "            print(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results saved to: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/evaluation_hyperparameter_Mapillary_small.json\n",
      "Best checkpoint paths saved to: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/best_checkpoints_Mapillary_small.json\n",
      "✅ Checkpoint for fcn_resnet101 saved: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/hyper_map_small/fcn_resnet101_best_checkpoint.pkl\n",
      "✅ Checkpoint for deeplabv3_resnet50 saved: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/hyper_map_small/deeplabv3_resnet50_best_checkpoint.pkl\n",
      "✅ Checkpoint for fcn_resnet50 saved: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/hyper_map_small/fcn_resnet50_best_checkpoint.pkl\n",
      "✅ Checkpoint for deeplabv3_resnet101 saved: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/hyper_map_small/deeplabv3_resnet101_best_checkpoint.pkl\n",
      "✅ All checkpoints have been saved!\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 3C: Ergebnisse speichern & Best Checkpoints kopieren\n",
    "# Speichere die Evaluationsergebnisse als JSON\n",
    "os.makedirs(os.path.dirname(EVAL_RESULTS_SAVE_PATH), exist_ok=True)\n",
    "with open(EVAL_RESULTS_SAVE_PATH, \"w\") as f:\n",
    "    json.dump(evaluation_results, f, indent=4)\n",
    "print(f\"Evaluation results saved to: {EVAL_RESULTS_SAVE_PATH}\")\n",
    "\n",
    "# Best Checkpoints extrahieren\n",
    "best_checkpoints = {}\n",
    "for model_name, runs_dict in sorted_hyperparameter_data.items():\n",
    "    best_run_info = runs_dict.get(\"0\")\n",
    "    best_checkpoint_path = best_run_info.get(\"path\", None) if best_run_info else None\n",
    "    best_checkpoints[model_name] = best_checkpoint_path\n",
    "\n",
    "# Speichere die Best Checkpoints als JSON\n",
    "os.makedirs(os.path.dirname(BEST_CHECKPOINTS_SAVE_PATH), exist_ok=True)\n",
    "with open(BEST_CHECKPOINTS_SAVE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_checkpoints, f, indent=4)\n",
    "print(f\"Best checkpoint paths saved to: {BEST_CHECKPOINTS_SAVE_PATH}\")\n",
    "\n",
    "# Kopiere die besten Checkpoints in das Zielverzeichnis\n",
    "os.makedirs(TARGET_DIR, exist_ok=True)\n",
    "for model_name, checkpoint_path in best_checkpoints.items():\n",
    "    if checkpoint_path and os.path.isfile(checkpoint_path):\n",
    "        dest_checkpoint = os.path.join(TARGET_DIR, f\"{model_name}_best_checkpoint.pkl\")\n",
    "        shutil.copy2(checkpoint_path, dest_checkpoint)\n",
    "        print(f\"✅ Checkpoint for {model_name} saved: {dest_checkpoint}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No valid checkpoint for {model_name} found!\")\n",
    "print(\"✅ All checkpoints have been saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
