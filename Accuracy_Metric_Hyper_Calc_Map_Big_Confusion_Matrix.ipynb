{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2025-03-22 11:53:13.292438: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-22 11:53:13.837353: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_PATH: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation\n",
      "CONF_MATRIX_DIR: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/confusion_matrices_hyper_big\n",
      "\n",
      "Dataset-Pfade:\n",
      "  Train Images      : /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Mapillary_Vistas/training/images\n",
      "  Train Annotations : /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Mapillary_Vistas/training/v2.0/labels_big\n",
      "  Validation Images : /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Mapillary_Vistas/validation/images\n",
      "  Validation Annotations: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Mapillary_Vistas/validation/v2.0/labels_big\n",
      "\n",
      "Best Checkpoints JSON: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/best_checkpoints_Mapillary.json\n",
      "NUM_CLASSES_MAPILLARY: 124\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ray.cloudpickle as pickle  # alias für pickle\n",
    "from Helper.ml_models import *   # Importiere deine Modelle und DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Basisverzeichnis des Projekts\n",
    "BASE_PATH = \"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation\"\n",
    "\n",
    "# Verzeichnis zur Speicherung der Confusion Matrices (für Mapillary)\n",
    "CONF_MATRIX_DIR = os.path.join(BASE_PATH, \"FINAL_DATEN/confusion_matrices_hyper_big\")\n",
    "os.makedirs(CONF_MATRIX_DIR, exist_ok=True)\n",
    "\n",
    "# Pfade zu den Mapillary-Datensätzen\n",
    "TRAIN_IMAGES_DIR = os.path.join(BASE_PATH, \"Mapillary_Vistas/training/images\")\n",
    "TRAIN_ANNOTATIONS_DIR = os.path.join(BASE_PATH, \"Mapillary_Vistas/training/v2.0/labels_big\")\n",
    "VAL_IMAGES_DIR = os.path.join(BASE_PATH, \"Mapillary_Vistas/validation/images\")\n",
    "VAL_ANNOTATIONS_DIR = os.path.join(BASE_PATH, \"Mapillary_Vistas/validation/v2.0/labels_big\")\n",
    "\n",
    "# Evaluationseinstellungen: Für Mapillary mit mehr Labels (z. B. 124 Klassen)\n",
    "NUM_CLASSES_MAPILLARY = 124\n",
    "\n",
    "# Pfad zur Datei mit den besten Checkpoints (Mapillary)\n",
    "BEST_CHECKPOINTS_JSON = os.path.join(BASE_PATH, \"FINAL_DATEN/best_checkpoints_Mapillary.json\")\n",
    "\n",
    "# Ausgabe zur Kontrolle\n",
    "print(\"BASE_PATH:\", BASE_PATH)\n",
    "print(\"CONF_MATRIX_DIR:\", CONF_MATRIX_DIR)\n",
    "print(\"\\nDataset-Pfade:\")\n",
    "print(\"  Train Images      :\", TRAIN_IMAGES_DIR)\n",
    "print(\"  Train Annotations :\", TRAIN_ANNOTATIONS_DIR)\n",
    "print(\"  Validation Images :\", VAL_IMAGES_DIR)\n",
    "print(\"  Validation Annotations:\", VAL_ANNOTATIONS_DIR)\n",
    "print(\"\\nBest Checkpoints JSON:\", BEST_CHECKPOINTS_JSON)\n",
    "print(\"NUM_CLASSES_MAPILLARY:\", NUM_CLASSES_MAPILLARY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefundene Modelle:\n",
      "  fcn_resnet101: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_Big/fcn_resnet101/train_hyper_c75247d6_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-02-22_22-23-45/checkpoint_000099/checkpoint.pkl\n",
      "  deeplabv3_resnet50: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_Big/deeplabv3_resnet50/train_hyper_56ebb659_2_auto_cast=True,batch_size=8,learning_rate=0.0000,max_epochs=100,weight_decay=0.0007_2025-02-16_19-30-53/checkpoint_000099/checkpoint.pkl\n",
      "  fcn_resnet50: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_Big/fcn_resnet50/train_hyper_4b822bcf_2_auto_cast=True,batch_size=8,learning_rate=0.0000,max_epochs=100,weight_decay=0.0007_2025-02-20_17-09-00/checkpoint_000099/checkpoint.pkl\n",
      "  deeplabv3_resnet101: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG_Big/deeplabv3_resnet101/train_hyper_0951e564_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-02-19_04-23-11/checkpoint_000099/checkpoint.pkl\n"
     ]
    }
   ],
   "source": [
    "# Die JSON-Datei enthält die besten Konfigurationen.\n",
    "# Möglicherweise ist der Wert direkt der Checkpoint-Pfad (als String)\n",
    "with open(BEST_CHECKPOINTS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    best_checkpoints_mapillary = json.load(f)\n",
    "\n",
    "print(\"Gefundene Modelle:\")\n",
    "for model_name, info in best_checkpoints_mapillary.items():\n",
    "    # Falls info ein Dictionary ist, versuchen wir den Pfad daraus zu extrahieren,\n",
    "    # andernfalls gehen wir davon aus, dass info direkt der Pfad ist.\n",
    "    if isinstance(info, dict):\n",
    "        checkpoint_path = info.get(\"path\", \"Kein Checkpoint\")\n",
    "    else:\n",
    "        checkpoint_path = info\n",
    "    print(f\"  {model_name}: {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-Datensatz geladen: 2000 Samples\n"
     ]
    }
   ],
   "source": [
    "# Wir gehen davon aus, dass du einen MapillaryDataLoader in Helper.ml_models definiert hast.\n",
    "# Dieser liefert hier das Testset, auf dem die Confusion Matrix berechnet wird.\n",
    "mapillary_loader = MapillaryDataLoader(\n",
    "    train_images_dir=TRAIN_IMAGES_DIR,\n",
    "    train_annotations_dir=TRAIN_ANNOTATIONS_DIR,\n",
    "    val_images_dir=VAL_IMAGES_DIR,\n",
    "    val_annotations_dir=VAL_ANNOTATIONS_DIR\n",
    ")\n",
    "test_dataset = mapillary_loader.test_dataset  # oder ein passendes Attribut\n",
    "print(f\"Test-Datensatz geladen: {len(test_dataset)} Samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpointed_model_ray(model_name, checkpoint_path, num_classes=None):\n",
    "    \"\"\"\n",
    "    Lädt ein Modell aus einem Checkpoint. Das Modell wird über MapillaryTrainedModel instanziiert.\n",
    "    Der Parameter 'skip_local_load' sorgt dafür, dass keine lokalen Gewichte überschrieben werden.\n",
    "    \"\"\"\n",
    "    # Instanziiere das Modell (passe width, height ggf. an)\n",
    "    loaded_model = MapillaryTrainedModel(\n",
    "        model_name=model_name,\n",
    "        width=520,\n",
    "        height=520,\n",
    "        weights_name='',\n",
    "        skip_local_load=True  # WICHTIG!\n",
    "    )\n",
    "    # Lade den Checkpoint mit ray.cloudpickle (alias pickle)\n",
    "    with open(checkpoint_path, \"rb\") as fp:\n",
    "        checkpoint_data = pickle.load(fp)\n",
    "    # Lade die Modellgewichte\n",
    "    loaded_model.model.load_state_dict(checkpoint_data[\"model_state\"], strict=True)\n",
    "    # Falls vorhanden, lade auch den Optimizer-Zustand\n",
    "    if \"optimizer_state\" in checkpoint_data:\n",
    "        loaded_model.optimizer.load_state_dict(checkpoint_data[\"optimizer_state\"])\n",
    "    return loaded_model\n",
    "\n",
    "def compute_confusion_matrix(predicted, ground_truth, num_classes):\n",
    "    \"\"\"\n",
    "    Berechnet die Confusion Matrix für ein Bild.\n",
    "    \"\"\"\n",
    "    mask = (ground_truth >= 0) & (ground_truth < num_classes)\n",
    "    labels = num_classes * ground_truth[mask] + predicted[mask]\n",
    "    count = torch.bincount(labels, minlength=num_classes**2)\n",
    "    cm = count.reshape(num_classes, num_classes)\n",
    "    return cm\n",
    "\n",
    "def evaluate_and_store_confusion_matrix(model, dataset, num_classes, save_path):\n",
    "    \"\"\"\n",
    "    Evaluiert das Modell über den gesamten Datensatz, summiert die Confusion Matrix und speichert sie.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.model.to(device)\n",
    "    confusion_matrix_total = torch.zeros((num_classes, num_classes), dtype=torch.int64).to(device)\n",
    "    \n",
    "    model.model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(dataset)), desc=\"Evaluating Dataset\"):\n",
    "            image, annotation = dataset[i]\n",
    "            image = image.to(device)\n",
    "            annotation = annotation.to(device)\n",
    "            output = model.inference(image)\n",
    "            predicted = output.argmax(1).squeeze(0)\n",
    "            cm = compute_confusion_matrix(predicted.cpu(), annotation.cpu(), num_classes)\n",
    "            confusion_matrix_total += cm.to(device)\n",
    "    \n",
    "    torch.save(confusion_matrix_total, save_path)\n",
    "    print(f\"Confusion Matrix gespeichert: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Kein gültiger Checkpoint für fcn_resnet101. Überspringe.\n",
      "[WARNING] Kein gültiger Checkpoint für deeplabv3_resnet50. Überspringe.\n",
      "[WARNING] Kein gültiger Checkpoint für fcn_resnet50. Überspringe.\n",
      "[WARNING] Kein gültiger Checkpoint für deeplabv3_resnet101. Überspringe.\n"
     ]
    }
   ],
   "source": [
    "# Iteriere über alle Modelle aus best_checkpoints_mapillary\n",
    "for model_name, info in best_checkpoints_mapillary.items():\n",
    "    # Falls info ein Dictionary ist, extrahiere den Pfad, ansonsten setze info direkt als Pfad.\n",
    "    if isinstance(info, dict):\n",
    "        checkpoint_path = info.get(\"path\", None)\n",
    "    else:\n",
    "        checkpoint_path = info\n",
    "\n",
    "    if not checkpoint_path or not os.path.isfile(checkpoint_path):\n",
    "        print(f\"[WARNING] Kein gültiger Checkpoint für {model_name}. Überspringe.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nEvaluierung von Modell: {model_name}\")\n",
    "    print(f\"Verwende Checkpoint: {checkpoint_path}\")\n",
    "\n",
    "    try:\n",
    "        model_loaded = load_checkpointed_model_ray(model_name, checkpoint_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Modell {model_name} konnte nicht geladen werden: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Speicherpfad für die Confusion Matrix\n",
    "    conf_matrix_save_path = os.path.join(CONF_MATRIX_DIR, f\"{model_name}_confusion_matrix.pt\")\n",
    "    evaluate_and_store_confusion_matrix(model_loaded, test_dataset, NUM_CLASSES_MAPILLARY, conf_matrix_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste aller gespeicherten Confusion Matrix-Dateien in: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/confusion_matrices_hyper_big\n",
      "Summary gespeichert unter: /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/FINAL_DATEN/confusion_matrices_hyper_big/confusion_matrices_summary.json\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "print(\"Liste aller gespeicherten Confusion Matrix-Dateien in:\", CONF_MATRIX_DIR)\n",
    "conf_files = glob.glob(os.path.join(CONF_MATRIX_DIR, \"*_confusion_matrix.pt\"))\n",
    "summary = {}\n",
    "for file in conf_files:\n",
    "    cm = torch.load(file)\n",
    "    shape = cm.shape\n",
    "    model_name = os.path.basename(file).split(\"_confusion_matrix.pt\")[0]\n",
    "    summary[model_name] = {\"path\": file, \"shape\": list(shape)}\n",
    "    print(f\"Modell: {model_name}, Matrix-Shape: {shape}\")\n",
    "\n",
    "# Speichere die Zusammenfassung als JSON\n",
    "summary_save_path = os.path.join(CONF_MATRIX_DIR, \"confusion_matrices_summary.json\")\n",
    "with open(summary_save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=4)\n",
    "\n",
    "print(f\"Summary gespeichert unter: {summary_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
