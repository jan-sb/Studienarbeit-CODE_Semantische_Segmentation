{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2025-03-04 11:54:28.068419: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-04 11:54:28.614484: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA GPU\n",
      "Model loaded: fcn_resnet101 | Device: cuda \n",
      "Error loading Model with Epoch latest: Error(s) in loading state_dict for FCN:\n",
      "\tsize mismatch for classifier.4.weight: copying a param with shape torch.Size([20, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([124, 512, 1, 1]).\n",
      "\tsize mismatch for classifier.4.bias: copying a param with shape torch.Size([20]) from checkpoint, the shape in current model is torch.Size([124]).\n",
      "Skipping local .pth load due to error above.\n",
      "Modellgerät: cuda:0\n",
      "Anzahl der Klassen (num_classes): 124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from Helper.ml_models import MapillaryTrainedModel\n",
    "\n",
    "# Setze hier den gewünschten Modellnamen ein (z.B. 'fcn_resnet101' oder 'deeplabv3_resnet50')\n",
    "model_name = 'fcn_resnet101'\n",
    "\n",
    "# Initialisiere das Modell (verwende einen Dummy-Ordner, falls du nur die Initialisierung testen möchtest)\n",
    "dummy_folder = \"dummy_folder_for_testing\"\n",
    "# Erstelle den Ordner, falls er nicht existiert:\n",
    "import os\n",
    "os.makedirs(dummy_folder, exist_ok=True)\n",
    "\n",
    "model_instance = MapillaryTrainedModel(\n",
    "    model_name=model_name,\n",
    "    width=520,\n",
    "    height=520,\n",
    "    weights_name='',\n",
    "    folder_path=dummy_folder,\n",
    "    start_epoch='latest'\n",
    ")\n",
    "\n",
    "# Ausgabe: Auf welchem Gerät befindet sich das Modell und wie viele Klassen wurden initialisiert?\n",
    "print(\"Modellgerät:\", next(model_instance.model.parameters()).device)\n",
    "print(\"Anzahl der Klassen (num_classes):\", model_instance.num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint classifier.4.weight shape: torch.Size([124, 512, 1, 1])\n",
      "Checkpoint classifier.4.bias shape: torch.Size([124])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ray.cloudpickle as pickle\n",
    "\n",
    "# Gib hier den Pfad zum Checkpoint an, den du laden möchtest.\n",
    "# Passe den Pfad ggf. an den tatsächlichen Speicherort an.\n",
    "checkpoint_path = \"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/fcn_resnet101/train_hyper_c75247d6_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-02-22_22-23-45/checkpoint_000099/checkpoint.pkl\"\n",
    "\n",
    "if os.path.isfile(checkpoint_path):\n",
    "    with open(checkpoint_path, \"rb\") as fp:\n",
    "        checkpoint_data = pickle.load(fp)\n",
    "    \n",
    "    # Überprüfe die Form der Parameter im Klassifikationslayer\n",
    "    if \"classifier.4.weight\" in checkpoint_data[\"model_state\"]:\n",
    "        weight_shape = checkpoint_data[\"model_state\"][\"classifier.4.weight\"].shape\n",
    "        bias_shape = checkpoint_data[\"model_state\"][\"classifier.4.bias\"].shape\n",
    "        print(\"Checkpoint classifier.4.weight shape:\", weight_shape)\n",
    "        print(\"Checkpoint classifier.4.bias shape:\", bias_shape)\n",
    "    else:\n",
    "        print(\"Parameter 'classifier.4.weight' nicht im Checkpoint gefunden.\")\n",
    "else:\n",
    "    print(\"Kein Checkpoint gefunden an:\", checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefundene .pth-Dateien im Modellordner: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Passe diesen Pfad an den Ordner an, der in deinem Modell als folder_path verwendet wird.\n",
    "# In unserem Test haben wir einen Dummy-Ordner benutzt.\n",
    "model_folder = \"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG\"  # oder den tatsächlichen Ordner, der für deine Modelle genutzt wird\n",
    "\n",
    "# Suche nach .pth-Dateien im Modellordner\n",
    "pth_files = [f for f in os.listdir(model_folder) if f.endswith('.pth')]\n",
    "print(\"Gefundene .pth-Dateien im Modellordner:\", pth_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5) REMAINDER OF YOUR EVAL CODE\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "from Helper.ml_models import TrainedModel, K_Fold_Dataset\n",
    "\n",
    "def compute_confusion_matrix(predicted, ground_truth, num_classes):\n",
    "    mask = (ground_truth >= 0) & (ground_truth < num_classes)\n",
    "    label = num_classes * ground_truth[mask] + predicted[mask]\n",
    "    count = torch.bincount(label, minlength=num_classes**2)\n",
    "    confusion_matrix = count.reshape(num_classes, num_classes)\n",
    "    return confusion_matrix\n",
    "\n",
    "def compute_miou(confusion_matrix):\n",
    "    intersection = torch.diag(confusion_matrix)\n",
    "    ground_truth_set = confusion_matrix.sum(1)\n",
    "    predicted_set = confusion_matrix.sum(0)\n",
    "    union = ground_truth_set + predicted_set - intersection\n",
    "    IoU = intersection / (union + 1e-6)\n",
    "    mIoU = torch.mean(IoU)\n",
    "    return mIoU.item(), IoU\n",
    "\n",
    "def compute_mean_pixel_accuracy(confusion_matrix):\n",
    "    true_positive = torch.diag(confusion_matrix)\n",
    "    total_pixels = confusion_matrix.sum(1)\n",
    "    pixel_accuracy = true_positive / (total_pixels + 1e-6)\n",
    "    mPA = torch.mean(pixel_accuracy)\n",
    "    return mPA.item(), pixel_accuracy\n",
    "\n",
    "def compute_fwiou(confusion_matrix):\n",
    "    total_pixels = confusion_matrix.sum()\n",
    "    ground_truth_set = confusion_matrix.sum(1)\n",
    "    intersection = torch.diag(confusion_matrix)\n",
    "    union = ground_truth_set + confusion_matrix.sum(0) - intersection\n",
    "    IoU = intersection / (union + 1e-6)\n",
    "    FWIoU = (ground_truth_set * IoU) / total_pixels\n",
    "    FWIoU = FWIoU.sum()\n",
    "    return FWIoU.item()\n",
    "\n",
    "def compute_dice_coefficient(confusion_matrix):\n",
    "    intersection = torch.diag(confusion_matrix)\n",
    "    ground_truth_set = confusion_matrix.sum(1)\n",
    "    predicted_set = confusion_matrix.sum(0)\n",
    "    dice = (2 * intersection) / (ground_truth_set + predicted_set + 1e-6)\n",
    "    mean_dice = torch.mean(dice)\n",
    "    return mean_dice.item(), dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zielgerät: cuda\n",
      "Output befindet sich auf: cuda:0\n",
      "Predicted Tensor befindet sich auf: cuda:0\n",
      "Annotation befindet sich auf: cuda:0\n",
      "Confusion Matrix (vor .to(device)) befindet sich auf: cpu\n",
      "Confusion Matrix (nach .to(device)) befindet sich auf: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from Helper.ml_models import MapillaryTrainedModel, MapillaryDataLoader\n",
    "NUM_CLASSES = 124\n",
    "\n",
    "mapillary_loader = MapillaryDataLoader(\n",
    "    train_images_dir=\"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Mapillary_Vistas/training/images\",\n",
    "    train_annotations_dir=\"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Mapillary_Vistas/training_own\",\n",
    "    val_images_dir=\"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Mapillary_Vistas/validation/images\",\n",
    "    val_annotations_dir=\"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Mapillary_Vistas/validation_own\"\n",
    ")\n",
    "\n",
    "# Stelle sicher, dass das Testdataset definiert ist:\n",
    "test_dataset = mapillary_loader.test_dataset  # Falls noch nicht gesetzt\n",
    "\n",
    "# Setze das Zielgerät (CUDA oder CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Zielgerät:\", device)\n",
    "\n",
    "# Hole ein einzelnes Beispiel aus dem Testdatensatz\n",
    "sample_image, sample_annotation = test_dataset[0]\n",
    "\n",
    "# Verschiebe die Daten auf das Zielgerät (falls nicht schon geschehen)\n",
    "sample_image_device = sample_image.to(device)\n",
    "sample_annotation_device = sample_annotation.to(device)\n",
    "\n",
    "# Führe die Inferenz mit dem geladenen Modell durch (model_loaded muss vorher erfolgreich geladen worden sein)\n",
    "output = model_instance.inference(sample_image_device)\n",
    "print(\"Output befindet sich auf:\", output.device)\n",
    "\n",
    "# Berechne die Vorhersage und prüfe das Gerät\n",
    "predicted = output.argmax(1).squeeze(0)\n",
    "print(\"Predicted Tensor befindet sich auf:\", predicted.device)\n",
    "print(\"Annotation befindet sich auf:\", sample_annotation_device.device)\n",
    "\n",
    "# Berechne die Confusion Matrix; beachte, dass compute_confusion_matrix mit CPU-Tensoren arbeitet\n",
    "conf_mat = compute_confusion_matrix(predicted.cpu(), sample_annotation_device.cpu(), NUM_CLASSES)\n",
    "print(\"Confusion Matrix (vor .to(device)) befindet sich auf:\", conf_mat.device)\n",
    "\n",
    "# Übertrage die Confusion Matrix auf das Zielgerät\n",
    "conf_mat = conf_mat.to(device)\n",
    "print(\"Confusion Matrix (nach .to(device)) befindet sich auf:\", conf_mat.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint classifier.4.weight shape: torch.Size([124, 512, 1, 1])\n",
      "Checkpoint classifier.4.bias shape: torch.Size([124])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ray.cloudpickle as pickle\n",
    "\n",
    "# Pfad aus der Fehlermeldung – passe diesen ggf. an deinen konkreten Fall an\n",
    "checkpoint_path = \"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/fcn_resnet101/train_hyper_c75247d6_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-02-22_22-23-45/checkpoint_000099/checkpoint.pkl\"\n",
    "\n",
    "if os.path.isfile(checkpoint_path):\n",
    "    with open(checkpoint_path, \"rb\") as fp:\n",
    "        checkpoint_data = pickle.load(fp)\n",
    "    \n",
    "    # Überprüfe die Form der Parameter im Klassifikationslayer\n",
    "    weight_shape = checkpoint_data[\"model_state\"].get(\"classifier.4.weight\", None)\n",
    "    bias_shape = checkpoint_data[\"model_state\"].get(\"classifier.4.bias\", None)\n",
    "    \n",
    "    if weight_shape is not None and bias_shape is not None:\n",
    "        print(\"Checkpoint classifier.4.weight shape:\", weight_shape.shape)\n",
    "        print(\"Checkpoint classifier.4.bias shape:\", bias_shape.shape)\n",
    "    else:\n",
    "        print(\"Die Schlüssel 'classifier.4.weight' oder 'classifier.4.bias' wurden im Checkpoint nicht gefunden.\")\n",
    "else:\n",
    "    print(\"Kein Checkpoint gefunden an:\", checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA GPU\n",
      "Model loaded: fcn_resnet101 | Device: cuda \n",
      "Error loading Model with Epoch latest: Error(s) in loading state_dict for FCN:\n",
      "\tsize mismatch for classifier.4.weight: copying a param with shape torch.Size([20, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([124, 512, 1, 1]).\n",
      "\tsize mismatch for classifier.4.bias: copying a param with shape torch.Size([20]) from checkpoint, the shape in current model is torch.Size([124]).\n",
      "Skipping local .pth load due to error above.\n",
      "Model classifier.4.weight shape: torch.Size([124, 512, 1, 1])\n",
      "Model classifier.4.bias shape: torch.Size([124])\n"
     ]
    }
   ],
   "source": [
    "# Schritt 3: Überprüfe die Form des Klassifikationslayers im aktuell instanzierten Modell\n",
    "model_instance = MapillaryTrainedModel(\n",
    "    model_name=model_name,\n",
    "    width=520,\n",
    "    height=520,\n",
    "    weights_name='',\n",
    "    folder_path=\"dummy_folder_for_testing\",  # Dummy-Pfad oder ein anderer Testpfad\n",
    "    start_epoch='latest'\n",
    ")\n",
    "\n",
    "state_dict_model = model_instance.model.state_dict()\n",
    "print(\"Model classifier.4.weight shape:\", state_dict_model[\"classifier.4.weight\"].shape)\n",
    "print(\"Model classifier.4.bias shape:\", state_dict_model[\"classifier.4.bias\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA GPU\n",
      "Model loaded: fcn_resnet101 | Device: cuda \n",
      "Error loading Model with Epoch latest: Error(s) in loading state_dict for FCN:\n",
      "\tsize mismatch for classifier.4.weight: copying a param with shape torch.Size([20, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([124, 512, 1, 1]).\n",
      "\tsize mismatch for classifier.4.bias: copying a param with shape torch.Size([20]) from checkpoint, the shape in current model is torch.Size([124]).\n",
      "Skipping local .pth load due to error above.\n",
      "DEBUG: Checkpoint classifier.4.weight shape: torch.Size([124, 512, 1, 1])\n",
      "DEBUG: Checkpoint classifier.4.bias shape: torch.Size([124])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ray.cloudpickle as pickle\n",
    "from Helper.ml_models import MapillaryTrainedModel\n",
    "\n",
    "def load_checkpointed_model_ray(model_name, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Debug-Version der Funktion: Lädt ein Modell und gibt die Shapes der classifier-Parameter aus.\n",
    "    \"\"\"\n",
    "    # Initialisiere das Modell (MapillaryTrainedModel erwartet 124 Klassen, wenn die Colormap so definiert ist)\n",
    "    loaded_model = MapillaryTrainedModel(model_name=model_name, width=520, height=520, weights_name='')\n",
    "    \n",
    "    # Lade den Checkpoint\n",
    "    with open(checkpoint_path, \"rb\") as fp:\n",
    "        checkpoint_data = pickle.load(fp)\n",
    "    \n",
    "    # Debug-Ausgabe: Shapes der Parameter des Klassifikationslayers aus dem Checkpoint\n",
    "    cp_weight = checkpoint_data[\"model_state\"].get(\"classifier.4.weight\")\n",
    "    cp_bias = checkpoint_data[\"model_state\"].get(\"classifier.4.bias\")\n",
    "    print(\"DEBUG: Checkpoint classifier.4.weight shape:\", cp_weight.shape)\n",
    "    print(\"DEBUG: Checkpoint classifier.4.bias shape:\", cp_bias.shape)\n",
    "    \n",
    "    # Versuch, den State Dict zu laden\n",
    "    try:\n",
    "        loaded_model.model.load_state_dict(checkpoint_data[\"model_state\"], strict=True)\n",
    "    except Exception as e:\n",
    "        print(\"Error loading checkpoint:\", e)\n",
    "    \n",
    "    return loaded_model\n",
    "\n",
    "# Bitte passe diesen Pfad an den tatsächlichen Checkpoint an, den du debuggen möchtest.\n",
    "checkpoint_path = \"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/fcn_resnet101/train_hyper_c75247d6_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-02-22_22-23-45/checkpoint_000099/checkpoint.pkl\"\n",
    "model_name = \"fcn_resnet101\"\n",
    "\n",
    "# Lade das Modell über die Debug-Funktion\n",
    "model_loaded = load_checkpointed_model_ray(model_name, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.0.weight: torch.Size([512, 2048, 3, 3])\n",
      "classifier.1.bias: torch.Size([512])\n",
      "classifier.1.num_batches_tracked: torch.Size([])\n",
      "classifier.1.running_mean: torch.Size([512])\n",
      "classifier.1.running_var: torch.Size([512])\n",
      "classifier.1.weight: torch.Size([512])\n",
      "classifier.4.bias: torch.Size([124])\n",
      "classifier.4.weight: torch.Size([124, 512, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ray.cloudpickle as pickle\n",
    "\n",
    "# Passe den Pfad ggf. an deinen Checkpoint an\n",
    "checkpoint_path = \"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/fcn_resnet101/train_hyper_c75247d6_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-02-22_22-23-45/checkpoint_000099/checkpoint.pkl\"\n",
    "\n",
    "with open(checkpoint_path, \"rb\") as fp:\n",
    "    checkpoint_data = pickle.load(fp)\n",
    "\n",
    "# Drucke alle Schlüssel, die \"classifier\" enthalten, zusammen mit ihrer Shape:\n",
    "for key, tensor in sorted(checkpoint_data[\"model_state\"].items()):\n",
    "    if \"classifier\" in key:\n",
    "        print(f\"{key}: {tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA GPU\n",
      "Model loaded: fcn_resnet101 | Device: cuda \n",
      "Error loading Model with Epoch latest: Error(s) in loading state_dict for FCN:\n",
      "\tsize mismatch for classifier.4.weight: copying a param with shape torch.Size([20, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([124, 512, 1, 1]).\n",
      "\tsize mismatch for classifier.4.bias: copying a param with shape torch.Size([20]) from checkpoint, the shape in current model is torch.Size([124]).\n",
      "Skipping local .pth load due to error above.\n",
      "Modell initialisiert mit skip_local_load=True\n",
      "Model classifier.4.weight shape: torch.Size([124, 512, 1, 1])\n",
      "Anzahl der Klassen (num_classes): 124\n",
      "DEBUG: Checkpoint classifier.4.weight shape: torch.Size([124, 512, 1, 1])\n",
      "DEBUG: Checkpoint classifier.4.bias shape: torch.Size([124])\n",
      "Checkpoint erfolgreich geladen.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ray.cloudpickle as pickle\n",
    "from Helper.ml_models import MapillaryTrainedModel\n",
    "\n",
    "# Verwende den Parameter skip_local_load=True, um das Laden eines lokalen .pth Checkpoints zu verhindern.\n",
    "model_name = \"fcn_resnet101\"\n",
    "dummy_folder = \"dummy_folder_for_testing\"\n",
    "os.makedirs(dummy_folder, exist_ok=True)\n",
    "\n",
    "# Initialisiere das Modell ohne lokalen Checkpoint-Ladevorgang\n",
    "model_instance = MapillaryTrainedModel(\n",
    "    model_name=model_name,\n",
    "    width=520,\n",
    "    height=520,\n",
    "    weights_name='',\n",
    "    folder_path=dummy_folder,\n",
    "    start_epoch='latest',\n",
    "    skip_local_load=True  # Wichtig: überspringt das Laden des lokalen .pth Checkpoints\n",
    ")\n",
    "\n",
    "print(\"Modell initialisiert mit skip_local_load=True\")\n",
    "print(\"Model classifier.4.weight shape:\", model_instance.model.state_dict()[\"classifier.4.weight\"].shape)\n",
    "print(\"Anzahl der Klassen (num_classes):\", model_instance.num_classes)\n",
    "\n",
    "# Jetzt laden wir den Ray Tune Checkpoint (das .pkl), der korrekt 124 Klassen hat.\n",
    "def load_checkpointed_model_ray(model_instance, checkpoint_path):\n",
    "    with open(checkpoint_path, \"rb\") as fp:\n",
    "        checkpoint_data = pickle.load(fp)\n",
    "    \n",
    "    # Debug-Ausgabe\n",
    "    cp_weight = checkpoint_data[\"model_state\"].get(\"classifier.4.weight\")\n",
    "    cp_bias = checkpoint_data[\"model_state\"].get(\"classifier.4.bias\")\n",
    "    print(\"DEBUG: Checkpoint classifier.4.weight shape:\", cp_weight.shape)\n",
    "    print(\"DEBUG: Checkpoint classifier.4.bias shape:\", cp_bias.shape)\n",
    "    \n",
    "    try:\n",
    "        model_instance.model.load_state_dict(checkpoint_data[\"model_state\"], strict=True)\n",
    "        print(\"Checkpoint erfolgreich geladen.\")\n",
    "    except Exception as e:\n",
    "        print(\"Fehler beim Laden des Checkpoints:\", e)\n",
    "    \n",
    "    return model_instance\n",
    "\n",
    "# Bitte passe den Pfad an deinen tatsächlichen Checkpoint an:\n",
    "checkpoint_path = \"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/fcn_resnet101/train_hyper_c75247d6_1_auto_cast=True,batch_size=4,learning_rate=0.0001,max_epochs=100,weight_decay=0.0000_2025-02-22_22-23-45/checkpoint_000099/checkpoint.pkl\"\n",
    "\n",
    "model_loaded = load_checkpointed_model_ray(model_instance, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
