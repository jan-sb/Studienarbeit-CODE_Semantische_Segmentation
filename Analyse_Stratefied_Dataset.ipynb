{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helper.Helper_functions import *\n",
    "from Helper.ml_models import *\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "# Erstelle den Speicherordner, falls er nicht existiert\n",
    "output_dir = \"FINAL_DATEN/Bilder_Studienarbeit\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kit_image_dir = 'KittiDaten/training/image_2'\n",
    "kit_annotation_dir = 'KittiDaten/training/semantic'\n",
    "\n",
    "cit_image_dir = 'CityscapesDaten/images'\n",
    "cit_annotation_dir = 'CityscapesDaten/semantic'\n",
    "\n",
    "\n",
    "df = analyse_dataset_GRAY(kit_annotation_dir)\n",
    "class_distribution_violin_plot(df, output='KittiDaten')\n",
    "stratified_kfold_and_violin_plot(df, output='KittiDaten',  k=5)\n",
    "\n",
    "\n",
    "df2 = analyse_dataset_GRAY(cit_annotation_dir)\n",
    "class_distribution_violin_plot(df2, output='CityscapesDaten')\n",
    "stratified_kfold_and_violin_plot(df2, output='CityscapesDaten',  k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_dataset_GRAY(path, classes = None):\n",
    "    if classes is None:\n",
    "        classes = {\n",
    "            7: 'road',\n",
    "            8: 'sidewalk',\n",
    "            11: 'building',\n",
    "            12: 'wall',\n",
    "            13: 'fence',\n",
    "            17: 'pole',\n",
    "            19: 'traffic light',\n",
    "            20: 'traffic sign',\n",
    "            21: 'vegetation',\n",
    "            22: 'terrain',\n",
    "            23: 'sky',\n",
    "            24: 'person',\n",
    "            25: 'rider',\n",
    "            26: 'car',\n",
    "            27: 'truck',\n",
    "            28: 'bus',\n",
    "            31: 'train',\n",
    "            32: 'motorcycle',\n",
    "            33: 'bicycle',\n",
    "            0: 'unlabeled'\n",
    "        }\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Initialize the counts\n",
    "    counts = {class_name: [] for class_name in classes.values()}      \n",
    "    absolute_counts = {class_name: [] for class_name in classes.values()} \n",
    "    image_names = []\n",
    "    \n",
    "    def count_classes(image_path):\n",
    "        # Initialize the counts for this image\n",
    "        image_counts = {class_name: 0 for class_name in classes.values()}\n",
    "        absolute_image_counts = {class_name: 0 for class_name in classes.values()} \n",
    "        # Open the image and convert it to grayscale\n",
    "        image = Image.open(image_path).convert('L')\n",
    "        # Calculate the total number of pixels for this image\n",
    "        total_pixels = np.prod(image.size)\n",
    "    # Convert the image to a PyTorch tensor and send it to the device\n",
    "        image_tensor = torch.from_numpy(np.array(image)).to(device)\n",
    "        # Iterate over each pixel in the image\n",
    "        for rgb in classes.keys():\n",
    "            # Create a mask for the current class\n",
    "            mask = (image_tensor == torch.tensor(rgb, device=device))\n",
    "            # Count the pixels in the mask and add them to the count for the current class\n",
    "            absolute_image_counts[classes[rgb]] += mask.sum().item()\n",
    "            image_counts[classes[rgb]] += mask.sum().item() / total_pixels\n",
    "        # Add the counts for this image to the overall counts\n",
    "        for class_name in classes.values():\n",
    "            counts[class_name].append(image_counts[class_name])\n",
    "            absolute_counts[class_name].append(absolute_image_counts[class_name])\n",
    "        # Store the image name\n",
    "        image_names.append(os.path.basename(image_path))\n",
    "\n",
    "    # Get the total number of images in the directory\n",
    "    total_images = len(os.listdir(path))\n",
    "    \n",
    "    # Iterate over all the image files in the directory\n",
    "    for i, image_file in tqdm(enumerate(os.listdir(path)), total=total_images, desc='Analyzing images'):\n",
    "        # Print the progress\n",
    "        #print(f'Analyzing image {i}/{total_images}')\n",
    "        # Count the classes in the image\n",
    "        count_classes(os.path.join(path, image_file))\n",
    "\n",
    "    # Create the DataFrame\n",
    "    data = []\n",
    "    for i, image_name in tqdm(enumerate(image_names), total=total_images, desc='Creating DataFrame'):\n",
    "        for class_name in classes.values():\n",
    "            data.append({\"Image\": image_name, \"Class\": class_name, \"Pixel Percentage\": counts[class_name][i], \"Pixel Count\": absolute_counts[class_name][i]})\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n",
    "def analyse_dataset_RGB(path):\n",
    "    classes = {\n",
    "        (128, 64, 128): 'road',\n",
    "        (244, 35, 232): 'sidewalk',\n",
    "        (70, 70, 70): 'building',\n",
    "        (102, 102, 156): 'wall',\n",
    "        (190, 153, 153): 'fence',\n",
    "        (153, 153, 153): 'pole',\n",
    "        (250, 170, 30): 'traffic light',\n",
    "        (220, 220, 0): 'traffic sign',\n",
    "        (107, 142, 35): 'vegetation',\n",
    "        (152, 251, 152): 'terrain',\n",
    "        (70, 130, 180): 'sky',\n",
    "        (220, 20, 60): 'person',\n",
    "        (255, 0, 0): 'rider',\n",
    "        (0, 0, 142): 'car',\n",
    "        (0, 0, 70): 'truck',\n",
    "        (0, 60, 100): 'bus',\n",
    "        (0, 80, 100): 'train',\n",
    "        (0, 0, 230): 'motorcycle',\n",
    "        (119, 11, 32): 'bicycle',\n",
    "        (0, 0, 0): 'unlabeled'\n",
    "    }\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Initialize the counts\n",
    "    counts = {class_name: [] for class_name in classes.values()}      \n",
    "    image_names = []\n",
    "\n",
    "    def count_classes(image_path):\n",
    "        # Initialize the counts for this image\n",
    "        image_counts = {class_name: 0 for class_name in classes.values()}\n",
    "        # Open the image and convert it to RGB\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        # Calculate the total number of pixels for this image\n",
    "        total_pixels = np.prod(image.size)\n",
    "        # Convert the image to a PyTorch tensor and send it to the device\n",
    "        image_tensor = torch.from_numpy(np.array(image)).to(device)\n",
    "        # Iterate over each pixel in the image\n",
    "        for rgb in classes.keys():\n",
    "            # Create a mask for the current class\n",
    "            mask = (image_tensor == torch.tensor(rgb, device=device)).all(dim=2)\n",
    "            # Count the pixels in the mask and add them to the count for the current class\n",
    "            image_counts[classes[rgb]] += mask.sum().item() / total_pixels\n",
    "        # Add the counts for this image to the overall counts\n",
    "        for class_name in classes.values():\n",
    "            counts[class_name].append(image_counts[class_name])\n",
    "        # Store the image name\n",
    "        image_names.append(os.path.basename(image_path))\n",
    "\n",
    "    # Get the total number of images in the directory\n",
    "    total_images = len(os.listdir(path))\n",
    "    \n",
    "    # Iterate over all the image files in the directory\n",
    "    for i, image_file in enumerate(os.listdir(path)):\n",
    "        # Print the progress\n",
    "        #print(f'Analyzing image {i}/{total_images}')\n",
    "        # Count the classes in the image\n",
    "        count_classes(os.path.join(path, image_file))\n",
    "\n",
    "    # Create the DataFrame\n",
    "    data = []\n",
    "    for i, image_name in enumerate(image_names):\n",
    "        for class_name in classes.values():\n",
    "            data.append({\"Image\": image_name, \"Class\": class_name, \"Pixel Count\": counts[class_name][i]})\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n",
    "def class_distribution_violin_plot(df):\n",
    "    # Min-Max normalisieren der \"Pixel Count\"-Werte für jede Klasse\n",
    "    df2 = df.copy()\n",
    "    df2['Pixelanteil pro Bild'] = df.groupby('Class')['Pixel Percentage'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "    # Erstellen eines horizontalen Violinplots\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.violinplot(x=\"Pixelanteil pro Bild\", y=\"Class\", data=df2, orient='h', cut=0, inner='quart', density_norm='count')\n",
    "    plt.title('Verteilung der min-max normalisierten Pixelanzahl pro Bild für jede Klasse')\n",
    "    plt.xlabel(\"Pixelanteil pro Bild\")\n",
    "    plt.ylabel(\"Klasse\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Speichern des Plots\n",
    "    save_path = os.path.join(output_dir, \"Verteilung_Pixelanzahl.png\")\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Plot gespeichert: {save_path}\")\n",
    "\n",
    "def stratified_kfold_and_violin_plot(df, k=5):\n",
    "    # Die Klasse für jedes Bild abrufen\n",
    "    image_classes = df.groupby('Image')['Class'].first().values\n",
    "\n",
    "    # Initialisieren des StratifiedKFold-Objekts\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    # Eine neue Spalte zur Original-DataFrame hinzufügen, um die Quelle anzugeben\n",
    "    df['Quelle'] = 'Original'\n",
    "\n",
    "    # Bilder in k Gruppen aufteilen\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(df['Image'].unique(), image_classes)):\n",
    "        # Bilder für diese Gruppe abrufen\n",
    "        group_images = df['Image'].unique()[test_index]\n",
    "        # Zeilen für diese Gruppe abrufen\n",
    "        group_rows = df[df['Image'].isin(group_images)].copy()  # Kopie erstellen, um eine SettingWithCopyWarning zu vermeiden\n",
    "        # Neue Spalte zur k-fold DataFrame hinzufügen, um die Quelle anzugeben\n",
    "        group_rows['Quelle'] = f'Fold {i}'\n",
    "\n",
    "        # Die Original-DataFrame und die k-fold DataFrame zusammenfügen\n",
    "        combined_df = pd.concat([df, group_rows])\n",
    "\n",
    "        # Violinplot für die kombinierten Daten erstellen\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.violinplot(x=\"Pixel Percentage\", y=\"Class\", hue=\"Quelle\", split=True, data=combined_df, orient='h', cut=0, inner='quart', density_norm='count')\n",
    "        plt.title(f'Vergleich der ursprünglichen Verteilung und der Verteilung für Fold {i}')\n",
    "        plt.xlabel(\"Pixelanteil pro Bild\")\n",
    "        plt.ylabel(\"Klasse\")\n",
    "        plt.legend(title=\"Quelle\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Speichern des Plots\n",
    "        save_path = os.path.join(output_dir, f\"Vergleich_Fold_{i}.png\")\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"Plot gespeichert: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Kitti Dataset und Cityscapes Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing images: 100%|██████████| 3475/3475 [00:32<00:00, 105.91it/s]\n",
      "Creating DataFrame: 100%|██████████| 3475/3475 [00:00<00:00, 162464.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot gespeichert: FINAL_DATEN/Bilder_Studienarbeit/Verteilung_Pixelanzahl.png\n",
      "Plot gespeichert: FINAL_DATEN/Bilder_Studienarbeit/Vergleich_Fold_0.png\n",
      "Plot gespeichert: FINAL_DATEN/Bilder_Studienarbeit/Vergleich_Fold_1.png\n",
      "Plot gespeichert: FINAL_DATEN/Bilder_Studienarbeit/Vergleich_Fold_2.png\n",
      "Plot gespeichert: FINAL_DATEN/Bilder_Studienarbeit/Vergleich_Fold_3.png\n",
      "Plot gespeichert: FINAL_DATEN/Bilder_Studienarbeit/Vergleich_Fold_4.png\n"
     ]
    }
   ],
   "source": [
    "# kit_image_dir = 'KittiDaten/training/image_2'\n",
    "# kit_annotation_dir = 'KittiDaten/training/semantic'\n",
    "\n",
    "city_classes = {\n",
    "    0: 'road',\n",
    "    1: 'sidewalk',\n",
    "    2: 'building',\n",
    "    3: 'wall',\n",
    "    4: 'fence',\n",
    "    5: 'pole',\n",
    "    6: 'traffic light',\n",
    "    7: 'traffic sign',\n",
    "    8: 'vegetation',\n",
    "    9: 'terrain',\n",
    "    10: 'sky',\n",
    "    11: 'person',\n",
    "    12: 'rider',\n",
    "    13: 'car',\n",
    "    14: 'truck',\n",
    "    15: 'bus',\n",
    "    16: 'train',\n",
    "    17: 'motorcycle',\n",
    "    18: 'bicycle',\n",
    "    19: 'unlabeled',\n",
    "}\n",
    "\n",
    "cit_image_dir = 'CityscapesDaten/images'\n",
    "cit_annotation_dir = 'CityscapesDaten/semantic'\n",
    "\n",
    "# df = analyse_dataset_GRAY(kit_annotation_dir)\n",
    "# class_distribution_violin_plot(df)\n",
    "# stratified_kfold_and_violin_plot(df, k=5)\n",
    "\n",
    "\n",
    "df2 = analyse_dataset_GRAY(cit_annotation_dir, classes=city_classes)\n",
    "class_distribution_violin_plot(df2)\n",
    "stratified_kfold_and_violin_plot(df2,  k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([3.6438016e+08]), array([20]))\n",
      "Class\n",
      "road              0.152993\n",
      "sidewalk          0.944824\n",
      "building          0.249295\n",
      "wall              8.490185\n",
      "fence             6.509589\n",
      "pole              4.478882\n",
      "traffic light    27.426945\n",
      "traffic sign      9.970298\n",
      "vegetation        0.350820\n",
      "terrain           5.090917\n",
      "sky               1.441691\n",
      "person            4.598446\n",
      "rider            38.604672\n",
      "car               0.816887\n",
      "truck            20.783080\n",
      "bus              22.012393\n",
      "train            26.222695\n",
      "motorcycle       58.974866\n",
      "bicycle          12.408356\n",
      "unlabeled         0.430046\n",
      "Name: Pixel Count, dtype: float64\n",
      "Sum of all weights249.95787985107148\n"
     ]
    }
   ],
   "source": [
    "def calculate_class_weights(df):\n",
    "    # Calculate the total pixel count for each class\n",
    "    total_pixel_counts = df.groupby('Class')['Pixel Count'].sum()\n",
    "    \n",
    "    # Reindex the total_pixel_counts series to match the order of city_classes\n",
    "    total_pixel_counts = total_pixel_counts.reindex(city_classes.values())\n",
    "\n",
    "\n",
    "    # Calculate the total pixel count\n",
    "    total_pixels = total_pixel_counts.sum()\n",
    "\n",
    "    # Calculate the class weights\n",
    "    class_weights = total_pixels / (20 * (total_pixel_counts))\n",
    "    \n",
    "    # Convert class weights to a PyTorch tensor\n",
    "    class_weights_tensor = torch.from_numpy(class_weights.values.astype(np.float32))\n",
    "    \n",
    "    # Save the tensor to a file\n",
    "    torch.save(class_weights_tensor, 'CityscapesDaten/class_weights.pt')\n",
    "\n",
    "    return class_weights, total_pixel_counts\n",
    "\n",
    "\n",
    "class_weights, total = calculate_class_weights(df2)\n",
    "sorted_class_weights = class_weights.sort_values(ascending=True)\n",
    "sorted_total = total.sort_values(ascending=False)\n",
    "\n",
    "weights_equal_test = class_weights.values *  total.values\n",
    "arr = np.array(weights_equal_test)\n",
    "print(np.unique(arr, return_counts=True))\n",
    "\n",
    "print(class_weights)\n",
    "total_weight_sum = 0\n",
    "for weight in class_weights:\n",
    "    total_weight_sum += weight\n",
    "print(f'Sum of all weights {total_weight_sum}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1530,  0.9448,  0.2493,  8.4902,  6.5096,  4.4789, 27.4269,  9.9703,\n",
      "         0.3508,  5.0909,  1.4417,  4.5984, 38.6047,  0.8169, 20.7831, 22.0124,\n",
      "        26.2227, 58.9749, 12.4084,  0.4300])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class_weights_tensor = torch.load('CityscapesDaten/class_weights.pt')\n",
    "\n",
    "# Print out the values\n",
    "print(class_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19] / \n",
      " Class\n",
      "0       29365708\n",
      "1     1461641548\n",
      "2       16553410\n",
      "3      446059503\n",
      "4       55975907\n",
      "5        6178567\n",
      "6       79239848\n",
      "7       81355164\n",
      "8        9438758\n",
      "9     2381680967\n",
      "10     385659445\n",
      "11     252744993\n",
      "12      71574562\n",
      "13      13285481\n",
      "14      36546566\n",
      "15      13895603\n",
      "16      17532539\n",
      "17     847304822\n",
      "18    1038651996\n",
      "19      42917813\n",
      "Name: Pixel Count, dtype: int64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "classes should include all valid labels that can be in y",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 52\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Use the function\u001b[39;00m\n\u001b[1;32m     30\u001b[0m city_classes \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroad\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msidewalk\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;241m19\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munlabeled\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     51\u001b[0m }\n\u001b[0;32m---> 52\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_class_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcity_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Load the tensor from the file\u001b[39;00m\n\u001b[1;32m     55\u001b[0m class_weights_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCityscapesDaten/class_weights.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 10\u001b[0m, in \u001b[0;36mcalculate_class_weights\u001b[0;34m(df, city_classes)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_class_weights\u001b[39m(df, city_classes):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# # Convert class labels to integers\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# le = LabelEncoder()\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# df['Class'] = le.fit_transform(df['Class'])\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Compute the class weights\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39munique(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPixel Count\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     class_weights \u001b[38;5;241m=\u001b[39m \u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_class_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbalanced\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mClass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mClass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPixel Count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Create a dictionary mapping class labels to their weights\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     class_weights_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mcls\u001b[39m: weight \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m'\u001b[39m]), class_weights)}\n",
      "File \u001b[0;32m~/anaconda3/envs/studi/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/studi/lib/python3.9/site-packages/sklearn/utils/class_weight.py:65\u001b[0m, in \u001b[0;36mcompute_class_weight\u001b[0;34m(class_weight, classes, y)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(y) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(classes):\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses should include all valid labels that can be in y\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_weight) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# uniform class weights\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(classes\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: classes should include all valid labels that can be in y"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def calculate_class_weights(df, city_classes):\n",
    "    # # Convert class labels to integers\n",
    "    # le = LabelEncoder()\n",
    "    # df['Class'] = le.fit_transform(df['Class'])\n",
    "\n",
    "    # Compute the class weights\n",
    "    print(f'{np.unique(df[\"Class\"])} / \\n {df.groupby(\"Class\")[\"Pixel Count\"].sum()}')\n",
    "    class_weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(df['Class']), y=np.array(df2.groupby('Class')['Pixel Count'].sum()))\n",
    "\n",
    "    # Create a dictionary mapping class labels to their weights\n",
    "    class_weights_dict = {cls: weight for cls, weight in zip(np.unique(df['Class']), class_weights)}\n",
    "\n",
    "    print(\"Unique classes in DataFrame: \", np.unique(df['Class']))\n",
    "    print(\"Class weights dictionary: \", class_weights_dict)\n",
    "\n",
    "    # Reorder the class weights to match the order of city_classes\n",
    "    class_weights_ordered = [class_weights_dict.get(cls, 1) for cls in city_classes.keys()]\n",
    "\n",
    "    # Convert class weights to a PyTorch tensor\n",
    "    class_weights_tensor = torch.tensor(class_weights_ordered, dtype=torch.float32)\n",
    "\n",
    "    # Save the tensor to a file\n",
    "    torch.save(class_weights_tensor, 'CityscapesDaten/class_weights.pt')\n",
    "\n",
    "    return class_weights_ordered\n",
    "\n",
    "# Use the function\n",
    "city_classes = {\n",
    "    0: 'road',\n",
    "    1: 'sidewalk',\n",
    "    2: 'building',\n",
    "    3: 'wall',\n",
    "    4: 'fence',\n",
    "    5: 'pole',\n",
    "    6: 'traffic light',\n",
    "    7: 'traffic sign',\n",
    "    8: 'vegetation',\n",
    "    9: 'terrain',\n",
    "    10: 'sky',\n",
    "    11: 'person',\n",
    "    12: 'rider',\n",
    "    13: 'car',\n",
    "    14: 'truck',\n",
    "    15: 'bus',\n",
    "    16: 'train',\n",
    "    17: 'motorcycle',\n",
    "    18: 'bicycle',\n",
    "    19: 'unlabeled',\n",
    "}\n",
    "class_weights = calculate_class_weights(df2, city_classes)\n",
    "\n",
    "# Load the tensor from the file\n",
    "class_weights_tensor = torch.load('CityscapesDaten/class_weights.pt')\n",
    "\n",
    "# Print out the values\n",
    "print(class_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  29365708 1461641548   16553410  446059503   55975907    6178567\n",
      "   79239848   81355164    9438758 2381680967  385659445  252744993\n",
      "   71574562   13285481   36546566   13895603   17532539  847304822\n",
      " 1038651996   42917813] \n",
      " shape: (20,)\n"
     ]
    }
   ],
   "source": [
    "pixel_count_dict = np.array(df2.groupby('Class')['Pixel Count'].sum())\n",
    "\n",
    "print(f'{pixel_count_dict} \\n shape: {pixel_count_dict.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(df2['Class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
