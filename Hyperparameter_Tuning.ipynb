{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 15:12:19.675049: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-10 15:12:20.230358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import tempfile\n",
    "import torch\n",
    "from ray import tune, train\n",
    "from ray.train import Checkpoint, get_checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune import CLIReporter\n",
    "from Helper.ml_models import * \n",
    "import json\n",
    "from datetime import datetime\n",
    "import ray.cloudpickle as pickle\n",
    "\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directory(model):\n",
    "    dir_name = f'Hyperparameter/{model}'\n",
    "    os.makedirs(dir_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data leaks found.\n"
     ]
    }
   ],
   "source": [
    "all_models = ['deeplabv3_resnet50', 'deeplabv3_resnet101', 'deeplabv3_mobilenet_v3_large', 'lraspp_mobilenet_v3_large']\n",
    "not_yet_studied = ['fcn_resnet50', 'fcn_resnet101']\n",
    "\n",
    "k_fold_dataset = K_Fold_Dataset('/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/CityscapesDaten/images',\n",
    "                         '/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/CityscapesDaten/semantic',\n",
    "                         k_fold_csv_dir='/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Daten/CityscapesDaten',\n",
    "                         leave_out_fold=0,\n",
    "                         )\n",
    "\n",
    "k_fold_dataset.check_for_data_leaks()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 15:06:58,884\tINFO worker.py:1749 -- Started a local Ray instance.\n",
      "2025-01-10 15:06:59,361\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
      "/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/tune/tune.py:583: UserWarning: The `local_dir` argument is deprecated and will be removed. This will pass-through to set the `storage_path` for now but will raise an error in the future. You should only set the `storage_path` from now on.\n",
      "  warnings.warn(\n",
      "2025-01-10 15:06:59,365\tINFO tune.py:614 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "[I 2025-01-10 15:06:59,375] A new study created in memory with name: optuna\n",
      "2025-01-10 15:06:59,394\tERROR tune_controller.py:235 -- Failed to restore the run state.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/tune/execution/tune_controller.py\", line 230, in __init__\n",
      "    self.resume(resume_config=resume_config)\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/tune/execution/tune_controller.py\", line 437, in resume\n",
      "    raise ValueError(\n",
      "ValueError: Tried to resume experiment from directory '/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet50_20250110-150657/train_hyper_2025-01-10_15-06-59', but no experiment state file of the form 'experiment_state-{}.json' was found. This is expected if you are launching a new experiment.\n",
      "2025-01-10 15:06:59,397\tINFO tune_controller.py:238 -- Restarting experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-01-10 15:06:59 (running for 00:00:00.11)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 1/100 (1 PENDING)\n",
      "+----------------------+----------+-------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc   | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+-------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | PENDING  |       | True        |            2 |     5.56418e-09 |     0.00101292 |\n",
      "+----------------------+----------+-------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=24740)\u001b[0m 2025-01-10 15:07:02.011580: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=24740)\u001b[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=24740)\u001b[0m 2025-01-10 15:07:02.627448: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_hyper pid=24740)\u001b[0m Using CUDA GPU\n",
      "\u001b[36m(train_hyper pid=24740)\u001b[0m Model loaded: deeplabv3_resnet50 | Device: cuda \n",
      "\u001b[36m(train_hyper pid=24740)\u001b[0m Latest Epoch Save doesnt exist or Epoch Number Save doesnt exist, initialising new Save\n",
      "\u001b[36m(train_hyper pid=24740)\u001b[0m Saved Model\n",
      "\u001b[36m(train_hyper pid=24740)\u001b[0m Successfully loaded Model\n",
      "\u001b[36m(train_hyper pid=24740)\u001b[0m Training Dataset prepared\n",
      "\u001b[36m(train_hyper pid=24740)\u001b[0m Validation Dataset prepared\n",
      "\u001b[36m(train_hyper pid=24740)\u001b[0m Test Dataset prepared\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:07:04 (running for 00:00:05.18)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:07:09 (running for 00:00:10.21)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:07:14 (running for 00:00:15.24)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:07:19 (running for 00:00:20.27)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:07:24 (running for 00:00:25.30)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:07:29 (running for 00:00:30.33)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:07:34 (running for 00:00:35.36)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:07:39 (running for 00:00:40.39)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:07:44 (running for 00:00:45.41)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:07:49 (running for 00:00:50.45)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:07:54 (running for 00:00:55.47)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:07:59 (running for 00:01:00.50)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:08:04 (running for 00:01:05.53)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:08:09 (running for 00:01:10.56)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:08:14 (running for 00:01:15.59)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:08:20 (running for 00:01:20.62)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:08:25 (running for 00:01:25.64)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:08:30 (running for 00:01:30.67)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:08:35 (running for 00:01:35.70)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:08:40 (running for 00:01:40.72)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:08:45 (running for 00:01:45.75)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:08:50 (running for 00:01:50.78)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:08:55 (running for 00:01:55.80)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:09:00 (running for 00:02:00.83)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "\u001b[36m(train_hyper pid=24740)\u001b[0m Epoch 2 |   Loss: nan    |   Accuracy: 6.378016780362692%\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:09:05 (running for 00:02:05.86)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:09:10 (running for 00:02:10.90)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:09:15 (running for 00:02:15.92)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_6b05bbc8 | RUNNING  | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n",
      "\u001b[36m(train_hyper pid=24740)\u001b[0m Validation Loss: nan\n",
      "\u001b[36m(train_hyper pid=24740)\u001b[0m Current learning rate: 5.285971214159819e-09\n",
      "\u001b[36m(train_hyper pid=24740)\u001b[0m Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 15:09:20,050\tERROR tune_controller.py:1331 -- Trial task failed for trial train_hyper_6b05bbc8\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/_private/worker.py\", line 861, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=24740, ip=10.7.0.4, actor_id=ec9a3fc2644dc6c5f9b17a5001000000, repr=train_hyper)\n",
      "AttributeError: type object 'Checkpoint' has no attribute 'from_dict'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=24740, ip=10.7.0.4, actor_id=ec9a3fc2644dc6c5f9b17a5001000000, repr=train_hyper)\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 98, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 248, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_23985/2024082435.py\", line 47, in train_hyper\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/train/_checkpoint.py\", line 42, in __getattr__\n",
      "    raise _get_migration_error(item) from exc\n",
      "AttributeError: The new `ray.train.Checkpoint` class does not support `from_dict()`. Instead, only directories are supported.\n",
      "\n",
      "Example to store a dictionary in a checkpoint:\n",
      "\n",
      "import os, tempfile\n",
      "import ray.cloudpickle as pickle\n",
      "from ray import train\n",
      "from ray.train import Checkpoint\n",
      "\n",
      "with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
      "  with open(os.path.join(checkpoint_dir, 'data.pkl'), 'wb') as fp:\n",
      "    pickle.dump({'data': 'value'}, fp)\n",
      "\n",
      "  checkpoint = Checkpoint.from_directory(checkpoint_dir)\n",
      "  train.report(..., checkpoint=checkpoint)\n",
      "\n",
      "Example to load a dictionary from a checkpoint:\n",
      "\n",
      "if train.get_checkpoint():\n",
      "  with train.get_checkpoint().as_directory() as checkpoint_dir:\n",
      "    with open(os.path.join(checkpoint_dir, 'data.pkl'), 'rb') as fp:\n",
      "      data = pickle.load(fp)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_hyper_6b05bbc8</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-01-10 15:09:20 (running for 00:02:20.95)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 0/12 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 ERROR, 1 PENDING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "| train_hyper_6b05bbc8 | ERROR    | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "Number of errored trials: 1\n",
      "+----------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name           |   # failures | error file                                                                                                                                                                                                                                                 |\n",
      "|----------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_hyper_6b05bbc8 |            1 | /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts/train_hyper_6b05bbc8_1_auto_cast=True,batch_size=2,learning_rate=0.0000,weight_decay=0.0010_2025-01-10_15-06-59/error.txt |\n",
      "+----------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:09:25 (running for 00:02:25.97)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 2/100 (1 ERROR, 1 PENDING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_9d18aff9 | PENDING  |                | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "| train_hyper_6b05bbc8 | ERROR    | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "Number of errored trials: 1\n",
      "+----------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name           |   # failures | error file                                                                                                                                                                                                                                                 |\n",
      "|----------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_hyper_6b05bbc8 |            1 | /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts/train_hyper_6b05bbc8_1_auto_cast=True,batch_size=2,learning_rate=0.0000,weight_decay=0.0010_2025-01-10_15-06-59/error.txt |\n",
      "+----------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=25106)\u001b[0m 2025-01-10 15:09:28.197737: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=25106)\u001b[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=25106)\u001b[0m 2025-01-10 15:09:28.889419: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_hyper pid=25106)\u001b[0m Using CUDA GPU\n",
      "\u001b[36m(train_hyper pid=25106)\u001b[0m Model loaded: deeplabv3_resnet50 | Device: cuda \n",
      "\u001b[36m(train_hyper pid=25106)\u001b[0m Latest Epoch Save doesnt exist or Epoch Number Save doesnt exist, initialising new Save\n",
      "== Status ==\n",
      "Current time: 2025-01-10 15:09:30 (running for 00:02:31.06)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 3/100 (1 ERROR, 1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_9d18aff9 | RUNNING  | 10.7.0.4:25106 | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "| train_hyper_e4dd098f | PENDING  |                | True        |            6 |     8.34539e-10 |    0.00109075  |\n",
      "| train_hyper_6b05bbc8 | ERROR    | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "Number of errored trials: 1\n",
      "+----------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name           |   # failures | error file                                                                                                                                                                                                                                                 |\n",
      "|----------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_hyper_6b05bbc8 |            1 | /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts/train_hyper_6b05bbc8_1_auto_cast=True,batch_size=2,learning_rate=0.0000,weight_decay=0.0010_2025-01-10_15-06-59/error.txt |\n",
      "+----------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\u001b[36m(train_hyper pid=25106)\u001b[0m Saved Model\n",
      "\u001b[36m(train_hyper pid=25106)\u001b[0m Successfully loaded Model\n",
      "\u001b[36m(train_hyper pid=25106)\u001b[0m Training Dataset prepared\n",
      "\u001b[36m(train_hyper pid=25106)\u001b[0m Validation Dataset prepared\n",
      "\u001b[36m(train_hyper pid=25106)\u001b[0m Test Dataset prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 15:09:33,877\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2025-01-10 15:09:33,880\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2025-01-10 15:09:33,881\tINFO tune.py:1007 -- Wrote the latest version of all result files and experiment state to '/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet50_20250110-150657/train_hyper_2025-01-10_15-06-59' in 0.0037s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-01-10 15:09:33 (running for 00:02:34.48)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 15.000: None | Iter 5.000: None\n",
      "Logical resource usage: 6.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts\n",
      "Number of trials: 3/100 (1 ERROR, 1 PENDING, 1 RUNNING)\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "| Trial name           | status   | loc            | auto_cast   |   batch_size |   learning_rate |   weight_decay |\n",
      "|----------------------+----------+----------------+-------------+--------------+-----------------+----------------|\n",
      "| train_hyper_9d18aff9 | RUNNING  | 10.7.0.4:25106 | True        |            2 |     0.00500148  |    2.85855e-05 |\n",
      "| train_hyper_e4dd098f | PENDING  |                | True        |            6 |     8.34539e-10 |    0.00109075  |\n",
      "| train_hyper_6b05bbc8 | ERROR    | 10.7.0.4:24740 | True        |            2 |     5.56418e-09 |    0.00101292  |\n",
      "+----------------------+----------+----------------+-------------+--------------+-----------------+----------------+\n",
      "Number of errored trials: 1\n",
      "+----------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name           |   # failures | error file                                                                                                                                                                                                                                                 |\n",
      "|----------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_hyper_6b05bbc8 |            1 | /tmp/ray/session_2025-01-10_15-06-57_086153_23985/artifacts/2025-01-10_15-06-59/train_hyper_2025-01-10_15-06-59/driver_artifacts/train_hyper_6b05bbc8_1_auto_cast=True,batch_size=2,learning_rate=0.0000,weight_decay=0.0010_2025-01-10_15-06-59/error.txt |\n",
      "+----------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 15:09:43,889\tERROR tune.py:1035 -- Trials did not complete: [train_hyper_6b05bbc8]\n",
      "2025-01-10 15:09:43,890\tINFO tune.py:1039 -- Total run time: 164.52 seconds (154.48 seconds for the tuning loop).\n",
      "2025-01-10 15:09:43,890\tWARNING tune.py:1054 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n",
      "2025-01-10 15:09:43,895\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n",
      "- train_hyper_e4dd098f: FileNotFoundError('Could not fetch metrics for train_hyper_e4dd098f: both result.json and progress.csv were not found at /home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/deeplabv3_resnet50_20250110-150657/train_hyper_2025-01-10_15-06-59/train_hyper_e4dd098f_3_auto_cast=True,batch_size=6,learning_rate=0.0000,weight_decay=0.0011_2025-01-10_15-09-29')\n",
      "2025-01-10 15:09:43,896\tWARNING experiment_analysis.py:558 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "To fetch the `best_config`, pass a `metric` and `mode` parameter to `tune.run()`. Alternatively, use the `get_best_config(metric, mode)` method to set the metric and mode explicitly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 95\u001b[0m\n\u001b[1;32m     67\u001b[0m analysis \u001b[38;5;241m=\u001b[39m tune\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m     68\u001b[0m     train_hyper,\n\u001b[1;32m     69\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m     resume\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     91\u001b[0m )\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters found were: \u001b[39m\u001b[38;5;124m\"\u001b[39m, analysis\u001b[38;5;241m.\u001b[39mget_best_config(metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 95\u001b[0m best_config \u001b[38;5;241m=\u001b[39m \u001b[43manalysis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_config\u001b[49m(metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Save the best configuration to a JSON file\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyper_best_config.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n",
      "File \u001b[0;32m~/anaconda3/envs/studi/lib/python3.9/site-packages/ray/tune/analysis/experiment_analysis.py:240\u001b[0m, in \u001b[0;36mExperimentAnalysis.best_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the config of the best trial of the experiment\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03mThe best trial is determined by comparing the last trial results\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m`get_best_config(metric, mode, scope)` instead.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metric \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_mode:\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo fetch the `best_config`, pass a `metric` and `mode` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter to `tune.run()`. Alternatively, use the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`get_best_config(metric, mode)` method to set the metric \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand mode explicitly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    245\u001b[0m     )\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_best_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metric, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_mode)\n",
      "\u001b[0;31mValueError\u001b[0m: To fetch the `best_config`, pass a `metric` and `mode` parameter to `tune.run()`. Alternatively, use the `get_best_config(metric, mode)` method to set the metric and mode explicitly."
     ]
    }
   ],
   "source": [
    "model = all_models[0]\n",
    "\n",
    "def train_hyper(config, checkpoint_dir=None):  \n",
    "    try:\n",
    "        make_directory(model)\n",
    "        hyper_model = TrainedModel(model, 2048, 1024, weights_name='', folder_path=f'Hyperparameter/{model}', start_epoch='latest')\n",
    "        \n",
    "        # Checkpoint laden, falls vorhanden\n",
    "        if checkpoint_dir:\n",
    "            with open(os.path.join(checkpoint_dir, 'checkpoint.pkl'), 'rb') as fp:\n",
    "                checkpoint = pickle.load(fp)\n",
    "                hyper_model.model.load_state_dict(checkpoint[\"model_state\"])\n",
    "                hyper_model.optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "                start_epoch = checkpoint[\"epoch\"]\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "        \n",
    "        \n",
    "        hyper_model.prepare_model_training(\n",
    "            dataset_train=k_fold_dataset.train_dataset,\n",
    "            dataset_val=k_fold_dataset.val_dataset,\n",
    "            dataset_test=k_fold_dataset.test_dataset,\n",
    "            batch_size=int(config['batch_size']), \n",
    "            val_batch_size=int(config['batch_size']),\n",
    "            shuffle=True, \n",
    "            learning_rate=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay'], \n",
    "            num_workers=4, \n",
    "            pin_memory=True,\n",
    "            ray_tune=True,\n",
    "            )\n",
    "\n",
    "        EPOCHS = 20 \n",
    "        \n",
    "        for epoch in range(start_epoch, EPOCHS):\n",
    "            epoch_loss, epoch_acc, val_loss, _ = hyper_model.train(use_autocast=config['auto_cast']) \n",
    "            #miou = hyper_model.calculate_miou(k_fold_dataset.val_dataset)\n",
    "            # with tune.checkpoint_dir(epoch) as cp_dir:\n",
    "            #     hyper_model.save_model(file_management=False, save_path=cp_dir)\n",
    "            # tune.report(loss=epoch_loss, val_loss= val_loss , acc=epoch_acc)\n",
    "            \n",
    "            with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "                checkpoint_data = {\n",
    "                    \"model_state\": hyper_model.model.state_dict(),\n",
    "                    \"optimizer_state\": hyper_model.optimizer.state_dict(),\n",
    "                    \"epoch\": epoch,\n",
    "                }\n",
    "                with open(os.path.join(checkpoint_dir, 'checkpoint.pkl'), 'wb') as fp:\n",
    "                    pickle.dump(checkpoint_data, fp)\n",
    "\n",
    "                checkpoint = Checkpoint.from_directory(checkpoint_dir)\n",
    "                train.report(\n",
    "                    {\"loss\": epoch_loss, \"val_loss\": val_loss, \"acc\": epoch_acc},\n",
    "                    checkpoint=checkpoint\n",
    "                )\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            tune.report(loss=float('inf'), val_loss= float('inf') , acc=0.0)\n",
    "        else:\n",
    "            raise e  \n",
    "        \n",
    "config = {\n",
    "    \"learning_rate\": tune.loguniform(1e-12, 1e-2),\n",
    "    'batch_size': tune.choice([2,4,6,8,12,14,16]),\n",
    "    \"weight_decay\": tune.loguniform(1e-6, 1e-1), \n",
    "    \"auto_cast\": tune.choice([True, False]),\n",
    "}\n",
    "\n",
    "analysis = tune.run(\n",
    "    train_hyper,\n",
    "    config=config,\n",
    "    resources_per_trial={\"cpu\": 6, \"gpu\": 1},\n",
    "    scheduler=ASHAScheduler(\n",
    "        metric=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=20,\n",
    "        grace_period=5,\n",
    "        reduction_factor=3,\n",
    "    ),\n",
    "    progress_reporter=CLIReporter(metric_columns=[\"loss\", \"val_loss\", \"acc\", \"training_iteration\"]),\n",
    "    local_dir=f\"/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/HyperparameterLOG/{model}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    search_alg=OptunaSearch(\n",
    "        metric=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        sampler=TPESampler(seed=42),\n",
    "    ),\n",
    "    num_samples=100,\n",
    "    #checkpoint_config=train.CheckpointConfig(\n",
    "        #checkpoint_frequency=5,\n",
    "        #checkpoint_at_end=True,\n",
    "    #),\n",
    "    resume=True,\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters found were: \", analysis.get_best_config(metric=\"val_loss\", mode=\"min\"))\n",
    "\n",
    "best_config = analysis.best_config(metric=\"val_loss\", mode=\"min\")\n",
    "\n",
    "# Save the best configuration to a JSON file\n",
    "with open('hyper_best_config.json', 'w') as json_file:\n",
    "    json.dump(best_config, json_file)\n",
    "\n",
    "print(\"Best configuration saved to best_config.json.\")\n",
    "\n",
    "# Speichere alle getesteten Konfigurationen und Ergebnisse\n",
    "all_trials = analysis.trials\n",
    "with open('hyper_all_trials.json', 'w') as json_file:\n",
    "    json.dump([trial.config for trial in all_trials], json_file)\n",
    "\n",
    "print(\"All configurations saved to all_trials.json.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
