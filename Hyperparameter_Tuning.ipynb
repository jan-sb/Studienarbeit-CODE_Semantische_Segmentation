{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ray import tune\n",
    "from ray import train\n",
    "from ray.train import Checkpoint, get_checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import ray.cloudpickle as pickle\n",
    "from ray.tune import CLIReporter\n",
    "from Helper.ml_models import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(image_dir='CityscapesDaten/images', annotation_dir='CityscapesDaten/semantic'):\n",
    "    trainset = CustomDataSet(image_dir=image_dir, annotation_dir=annotation_dir)\n",
    "\n",
    "    # If you have a separate set of images and annotations for testing, you can create a testset in a similar way:\n",
    "    # testset = CustomDataSet(image_dir=test_image_dir, annotation_dir=test_annotation_dir)\n",
    "\n",
    "    # If you don't have a separate test set, you can split the trainset into a training set and a test set:\n",
    "    train_size = int(0.8 * len(trainset))\n",
    "    test_size = len(trainset) - train_size\n",
    "    trainset, testset = torch.utils.data.random_split(trainset, [train_size, test_size])\n",
    "\n",
    "    return trainset, testset\n",
    "\n",
    "def make_directory(model):\n",
    "    dir_name = f'Hyperparameter/{model}'\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data leaks found.\n"
     ]
    }
   ],
   "source": [
    "all_models = ['deeplabv3_resnet50', 'deeplabv3_resnet101', 'deeplabv3_mobilenet_v3_large', 'fcn_resnet50', 'fcn_resnet101', 'lraspp_mobilenet_v3_large']\n",
    "test_epochs = 20\n",
    "\n",
    "k_fold_dataset = K_Fold_Dataset('CityscapesDaten/images',\n",
    "                         'CityscapesDaten/semantic',\n",
    "                         k_fold_csv_dir='Daten/CityscapesDaten',\n",
    "                         leave_out_fold=0,\n",
    "                         )\n",
    "\n",
    "k_fold_dataset.check_for_data_leaks()               \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA GPU\n",
      "Model loaded: deeplabv3_resnet50 | Device: cuda \n",
      "Latest Epoch Save doesnt exist or Epoch Number Save doesnt exist, initialising new Save\n",
      "Saved Model\n",
      "Successfully loaded Model\n",
      "Training Dataset prepared\n",
      "Validation Dataset prepared\n",
      "Epoch 1 von 1    |   Loss: 0.9581758890833173\n",
      "Saved Model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (20).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 21\u001b[0m\n\u001b[1;32m     11\u001b[0m hyper_model\u001b[38;5;241m.\u001b[39mprepare_model_training(dataset_train\u001b[38;5;241m=\u001b[39mk_fold_dataset\u001b[38;5;241m.\u001b[39mtrain_dataset,\n\u001b[1;32m     12\u001b[0m                                             dataset_val\u001b[38;5;241m=\u001b[39mk_fold_dataset\u001b[38;5;241m.\u001b[39mval_dataset,\n\u001b[1;32m     13\u001b[0m                                             batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]), \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m                                             momentum\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     17\u001b[0m                                             weight_decay\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],)\n\u001b[1;32m     20\u001b[0m hyper_model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mhyper_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk_fold_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Validate on the validation dataset\u001b[39;00m\n\u001b[1;32m     22\u001b[0m miou \u001b[38;5;241m=\u001b[39m hyper_model\u001b[38;5;241m.\u001b[39mcalculate_miou_miou(k_fold_dataset\u001b[38;5;241m.\u001b[39mval_dataset)\n\u001b[1;32m     23\u001b[0m tune\u001b[38;5;241m.\u001b[39mreport(loss\u001b[38;5;241m=\u001b[39mval_loss, miou\u001b[38;5;241m=\u001b[39mmiou)\n",
      "File \u001b[0;32m~/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Helper/ml_models.py:357\u001b[0m, in \u001b[0;36mTrainedModel.validate\u001b[0;34m(self, val_loader)\u001b[0m\n\u001b[1;32m    355\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(images)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    356\u001b[0m     _, labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 357\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    359\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_loader)\n",
      "File \u001b[0;32m~/anaconda3/envs/studi/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/studi/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/studi/lib/python3.9/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/studi/lib/python3.9/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (20)."
     ]
    }
   ],
   "source": [
    "model = all_models[0]\n",
    "make_directory(model)\n",
    "config = {  \n",
    "        'batch_size': 6,\n",
    "        'lr' : 0.001,\n",
    "        'momentum' : 0.9,\n",
    "        'weight_decay' : 0.0005,    \n",
    "}\n",
    "\n",
    "hyper_model = TrainedModel(model, 2048, 1024, weights_name='', folder_path=f'Hyperparameter/{model}', start_epoch='latest')\n",
    "hyper_model.prepare_model_training(dataset_train=k_fold_dataset.train_dataset,\n",
    "                                            dataset_val=k_fold_dataset.val_dataset,\n",
    "                                            batch_size=int(config['batch_size']), \n",
    "                                            shuffle=True, \n",
    "                                            learning_rate=config['lr'], \n",
    "                                            momentum=config['momentum'],\n",
    "                                            weight_decay=config['weight_decay'],)\n",
    "\n",
    "\n",
    "hyper_model.train(1)  # Train for one epoch\n",
    "val_loss = hyper_model.validate(k_fold_dataset.val_dataset)  # Validate on the validation dataset\n",
    "miou = hyper_model.calculate_miou_miou(k_fold_dataset.val_dataset)\n",
    "tune.report(loss=val_loss, miou=miou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 13:21:46,817\tINFO worker.py:1749 -- Started a local Ray instance.\n",
      "2024-05-04 13:21:47,309\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
      "2024-05-04 13:21:47,310\tINFO tune.py:614 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-04 13:21:47 (running for 00:00:00.11)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 6.000: None | Iter 3.000: None\n",
      "Logical resource usage: 0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2024-05-04_13-21-45_240245_12924/artifacts/2024-05-04_13-21-47/train_hyper_2024-05-04_13-21-47/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "+-------------------------+----------+-------+--------------+-----------------+------------+----------------+\n",
      "| Trial name              | status   | loc   |   batch_size |   learning_rate |   momentum |   weight_decay |\n",
      "|-------------------------+----------+-------+--------------+-----------------+------------+----------------|\n",
      "| train_hyper_7eaed_00000 | PENDING  |       |            2 |     1.05195e-06 |   0.192267 |    1.73692e-05 |\n",
      "+-------------------------+----------+-------+--------------+-----------------+------------+----------------+\n",
      "\n",
      "\n",
      "\u001b[36m(train_hyper pid=13654)\u001b[0m Using CUDA GPU\n",
      "\u001b[36m(train_hyper pid=13654)\u001b[0m Model loaded: deeplabv3_resnet50 | Device: cuda \n",
      "\u001b[36m(train_hyper pid=13654)\u001b[0m Latest Epoch Save doesnt exist or Epoch Number Save doesnt exist, initialising new Save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 13:21:50,716\tERROR tune_controller.py:1331 -- Trial task failed for trial train_hyper_7eaed_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/_private/worker.py\", line 863, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: 5cb41565fe55f3a5fe68ac8f01000000\n",
      "\tpid: 13654\n",
      "\tnamespace: c44767f3-5aea-4cd2-9cbc-69e010feb871\n",
      "\tip: 10.7.0.4\n",
      "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5cb41565fe55f3a5fe68ac8f01000000 Worker ID: acacad46c545bd5be07685f172fcfc55b24d52955dab8a2d082b1e6d Node ID: 2498711258b425fdebcab6b10262372b301234b82601d67499adc2bd Worker IP address: 10.7.0.4 Worker port: 32973 Worker PID: 13654 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_hyper_7eaed_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 13:21:50,726\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-04 13:21:50,728\tINFO tune.py:1007 -- Wrote the latest version of all result files and experiment state to '/home/jan/ray_results/train_hyper_2024-05-04_13-21-47' in 0.0040s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-05-04 13:21:50 (running for 00:00:03.40)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 6.000: None | Iter 3.000: None\n",
      "Logical resource usage: 0/12 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2024-05-04_13-21-45_240245_12924/artifacts/2024-05-04_13-21-47/train_hyper_2024-05-04_13-21-47/driver_artifacts\n",
      "Number of trials: 1/1 (1 ERROR)\n",
      "+-------------------------+----------+----------------+--------------+-----------------+------------+----------------+\n",
      "| Trial name              | status   | loc            |   batch_size |   learning_rate |   momentum |   weight_decay |\n",
      "|-------------------------+----------+----------------+--------------+-----------------+------------+----------------|\n",
      "| train_hyper_7eaed_00000 | ERROR    | 10.7.0.4:13654 |            2 |     1.05195e-06 |   0.192267 |    1.73692e-05 |\n",
      "+-------------------------+----------+----------------+--------------+-----------------+------------+----------------+\n",
      "Number of errored trials: 1\n",
      "+-------------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name              |   # failures | error file                                                                                                                                                                                                                                                     |\n",
      "|-------------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_hyper_7eaed_00000 |            1 | /tmp/ray/session_2024-05-04_13-21-45_240245_12924/artifacts/2024-05-04_13-21-47/train_hyper_2024-05-04_13-21-47/driver_artifacts/train_hyper_7eaed_00000_0_batch_size=2,learning_rate=0.0000,momentum=0.1923,weight_decay=0.0000_2024-05-04_13-21-47/error.txt |\n",
      "+-------------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\u001b[36m(train_hyper pid=13654)\u001b[0m Failed to initialise new model\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [train_hyper_7eaed_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 40\u001b[0m\n\u001b[1;32m     30\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m ASHAScheduler(\n\u001b[1;32m     31\u001b[0m     metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmiou\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     reduction_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     38\u001b[0m reporter \u001b[38;5;241m=\u001b[39m CLIReporter(metric_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmiou\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 40\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_hyper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mresources_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mprogress_reporter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreporter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/studi/lib/python3.9/site-packages/ray/tune/tune.py:1033\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incomplete_trials:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failed_trial \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment_interrupted_event\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[0;32m-> 1033\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TuneError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete\u001b[39m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [train_hyper_7eaed_00000])"
     ]
    }
   ],
   "source": [
    "# model = all_models[0]\n",
    "\n",
    "\n",
    "# def train_hyper(config):\n",
    "    \n",
    "#     make_directory(model)\n",
    "#     hyper_model = TrainedModel(model, 2048, 1024, weights_name='', folder_path=f'Hyperparameter/{model}', start_epoch='latest')\n",
    "#     hyper_model.prepare_model_training(dataset_train=k_fold_dataset.train_dataset,\n",
    "#                                                 dataset_val=k_fold_dataset.val_dataset,\n",
    "#                                                 batch_size=int(config['batch_size']), \n",
    "#                                                 shuffle=True, \n",
    "#                                                 learning_rate=config['lr'], \n",
    "#                                                 momentum=config['momentum'],\n",
    "#                                                 weight_decay=config['weight_decay'],)\n",
    "\n",
    "    \n",
    "#     hyper_model.train(1)  # Train for one epoch\n",
    "#     val_loss = hyper_model.validate(k_fold_dataset.val_dataset)  # Validate on the validation dataset\n",
    "#     miou = hyper_model.calculate_miou_miou(k_fold_dataset.val_dataset)\n",
    "#     tune.report(loss=val_loss, miou=miou)\n",
    "        \n",
    "\n",
    "# config = {\n",
    "#     \"learning_rate\": tune.loguniform(1e-12, 1e-2),\n",
    "#     \"batch_size\": tune.choice([2, 3, 4]),\n",
    "#     \"momentum\": tune.uniform(0.1, 0.9),\n",
    "#     \"weight_decay\": tune.loguniform(1e-6, 1e-1)\n",
    "# }\n",
    "\n",
    "# scheduler = ASHAScheduler(\n",
    "#     metric=\"miou\",\n",
    "#     mode=\"max\",\n",
    "#     max_t=10,\n",
    "#     grace_period=3,\n",
    "#     reduction_factor=2\n",
    "# )\n",
    "\n",
    "# reporter = CLIReporter(metric_columns=[\"loss\", \"miou\", \"training_iteration\"])\n",
    "\n",
    "# analysis = tune.run(train_hyper,\n",
    "#                     config=config,\n",
    "#                     resources_per_trial={\"gpu\": 1},\n",
    "#                     scheduler=scheduler,\n",
    "#                     progress_reporter=reporter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
