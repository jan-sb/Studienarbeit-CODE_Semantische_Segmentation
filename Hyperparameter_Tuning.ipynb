{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-01 14:00:41.434427: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-01 14:00:41.994560: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import torch\n",
    "from ray import tune\n",
    "from ray import train\n",
    "from ray.train import Checkpoint, get_checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune import CLIReporter\n",
    "from Helper.ml_models import * \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(image_dir='CityscapesDaten/images', annotation_dir='CityscapesDaten/semantic'):\n",
    "    trainset = CustomDataSet(image_dir=image_dir, annotation_dir=annotation_dir)\n",
    "\n",
    "    # If you have a separate set of images and annotations for testing, you can create a testset in a similar way:\n",
    "    # testset = CustomDataSet(image_dir=test_image_dir, annotation_dir=test_annotation_dir)\n",
    "\n",
    "    # If you don't have a separate test set, you can split the trainset into a training set and a test set:\n",
    "    train_size = int(0.8 * len(trainset))\n",
    "    test_size = len(trainset) - train_size\n",
    "    trainset, testset = torch.utils.data.random_split(trainset, [train_size, test_size])\n",
    "\n",
    "    return trainset, testset\n",
    "\n",
    "def make_directory(model):\n",
    "    dir_name = f'Hyperparameter/{model}'\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data leaks found.\n"
     ]
    }
   ],
   "source": [
    "all_models = ['deeplabv3_resnet50', 'deeplabv3_resnet101', 'deeplabv3_mobilenet_v3_large', 'lraspp_mobilenet_v3_large']\n",
    "not_yet_studied = ['fcn_resnet50', 'fcn_resnet101']\n",
    "test_epochs = 60\n",
    "\n",
    "k_fold_dataset = K_Fold_Dataset('/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/CityscapesDaten/images',\n",
    "                         '/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/CityscapesDaten/semantic',\n",
    "                         k_fold_csv_dir='/home/jan/studienarbeit/Studienarbeit-CODE_Semantische_Segmentation/Daten/CityscapesDaten',\n",
    "                         leave_out_fold=0,\n",
    "                         )\n",
    "\n",
    "k_fold_dataset.check_for_data_leaks()               \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = all_models[0]\n",
    "# make_directory(model)\n",
    "# config = {  \n",
    "#         'batch_size': 10,\n",
    "#         'lr' : 0.001,\n",
    "#         'momentum' : 0.9,\n",
    "#         'weight_decay' : 0.0005,    \n",
    "# }\n",
    "\n",
    "# hyper_model = TrainedModel(model, 2048, 1024, weights_name='', folder_path=f'Hyperparameter/{model}', start_epoch='latest')\n",
    "\n",
    "# hyper_model.prepare_model_training(dataset_train=k_fold_dataset.train_dataset,\n",
    "#                                             dataset_val=k_fold_dataset.val_dataset,\n",
    "#                                             dataset_test=k_fold_dataset.test_dataset,\n",
    "#                                             batch_size=int(config['batch_size']), \n",
    "#                                             shuffle=True, \n",
    "#                                             learning_rate=config['lr'], \n",
    "#                                             momentum=config['momentum'],\n",
    "#                                             weight_decay=config['weight_decay'], \n",
    "#                                             num_workers=4, \n",
    "#                                             pin_memory=True,\n",
    "#                                             )\n",
    "\n",
    "\n",
    "# epoch_loss, epoch_acc = hyper_model.train()  # Train for one epoch\n",
    "# #miou = hyper_model.calculate_miou_miou(k_fold_dataset.val_dataset)\n",
    "# tune.report(loss=epoch_loss, miou=epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-01 14:00:44,349\tINFO worker.py:1749 -- Started a local Ray instance.\n",
      "2024-06-01 14:00:44,866\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
      "2024-06-01 14:00:44,867\tINFO tune.py:614 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-06-01 14:00:44 (running for 00:00:00.11)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 6.000: None | Iter 3.000: None\n",
      "Logical resource usage: 0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2024-06-01_14-00-42_652936_15596/artifacts/2024-06-01_14-00-44/train_hyper_2024-06-01_14-00-44/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "+-------------------------+----------+-------+--------------+-----------------+----------------+\n",
      "| Trial name              | status   | loc   |   batch_size |   learning_rate |   weight_decay |\n",
      "|-------------------------+----------+-------+--------------+-----------------+----------------|\n",
      "| train_hyper_938ae_00000 | PENDING  |       |           12 |     1.34659e-06 |     0.00966882 |\n",
      "+-------------------------+----------+-------+--------------+-----------------+----------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=16351)\u001b[0m 2024-06-01 14:00:47.516532: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=16351)\u001b[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=16351)\u001b[0m 2024-06-01 14:00:48.145703: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_hyper pid=16351)\u001b[0m Using CUDA GPU\n",
      "\u001b[36m(train_hyper pid=16351)\u001b[0m Model loaded: deeplabv3_resnet50 | Device: cuda \n",
      "\u001b[36m(train_hyper pid=16351)\u001b[0m own lrs: 1e-05\n",
      "\u001b[36m(train_hyper pid=16351)\u001b[0m Latest Epoch Save doesnt exist or Epoch Number Save doesnt exist, initialising new Save\n",
      "\u001b[36m(train_hyper pid=16351)\u001b[0m own lrs: 1e-05\n",
      "\u001b[36m(train_hyper pid=16351)\u001b[0m Saved Model\n",
      "\u001b[36m(train_hyper pid=16351)\u001b[0m Successfully loaded Model\n",
      "\u001b[36m(train_hyper pid=16351)\u001b[0m Training Dataset prepared\n",
      "\u001b[36m(train_hyper pid=16351)\u001b[0m Validation Dataset prepared\n",
      "\u001b[36m(train_hyper pid=16351)\u001b[0m Test Dataset prepared\n",
      "== Status ==\n",
      "Current time: 2024-06-01 14:00:50 (running for 00:00:05.21)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 6.000: None | Iter 3.000: None\n",
      "Logical resource usage: 0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2024-06-01_14-00-42_652936_15596/artifacts/2024-06-01_14-00-44/train_hyper_2024-06-01_14-00-44/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+----------------+--------------+-----------------+----------------+\n",
      "| Trial name              | status   | loc            |   batch_size |   learning_rate |   weight_decay |\n",
      "|-------------------------+----------+----------------+--------------+-----------------+----------------|\n",
      "| train_hyper_938ae_00000 | RUNNING  | 10.7.0.4:16351 |           12 |     1.34659e-06 |     0.00966882 |\n",
      "+-------------------------+----------+----------------+--------------+-----------------+----------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-01 14:00:52,334\tERROR tune_controller.py:1331 -- Trial task failed for trial train_hyper_938ae_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/_private/worker.py\", line 2623, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/_private/worker.py\", line 861, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=16351, ip=10.7.0.4, actor_id=346d4df80d58ddfd63b4629c01000000, repr=train_hyper)\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 98, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/home/jan/anaconda3/envs/studi/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 248, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_15596/3047876297.py\", line 25, in train_hyper\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_hyper_938ae_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-01 14:00:52,346\tINFO tune.py:1007 -- Wrote the latest version of all result files and experiment state to '/home/jan/ray_results/train_hyper_2024-06-01_14-00-44' in 0.0033s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-06-01 14:00:52 (running for 00:00:07.46)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 6.000: None | Iter 3.000: None\n",
      "Logical resource usage: 0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2024-06-01_14-00-42_652936_15596/artifacts/2024-06-01_14-00-44/train_hyper_2024-06-01_14-00-44/driver_artifacts\n",
      "Number of trials: 1/1 (1 ERROR)\n",
      "+-------------------------+----------+----------------+--------------+-----------------+----------------+\n",
      "| Trial name              | status   | loc            |   batch_size |   learning_rate |   weight_decay |\n",
      "|-------------------------+----------+----------------+--------------+-----------------+----------------|\n",
      "| train_hyper_938ae_00000 | ERROR    | 10.7.0.4:16351 |           12 |     1.34659e-06 |     0.00966882 |\n",
      "+-------------------------+----------+----------------+--------------+-----------------+----------------+\n",
      "Number of errored trials: 1\n",
      "+-------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name              |   # failures | error file                                                                                                                                                                                                                                      |\n",
      "|-------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_hyper_938ae_00000 |            1 | /tmp/ray/session_2024-06-01_14-00-42_652936_15596/artifacts/2024-06-01_14-00-44/train_hyper_2024-06-01_14-00-44/driver_artifacts/train_hyper_938ae_00000_0_batch_size=12,learning_rate=0.0000,weight_decay=0.0097_2024-06-01_14-00-44/error.txt |\n",
      "+-------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [train_hyper_938ae_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 47\u001b[0m\n\u001b[1;32m     37\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m ASHAScheduler(\n\u001b[1;32m     38\u001b[0m     metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     39\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     reduction_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     45\u001b[0m reporter \u001b[38;5;241m=\u001b[39m CLIReporter(metric_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 47\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_hyper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mresources_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mprogress_reporter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreporter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters found were: \u001b[39m\u001b[38;5;124m\"\u001b[39m, analysis\u001b[38;5;241m.\u001b[39mbest_config)\n\u001b[1;32m     58\u001b[0m best_config \u001b[38;5;241m=\u001b[39m analysis\u001b[38;5;241m.\u001b[39mbest_config\n",
      "File \u001b[0;32m~/anaconda3/envs/studi/lib/python3.9/site-packages/ray/tune/tune.py:1033\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incomplete_trials:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failed_trial \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment_interrupted_event\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[0;32m-> 1033\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TuneError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete\u001b[39m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [train_hyper_938ae_00000])"
     ]
    }
   ],
   "source": [
    "model = all_models[0]\n",
    "\n",
    "\n",
    "def train_hyper(config):\n",
    "    try:\n",
    "        make_directory(model)\n",
    "        hyper_model = TrainedModel(model, 2048, 1024, weights_name='', folder_path=f'Hyperparameter/{model}', start_epoch='latest')\n",
    "        hyper_model.prepare_model_training(dataset_train=k_fold_dataset.train_dataset,\n",
    "                                                dataset_val=k_fold_dataset.val_dataset,\n",
    "                                                dataset_test=k_fold_dataset.test_dataset,\n",
    "                                                batch_size=int(config['batch_size']), \n",
    "                                                shuffle=True, \n",
    "                                                learning_rate=config['learning_rate'],\n",
    "                                                weight_decay=config['weight_decay'], \n",
    "                                                num_workers=4, \n",
    "                                                pin_memory=True,\n",
    "                                                ray_tune=True,\n",
    "                                                )\n",
    "\n",
    "        epoch_loss, epoch_acc = hyper_model.train() \n",
    "        miou = hyper_model.calculate_miou_miou(k_fold_dataset.val_dataset)\n",
    "        tune.report(loss=epoch_loss, miou=miou)\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            tune.report(loss=float('inf'), miou=0)  \n",
    "        else:\n",
    "            raise e  \n",
    "        \n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": tune.loguniform(1e-12, 1e-2),\n",
    "    'batch_size': tune.choice([2,4,6,8,12,14,16]),\n",
    "    \"weight_decay\": tune.loguniform(1e-6, 1e-1)\n",
    "}\n",
    "\n",
    "# Define the scheduler and reporter\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=10,\n",
    "    grace_period=3,\n",
    "    reduction_factor=2\n",
    ")\n",
    "\n",
    "reporter = CLIReporter(metric_columns=[\"loss\", \"acc\", \"training_iteration\"])\n",
    "\n",
    "analysis = tune.run(train_hyper,\n",
    "                    config=config,\n",
    "                    resources_per_trial={\"gpu\": 1},\n",
    "                    scheduler=scheduler,    \n",
    "                    progress_reporter=reporter, \n",
    "                    )\n",
    "\n",
    "print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "\n",
    "\n",
    "\n",
    "best_config = analysis.best_config\n",
    "\n",
    "# Save the best configuration to a JSON file\n",
    "with open('best_config.json', 'w') as json_file:\n",
    "    json.dump(best_config, json_file)\n",
    "\n",
    "print(\"Best configuration saved to best_config.json.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
